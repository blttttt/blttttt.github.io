<!DOCTYPE html>
<html lang="zh-CN">
<head>
  <meta charset="UTF-8">
<meta name="viewport" content="width=device-width">
<meta name="theme-color" content="#222"><meta name="generator" content="Hexo 7.1.1">

  <link rel="apple-touch-icon" sizes="180x180" href="/images/apple-touch-icon-next.png">
  <link rel="icon" type="image/png" sizes="32x32" href="/images/favicon-32x32-next.png">
  <link rel="icon" type="image/png" sizes="16x16" href="/images/favicon-16x16-next.png">
  <link rel="mask-icon" href="/images/logo.svg" color="#222">

<link rel="stylesheet" href="/css/main.css">



<link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/6.5.1/css/all.min.css" integrity="sha256-wiz7ZSCn/btzhjKDQBms9Hx4sSeUYsDrTLg7roPstac=" crossorigin="anonymous">
  <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/animate.css/3.1.1/animate.min.css" integrity="sha256-PR7ttpcvz8qrF57fur/yAx1qXMFJeJFiA6pSzWi0OIE=" crossorigin="anonymous">

<script class="next-config" data-name="main" type="application/json">{"hostname":"example.com","root":"/","images":"/images","scheme":"Pisces","darkmode":false,"version":"8.19.2","exturl":false,"sidebar":{"position":"left","display":"post","padding":18,"offset":12},"copycode":{"enable":true,"style":"mac"},"fold":{"enable":true,"height":500},"bookmark":{"enable":false,"color":"#222","save":"auto"},"mediumzoom":false,"lazyload":false,"pangu":false,"comments":{"style":"tabs","active":null,"storage":true,"lazyload":false,"nav":null},"stickytabs":false,"motion":{"enable":true,"async":false,"transition":{"menu_item":"fadeInDown","post_block":"fadeIn","post_header":"fadeInDown","post_body":"fadeInDown","coll_header":"fadeInLeft","sidebar":"fadeInUp"}},"i18n":{"placeholder":"搜索...","empty":"没有找到任何搜索结果：${query}","hits_time":"找到 ${hits} 个搜索结果（用时 ${time} 毫秒）","hits":"找到 ${hits} 个搜索结果"}}</script><script src="/js/config.js"></script>

    <meta name="description" content="urlliburllib是一个Python库，利用它可以实现HTTP请求的发送，我们要做的是指定请求的URL、请求头、请求体等信息。此外urllib还可以把服务器返回的对象转化为Python对象，通过该对象可以方便地获取响应的相关信息，如响应状态码、响应头、响应体。urllib库包含如下4个模块。  request: 最基本的http请求模块，可以模拟请求的发送。 error: 异常处理模块。 p">
<meta property="og:type" content="article">
<meta property="og:title" content="基本库的使用">
<meta property="og:url" content="http://example.com/2024/04/28/%E5%9F%BA%E6%9C%AC%E5%BA%93%E7%9A%84%E4%BD%BF%E7%94%A8/index.html">
<meta property="og:site_name" content="白乐天">
<meta property="og:description" content="urlliburllib是一个Python库，利用它可以实现HTTP请求的发送，我们要做的是指定请求的URL、请求头、请求体等信息。此外urllib还可以把服务器返回的对象转化为Python对象，通过该对象可以方便地获取响应的相关信息，如响应状态码、响应头、响应体。urllib库包含如下4个模块。  request: 最基本的http请求模块，可以模拟请求的发送。 error: 异常处理模块。 p">
<meta property="og:locale" content="zh_CN">
<meta property="og:image" content="http://example.com/2024/04/28/%E5%9F%BA%E6%9C%AC%E5%BA%93%E7%9A%84%E4%BD%BF%E7%94%A8/1.png">
<meta property="og:image" content="http://example.com/2024/04/28/%E5%9F%BA%E6%9C%AC%E5%BA%93%E7%9A%84%E4%BD%BF%E7%94%A8/2.png">
<meta property="og:image" content="http://example.com/2024/04/28/%E5%9F%BA%E6%9C%AC%E5%BA%93%E7%9A%84%E4%BD%BF%E7%94%A8/3.png">
<meta property="og:image" content="http://example.com/2024/04/28/%E5%9F%BA%E6%9C%AC%E5%BA%93%E7%9A%84%E4%BD%BF%E7%94%A8/4.png">
<meta property="og:image" content="http://example.com/2024/04/28/%E5%9F%BA%E6%9C%AC%E5%BA%93%E7%9A%84%E4%BD%BF%E7%94%A8/5.png">
<meta property="og:image" content="http://example.com/2024/04/28/%E5%9F%BA%E6%9C%AC%E5%BA%93%E7%9A%84%E4%BD%BF%E7%94%A8/6.png">
<meta property="og:image" content="http://example.com/2024/04/28/%E5%9F%BA%E6%9C%AC%E5%BA%93%E7%9A%84%E4%BD%BF%E7%94%A8/7.png">
<meta property="og:image" content="http://example.com/2024/04/28/%E5%9F%BA%E6%9C%AC%E5%BA%93%E7%9A%84%E4%BD%BF%E7%94%A8/8.png">
<meta property="og:image" content="http://example.com/2024/04/28/%E5%9F%BA%E6%9C%AC%E5%BA%93%E7%9A%84%E4%BD%BF%E7%94%A8/9.png">
<meta property="og:image" content="http://example.com/2024/04/28/%E5%9F%BA%E6%9C%AC%E5%BA%93%E7%9A%84%E4%BD%BF%E7%94%A8/10.png">
<meta property="og:image" content="http://example.com/2024/04/28/%E5%9F%BA%E6%9C%AC%E5%BA%93%E7%9A%84%E4%BD%BF%E7%94%A8/11.png">
<meta property="og:image" content="http://example.com/2024/04/28/%E5%9F%BA%E6%9C%AC%E5%BA%93%E7%9A%84%E4%BD%BF%E7%94%A8/12.png">
<meta property="article:published_time" content="2024-04-28T12:40:18.000Z">
<meta property="article:modified_time" content="2024-05-17T03:56:15.289Z">
<meta property="article:author" content="白乐天">
<meta property="article:tag" content="基本库的使用">
<meta name="twitter:card" content="summary">
<meta name="twitter:image" content="http://example.com/2024/04/28/%E5%9F%BA%E6%9C%AC%E5%BA%93%E7%9A%84%E4%BD%BF%E7%94%A8/1.png">


<link rel="canonical" href="http://example.com/2024/04/28/%E5%9F%BA%E6%9C%AC%E5%BA%93%E7%9A%84%E4%BD%BF%E7%94%A8/">



<script class="next-config" data-name="page" type="application/json">{"sidebar":"","isHome":false,"isPost":true,"lang":"zh-CN","comments":true,"permalink":"http://example.com/2024/04/28/%E5%9F%BA%E6%9C%AC%E5%BA%93%E7%9A%84%E4%BD%BF%E7%94%A8/","path":"2024/04/28/基本库的使用/","title":"基本库的使用"}</script>

<script class="next-config" data-name="calendar" type="application/json">""</script>
<title>基本库的使用 | 白乐天</title>
  








  <noscript>
    <link rel="stylesheet" href="/css/noscript.css">
  </noscript>
</head>

<body itemscope itemtype="http://schema.org/WebPage" class="use-motion">
  <div class="headband"></div>

  <main class="main">
    <div class="column">
      <header class="header" itemscope itemtype="http://schema.org/WPHeader"><div class="site-brand-container">
  <div class="site-nav-toggle">
    <div class="toggle" aria-label="切换导航栏" role="button">
        <span class="toggle-line"></span>
        <span class="toggle-line"></span>
        <span class="toggle-line"></span>
    </div>
  </div>

  <div class="site-meta">

    <a href="/" class="brand" rel="start">
      <i class="logo-line"></i>
      <p class="site-title">白乐天</p>
      <i class="logo-line"></i>
    </a>
  </div>

  <div class="site-nav-right">
    <div class="toggle popup-trigger" aria-label="搜索" role="button">
    </div>
  </div>
</div>



<nav class="site-nav">
  <ul class="main-menu menu"><li class="menu-item menu-item-home"><a href="/" rel="section"><i class="fa fa-home fa-fw"></i>首页</a></li><li class="menu-item menu-item-tags"><a href="/tags/" rel="section"><i class="fa fa-tags fa-fw"></i>标签<span class="badge">65</span></a></li><li class="menu-item menu-item-categories"><a href="/categories/" rel="section"><i class="fa fa-th fa-fw"></i>分类<span class="badge">30</span></a></li><li class="menu-item menu-item-archives"><a href="/archives/" rel="section"><i class="fa fa-archive fa-fw"></i>归档<span class="badge">70</span></a></li>
  </ul>
</nav>




</header>
        
  
  <aside class="sidebar">

    <div class="sidebar-inner sidebar-nav-active sidebar-toc-active">
      <ul class="sidebar-nav">
        <li class="sidebar-nav-toc">
          文章目录
        </li>
        <li class="sidebar-nav-overview">
          站点概览
        </li>
      </ul>

      <div class="sidebar-panel-container">
        <!--noindex-->
        <div class="post-toc-wrap sidebar-panel">
            <div class="post-toc animated"><ol class="nav"><li class="nav-item nav-level-1"><a class="nav-link" href="#urllib"><span class="nav-number">1.</span> <span class="nav-text">urllib</span></a><ol class="nav-child"><li class="nav-item nav-level-2"><a class="nav-link" href="#%E5%8F%91%E9%80%81%E8%AF%B7%E6%B1%82"><span class="nav-number">1.1.</span> <span class="nav-text">发送请求</span></a><ol class="nav-child"><li class="nav-item nav-level-3"><a class="nav-link" href="#urlopen"><span class="nav-number">1.1.1.</span> <span class="nav-text">urlopen</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#data%E5%8F%82%E6%95%B0"><span class="nav-number">1.1.2.</span> <span class="nav-text">data参数</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#timeout%E5%8F%82%E6%95%B0"><span class="nav-number">1.1.3.</span> <span class="nav-text">timeout参数</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#%E5%85%B6%E4%BB%96%E5%8F%82%E6%95%B0"><span class="nav-number">1.1.4.</span> <span class="nav-text">其他参数</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#Request"><span class="nav-number">1.1.5.</span> <span class="nav-text">Request</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#%E9%AB%98%E7%BA%A7%E7%94%A8%E6%B3%95"><span class="nav-number">1.1.6.</span> <span class="nav-text">高级用法</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#%E9%AA%8C%E8%AF%81"><span class="nav-number">1.1.7.</span> <span class="nav-text">验证</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#%E4%BB%A3%E7%90%86"><span class="nav-number">1.1.8.</span> <span class="nav-text">代理</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#Cookie"><span class="nav-number">1.1.9.</span> <span class="nav-text">Cookie</span></a></li></ol></li><li class="nav-item nav-level-2"><a class="nav-link" href="#%E5%A4%84%E7%90%86%E5%BC%82%E5%B8%B8"><span class="nav-number">1.2.</span> <span class="nav-text">处理异常</span></a><ol class="nav-child"><li class="nav-item nav-level-3"><a class="nav-link" href="#URLError"><span class="nav-number">1.2.1.</span> <span class="nav-text">URLError</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#HTTPError"><span class="nav-number">1.2.2.</span> <span class="nav-text">HTTPError</span></a></li></ol></li><li class="nav-item nav-level-2"><a class="nav-link" href="#%E8%A7%A3%E6%9E%90%E9%93%BE%E6%8E%A5"><span class="nav-number">1.3.</span> <span class="nav-text">解析链接</span></a><ol class="nav-child"><li class="nav-item nav-level-3"><a class="nav-link" href="#urlparse"><span class="nav-number">1.3.1.</span> <span class="nav-text">urlparse</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#urlunparse"><span class="nav-number">1.3.2.</span> <span class="nav-text">urlunparse</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#urlsplit"><span class="nav-number">1.3.3.</span> <span class="nav-text">urlsplit</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#urlunsplit"><span class="nav-number">1.3.4.</span> <span class="nav-text">urlunsplit</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#urljoin"><span class="nav-number">1.3.5.</span> <span class="nav-text">urljoin</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#urlencode"><span class="nav-number">1.3.6.</span> <span class="nav-text">urlencode</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#parse-qs"><span class="nav-number">1.3.7.</span> <span class="nav-text">parse_qs</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#parse-qsl"><span class="nav-number">1.3.8.</span> <span class="nav-text">parse_qsl</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#quote"><span class="nav-number">1.3.9.</span> <span class="nav-text">quote</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#unquote"><span class="nav-number">1.3.10.</span> <span class="nav-text">unquote</span></a></li></ol></li><li class="nav-item nav-level-2"><a class="nav-link" href="#%E5%88%86%E6%9E%90Robots%E5%8D%8F%E8%AE%AE"><span class="nav-number">1.4.</span> <span class="nav-text">分析Robots协议</span></a><ol class="nav-child"><li class="nav-item nav-level-3"><a class="nav-link" href="#Robots%E5%8D%8F%E8%AE%AE"><span class="nav-number">1.4.1.</span> <span class="nav-text">Robots协议</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#robotparser"><span class="nav-number">1.4.2.</span> <span class="nav-text">robotparser</span></a></li></ol></li></ol></li><li class="nav-item nav-level-1"><a class="nav-link" href="#requests%E7%9A%84%E4%BD%BF%E7%94%A8"><span class="nav-number">2.</span> <span class="nav-text">requests的使用</span></a><ol class="nav-child"><li class="nav-item nav-level-2"><a class="nav-link" href="#%E5%87%86%E5%A4%87"><span class="nav-number">2.1.</span> <span class="nav-text">准备</span></a></li><li class="nav-item nav-level-2"><a class="nav-link" href="#%E5%AE%9E%E4%BE%8B"><span class="nav-number">2.2.</span> <span class="nav-text">实例</span></a></li><li class="nav-item nav-level-2"><a class="nav-link" href="#GET%E8%AF%B7%E6%B1%82"><span class="nav-number">2.3.</span> <span class="nav-text">GET请求</span></a><ol class="nav-child"><li class="nav-item nav-level-3"><a class="nav-link" href="#%E5%9F%BA%E6%9C%AC%E5%AE%9E%E4%BE%8B"><span class="nav-number">2.3.1.</span> <span class="nav-text">基本实例</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#%E6%8A%93%E5%8F%96%E7%BD%91%E9%A1%B5"><span class="nav-number">2.3.2.</span> <span class="nav-text">抓取网页</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#%E6%8A%93%E5%8F%96%E4%BA%8C%E8%BF%9B%E5%88%B6%E6%95%B0%E6%8D%AE"><span class="nav-number">2.3.3.</span> <span class="nav-text">抓取二进制数据</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#%E6%B7%BB%E5%8A%A0%E8%AF%B7%E6%B1%82%E5%A4%B4"><span class="nav-number">2.3.4.</span> <span class="nav-text">添加请求头</span></a></li></ol></li><li class="nav-item nav-level-2"><a class="nav-link" href="#POST%E8%AF%B7%E6%B1%82"><span class="nav-number">2.4.</span> <span class="nav-text">POST请求</span></a></li><li class="nav-item nav-level-2"><a class="nav-link" href="#%E5%93%8D%E5%BA%94"><span class="nav-number">2.5.</span> <span class="nav-text">响应</span></a></li><li class="nav-item nav-level-2"><a class="nav-link" href="#%E9%AB%98%E7%BA%A7%E7%94%A8%E6%B3%95-1"><span class="nav-number">2.6.</span> <span class="nav-text">高级用法</span></a><ol class="nav-child"><li class="nav-item nav-level-3"><a class="nav-link" href="#%E6%96%87%E4%BB%B6%E4%B8%8A%E4%BC%A0"><span class="nav-number">2.6.1.</span> <span class="nav-text">文件上传</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#Cookie%E8%AE%BE%E7%BD%AE"><span class="nav-number">2.6.2.</span> <span class="nav-text">Cookie设置</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#Session%E7%BB%B4%E6%8C%81"><span class="nav-number">2.6.3.</span> <span class="nav-text">Session维持</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#SSL%E8%AF%81%E4%B9%A6%E9%AA%8C%E8%AF%81"><span class="nav-number">2.6.4.</span> <span class="nav-text">SSL证书验证</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#%E8%B6%85%E6%97%B6%E8%AE%BE%E7%BD%AE"><span class="nav-number">2.6.5.</span> <span class="nav-text">超时设置</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#%E8%BA%AB%E4%BB%BD%E8%AE%A4%E8%AF%81"><span class="nav-number">2.6.6.</span> <span class="nav-text">身份认证</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#%E4%BB%A3%E7%90%86%E8%AE%BE%E7%BD%AE"><span class="nav-number">2.6.7.</span> <span class="nav-text">代理设置</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#Prepared-Request"><span class="nav-number">2.6.8.</span> <span class="nav-text">Prepared Request</span></a></li></ol></li></ol></li><li class="nav-item nav-level-1"><a class="nav-link" href="#%E6%AD%A3%E5%88%99%E8%A1%A8%E8%BE%BE%E5%BC%8F"><span class="nav-number">3.</span> <span class="nav-text">正则表达式</span></a><ol class="nav-child"><li class="nav-item nav-level-2"><a class="nav-link" href="#%E5%AE%9E%E4%BE%8B%E5%BC%95%E5%85%A5"><span class="nav-number">3.1.</span> <span class="nav-text">实例引入</span></a></li><li class="nav-item nav-level-2"><a class="nav-link" href="#match"><span class="nav-number">3.2.</span> <span class="nav-text">match</span></a><ol class="nav-child"><li class="nav-item nav-level-3"><a class="nav-link" href="#%E5%8C%B9%E9%85%8D%E7%9B%AE%E6%A0%87"><span class="nav-number">3.2.1.</span> <span class="nav-text">匹配目标</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#%E9%80%9A%E7%94%A8%E5%8C%B9%E9%85%8D"><span class="nav-number">3.2.2.</span> <span class="nav-text">通用匹配</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#%E8%B4%AA%E5%A9%AA%E4%B8%8E%E9%9D%9E%E8%B4%AA%E5%A9%AA"><span class="nav-number">3.2.3.</span> <span class="nav-text">贪婪与非贪婪</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#%E4%BF%AE%E9%A5%B0%E7%AC%A6"><span class="nav-number">3.2.4.</span> <span class="nav-text">修饰符</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#%E8%BD%AC%E4%B9%89%E5%8C%B9%E9%85%8D"><span class="nav-number">3.2.5.</span> <span class="nav-text">转义匹配</span></a></li></ol></li><li class="nav-item nav-level-2"><a class="nav-link" href="#search"><span class="nav-number">3.3.</span> <span class="nav-text">search</span></a></li><li class="nav-item nav-level-2"><a class="nav-link" href="#findall"><span class="nav-number">3.4.</span> <span class="nav-text">findall</span></a></li><li class="nav-item nav-level-2"><a class="nav-link" href="#sub"><span class="nav-number">3.5.</span> <span class="nav-text">sub</span></a></li><li class="nav-item nav-level-2"><a class="nav-link" href="#compile"><span class="nav-number">3.6.</span> <span class="nav-text">compile</span></a></li></ol></li><li class="nav-item nav-level-1"><a class="nav-link" href="#httpx%E7%9A%84%E4%BD%BF%E7%94%A8"><span class="nav-number">4.</span> <span class="nav-text">httpx的使用</span></a><ol class="nav-child"><li class="nav-item nav-level-2"><a class="nav-link" href="#%E7%A4%BA%E4%BE%8B"><span class="nav-number">4.1.</span> <span class="nav-text">示例</span></a></li><li class="nav-item nav-level-2"><a class="nav-link" href="#Client%E5%AF%B9%E8%B1%A1"><span class="nav-number">4.2.</span> <span class="nav-text">Client对象</span></a></li><li class="nav-item nav-level-2"><a class="nav-link" href="#%E6%94%AF%E6%8C%81HTTP-2-0"><span class="nav-number">4.3.</span> <span class="nav-text">支持HTTP&#x2F;2.0</span></a></li><li class="nav-item nav-level-2"><a class="nav-link" href="#%E6%94%AF%E6%8C%81%E5%BC%82%E6%AD%A5%E8%AF%B7%E6%B1%82"><span class="nav-number">4.4.</span> <span class="nav-text">支持异步请求</span></a></li></ol></li><li class="nav-item nav-level-1"><a class="nav-link" href="#%E5%9F%BA%E7%A1%80%E5%AE%9E%E4%BE%8B%E6%A1%88%E4%BE%8B"><span class="nav-number">5.</span> <span class="nav-text">基础实例案例</span></a></li></ol></div>
        </div>
        <!--/noindex-->

        <div class="site-overview-wrap sidebar-panel">
          <div class="site-author animated" itemprop="author" itemscope itemtype="http://schema.org/Person">
    <img class="site-author-image" itemprop="image" alt="白乐天"
      src="/images/android.svg">
  <p class="site-author-name" itemprop="name">白乐天</p>
  <div class="site-description" itemprop="description">在安卓逆向和爬虫的道路上渐行渐远!</div>
</div>
<div class="site-state-wrap animated">
  <nav class="site-state">
      <div class="site-state-item site-state-posts">
        <a href="/archives/">
          <span class="site-state-item-count">70</span>
          <span class="site-state-item-name">日志</span>
        </a>
      </div>
      <div class="site-state-item site-state-categories">
          <a href="/categories/">
        <span class="site-state-item-count">30</span>
        <span class="site-state-item-name">分类</span></a>
      </div>
      <div class="site-state-item site-state-tags">
          <a href="/tags/">
        <span class="site-state-item-count">65</span>
        <span class="site-state-item-name">标签</span></a>
      </div>
  </nav>
</div>
  <div class="links-of-author animated">
      <span class="links-of-author-item">
        <a href="https://github.com/blttttt" title="GitHub → https:&#x2F;&#x2F;github.com&#x2F;blttttt" rel="noopener me" target="_blank"><i class="fab fa-github fa-fw"></i>GitHub</a>
      </span>
  </div>

        </div>
      </div>
        <div class="back-to-top animated" role="button" aria-label="返回顶部">
          <i class="fa fa-arrow-up"></i>
          <span>0%</span>
        </div>
    </div>

    
  </aside>


    </div>

    <div class="main-inner post posts-expand">


  


<div class="post-block">
  
  

  <article itemscope itemtype="http://schema.org/Article" class="post-content" lang="zh-CN">
    <link itemprop="mainEntityOfPage" href="http://example.com/2024/04/28/%E5%9F%BA%E6%9C%AC%E5%BA%93%E7%9A%84%E4%BD%BF%E7%94%A8/">

    <span hidden itemprop="author" itemscope itemtype="http://schema.org/Person">
      <meta itemprop="image" content="/images/android.svg">
      <meta itemprop="name" content="白乐天">
    </span>

    <span hidden itemprop="publisher" itemscope itemtype="http://schema.org/Organization">
      <meta itemprop="name" content="白乐天">
      <meta itemprop="description" content="在安卓逆向和爬虫的道路上渐行渐远!">
    </span>

    <span hidden itemprop="post" itemscope itemtype="http://schema.org/CreativeWork">
      <meta itemprop="name" content="基本库的使用 | 白乐天">
      <meta itemprop="description" content="">
    </span>
      <header class="post-header">
        <h1 class="post-title" itemprop="name headline">
          基本库的使用
        </h1>

        <div class="post-meta-container">
          <div class="post-meta">
    <span class="post-meta-item">
      <span class="post-meta-item-icon">
        <i class="far fa-calendar"></i>
      </span>
      <span class="post-meta-item-text">发表于</span>

      <time title="创建时间：2024-04-28 20:40:18" itemprop="dateCreated datePublished" datetime="2024-04-28T20:40:18+08:00">2024-04-28</time>
    </span>
    <span class="post-meta-item">
      <span class="post-meta-item-icon">
        <i class="far fa-calendar-check"></i>
      </span>
      <span class="post-meta-item-text">更新于</span>
      <time title="修改时间：2024-05-17 11:56:15" itemprop="dateModified" datetime="2024-05-17T11:56:15+08:00">2024-05-17</time>
    </span>
    <span class="post-meta-item">
      <span class="post-meta-item-icon">
        <i class="far fa-folder"></i>
      </span>
      <span class="post-meta-item-text">分类于</span>
        <span itemprop="about" itemscope itemtype="http://schema.org/Thing">
          <a href="/categories/%E7%88%AC%E8%99%AB/" itemprop="url" rel="index"><span itemprop="name">爬虫</span></a>
        </span>
    </span>

  
</div>

        </div>
      </header>

    
    
    
    <div class="post-body" itemprop="articleBody"><h1 id="urllib"><a href="#urllib" class="headerlink" title="urllib"></a>urllib</h1><p>urllib是一个Python库，利用它可以实现HTTP请求的发送，我们要做的是指定请求的URL、请求头、请求体等信息。此外urllib还可以把服务器返回的对象转化为Python对象，通过该对象可以方便地获取响应的相关信息，如响应状态码、响应头、响应体。<br>urllib库包含如下4个模块。</p>
<ul>
<li>request: 最基本的http请求模块，可以模拟请求的发送。</li>
<li>error: 异常处理模块。</li>
<li>parse: 一个工具模块。</li>
<li>robotparser: 主要用来识别网站的robots.txt文件，然后判断哪些网站可以爬，哪些网站不可以爬。</li>
</ul>
<h2 id="发送请求"><a href="#发送请求" class="headerlink" title="发送请求"></a>发送请求</h2><h3 id="urlopen"><a href="#urlopen" class="headerlink" title="urlopen"></a>urlopen</h3><p>urllib.request模块提供了最基本的构造HTTP请求的方法，利用这个模块可以模拟浏览器的请求发起过程。用百度测试</p>
<figure class="highlight py"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> urllib.request</span><br><span class="line">response = urllib.request.urlopen(<span class="string">&#x27;https://www.baidu.com&#x27;</span>)</span><br><span class="line"><span class="built_in">print</span>(response.read().decode(<span class="string">&#x27;utf-8&#x27;</span>))</span><br></pre></td></tr></table></figure>
<p><img src="/2024/04/28/%E5%9F%BA%E6%9C%AC%E5%BA%93%E7%9A%84%E4%BD%BF%E7%94%A8/1.png"></p>
<blockquote>
<p>response.read()方法<br>属于urllib.response对象，用来从HTTP响应中读取数据。read()方法会读取HTTP响应的主体（body），这通常是你请求的页面的HTML内容或API返回的JSON数据等。read()方法返回的是字节串（bytes），因为HTTP协议传输的是字节数据。由于返回的是字节串，通常需要将其解码为字符串才能阅读。常见的解码方式是使用.decode(‘utf-8’)，这假设响应内容是使用UTF-8编码的。除了read()，urllib.response对象还提供了其他方法来读取数据，如readline()（读取一行数据）和readlines()（读取所有行并返回一个列表）。</p>
</blockquote>
<figure class="highlight py"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> urllib.request</span><br><span class="line">response = urllib.request.urlopen(<span class="string">&#x27;https://www.baidu.com&#x27;</span>)</span><br><span class="line"><span class="built_in">print</span>(<span class="built_in">type</span>(response))</span><br><span class="line">&gt;&gt;&gt;</span><br><span class="line">&lt;<span class="keyword">class</span> <span class="string">&#x27;http.client.HTTPResponse&#x27;</span>&gt;</span><br></pre></td></tr></table></figure>
<p>这里响应的是<code>HTTPResponse</code>类型的对象，包含read、readinto、getheader、getheaders、fileno等方法。得到响应把它赋值给了response，可以通过response对象来调用哪些方法和属性。  </p>
<p>打印响应的属性</p>
<figure class="highlight py"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> urllib.request</span><br><span class="line">response = urllib.request.urlopen(<span class="string">&#x27;https://www.baidu.com&#x27;</span>)</span><br><span class="line"><span class="built_in">print</span>(response.status)  //响应状态码</span><br><span class="line"><span class="built_in">print</span>(response.getheaders())  //响应头的信息</span><br><span class="line"><span class="built_in">print</span>(response.getheader(<span class="string">&quot;Server&quot;</span>))  //提供参数，获取响应头对应参数的信息</span><br><span class="line"><span class="built_in">print</span>(response.read().decode(<span class="string">&#x27;utf-8&#x27;</span>))</span><br></pre></td></tr></table></figure>


<h3 id="data参数"><a href="#data参数" class="headerlink" title="data参数"></a>data参数</h3><p>&emsp;&emsp;data参数可选。在添加该参数的时候，需要使用bytes方法将参数转化为字节流编码格式的内容，即bytes类型。如果传递了这个参数，那么它的请求方式就不再是GET而是POST了。<br>我们请求的站点是<code>https://www.httpbin.org</code> ，它可以提供HTTP请求测试。本次请求的URL是 <code>https://www.httpbin.org/post</code>，这个连接可以测试POST请求，能够输出请求的一些信息，其中包含我们传递的data参数。</p>
<figure class="highlight py"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> urllib.request</span><br><span class="line"><span class="keyword">import</span> urllib.parse</span><br><span class="line">data = <span class="built_in">bytes</span>(urllib.parse.urlencode(&#123;<span class="string">&#x27;name&#x27;</span>:<span class="string">&#x27;germy&#x27;</span>&#125;),encoding=<span class="string">&#x27;utf-8&#x27;</span>)</span><br><span class="line">response = urllib.request.urlopen(<span class="string">&#x27;https://www.httpbin.org/post&#x27;</span>,data)</span><br><span class="line"><span class="built_in">print</span>(response.read().decode(<span class="string">&#x27;utf-8&#x27;</span>))</span><br></pre></td></tr></table></figure>
<p>我们传递的参数出现在了form字段中，表明是模拟表单提交，以POST方式传输数据。<br><img src="/2024/04/28/%E5%9F%BA%E6%9C%AC%E5%BA%93%E7%9A%84%E4%BD%BF%E7%94%A8/2.png"></p>
<h3 id="timeout参数"><a href="#timeout参数" class="headerlink" title="timeout参数"></a>timeout参数</h3><p>&emsp;&emsp;设置超时时间，单位是秒，如果请求超出了设置的这个时间，还没有得到响应，就会抛出异常，如果不指定这个参数，会使用全局默认时间。<br>实例</p>
<figure class="highlight py"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> socket</span><br><span class="line"><span class="keyword">import</span> urllib.request</span><br><span class="line">response = urllib.request.urlopen(<span class="string">&#x27;https://www.httpbin.org/get&#x27;</span>,timeout=<span class="number">0.1</span>)</span><br><span class="line"><span class="built_in">print</span>(response.read().decode(<span class="string">&#x27;utf-8&#x27;</span>))</span><br></pre></td></tr></table></figure>
<p>&emsp;&emsp;可能会出现<code>urllib.error.URLError: &lt;urlopen error timed out&gt;</code>这种报错</p>
<p>实例</p>
<figure class="highlight py"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> socket</span><br><span class="line"><span class="keyword">import</span> urllib.request</span><br><span class="line"><span class="keyword">import</span> urllib.parse</span><br><span class="line"><span class="keyword">import</span> urllib.error</span><br><span class="line"><span class="keyword">try</span> :</span><br><span class="line">    response = urllib.request.urlopen(<span class="string">&#x27;https://www.httpbin.org/get&#x27;</span>,timeout=<span class="number">0.1</span>)</span><br><span class="line"><span class="keyword">except</span> urllib.error.URLError <span class="keyword">as</span> e:</span><br><span class="line">    <span class="keyword">if</span> <span class="built_in">isinstance</span>(e.reason,socket.timeout):</span><br><span class="line">        <span class="built_in">print</span>(<span class="string">&#x27;Time Out&#x27;</span>)</span><br><span class="line">&gt;&gt;&gt;</span><br><span class="line">Time Out</span><br></pre></td></tr></table></figure>
<h3 id="其他参数"><a href="#其他参数" class="headerlink" title="其他参数"></a>其他参数</h3><p>&emsp;&emsp;<strong>context</strong>参数，该参数必须是ssl.SSLContext类型，用来指定SSL的设置。<br>&emsp;&emsp;<strong>cafile</strong>和<strong>capate</strong>这两个参数分别用来指定CA证书和其路径，在请求HTTPS连接时会用。  </p>
<h3 id="Request"><a href="#Request" class="headerlink" title="Request"></a>Request</h3><p>&emsp;&emsp;利用urlopen发起的最基本的请求，那几个参数并不足以构建一个完整的请求。如果想往请求中加入Headers信息，就要利用更强大的Request类来构建请求了。  </p>
<figure class="highlight py"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> urllib.request</span><br><span class="line">request = urllib.request.Request(<span class="string">&#x27;http://www.baidu.com&#x27;</span>)</span><br><span class="line">response = urllib.request.urlopen(request)</span><br><span class="line"><span class="built_in">print</span>(response.read().decode(<span class="string">&#x27;utf-8&#x27;</span>))</span><br></pre></td></tr></table></figure>
<p><img src="/2024/04/28/%E5%9F%BA%E6%9C%AC%E5%BA%93%E7%9A%84%E4%BD%BF%E7%94%A8/3.png"><br>这里我们依然用urlopen发送请求，但是这次该方法的参数不是URL了，而是一个Request类型的对象。<br>构造Request类：</p>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">class urllib.request.Request(url,data=None,headers=&#123;&#125;,origin_req_host,unverifiable=False,method=None)</span><br></pre></td></tr></table></figure>
<p>&emsp;&emsp;第一个参数url用于请求URL，是必传参数，其他都是可选参数。<br>&emsp;&emsp;第二个参数data如果要传数据，必须传bytes类型的。如果数据是字典，可以先用urllib.parse模块里的urlencode方法进行编码。<br>&emsp;&emsp;第三个参数headers是一个字典，就是请求头，在构造请求时，既可以通过headers参数直接构造此项，也可以通过调用请求实例的add_header方法添加。<br>&emsp;&emsp;添加请求头的常见方法是，通过修改User-Agent来伪装浏览器。默认的User-Agent是Python-urllib。<br>&emsp;&emsp;第四个参数origin_req_host指的是请求方的host名称或者IP地址。<br>&emsp;&emsp;第五个参数unverifiable表示请求方是否是无法验证的，默认取值是false，指用户没有足够的权限来接收这个请求的结果。<br>&emsp;&emsp;第六个参数是method是一个字符串，用来指示请求使用的方法。<br>&emsp;&emsp;传入多个参数构建Request类：</p>
<figure class="highlight py"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">from</span> urllib <span class="keyword">import</span> request,parse</span><br><span class="line">url = <span class="string">&#x27;http://www.httpbin.org/post&#x27;</span></span><br><span class="line">headers = &#123;<span class="string">&#x27;User-Agent&#x27;</span>: <span class="string">&#x27;Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/124.0.0.0 Safari/537.36 Edg/124.0.0.0&#x27;</span>,</span><br><span class="line">           <span class="string">&#x27;Host&#x27;</span>:<span class="string">&#x27;www.httpbin.org&#x27;</span>&#125;</span><br><span class="line"><span class="built_in">dict</span> = &#123;<span class="string">&#x27;name&#x27;</span>:<span class="string">&#x27;germy&#x27;</span>&#125;</span><br><span class="line">data = <span class="built_in">bytes</span>(parse.urlencode(<span class="built_in">dict</span>),encoding=<span class="string">&#x27;utf-8&#x27;</span>)</span><br><span class="line">req = request.Request(url,data,headers,method=<span class="string">&#x27;POST&#x27;</span>)</span><br><span class="line">response = request.urlopen(req)</span><br><span class="line"><span class="built_in">print</span>(response.read().decode(<span class="string">&#x27;utf-8&#x27;</span>))</span><br></pre></td></tr></table></figure>
<p>传入4个参数构造了一个Request类，data用urlencode方法和bytes方法把字典数据转换成字节流格式。<br>运行结果  </p>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br></pre></td><td class="code"><pre><span class="line">&#123;</span><br><span class="line">  &quot;args&quot;: &#123;&#125;, </span><br><span class="line">  &quot;data&quot;: &quot;&quot;, </span><br><span class="line">  &quot;files&quot;: &#123;&#125;, </span><br><span class="line">  &quot;form&quot;: &#123;</span><br><span class="line">    &quot;name&quot;: &quot;germy&quot;</span><br><span class="line">  &#125;, </span><br><span class="line">  &quot;headers&quot;: &#123;</span><br><span class="line">    &quot;Accept-Encoding&quot;: &quot;identity&quot;, </span><br><span class="line">    &quot;Content-Length&quot;: &quot;10&quot;, </span><br><span class="line">    &quot;Content-Type&quot;: &quot;application/x-www-form-urlencoded&quot;, </span><br><span class="line">    &quot;Host&quot;: &quot;www.httpbin.org&quot;, </span><br><span class="line">    &quot;User-Agent&quot;: &quot;Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/124.0.0.0 Safari/537.36 Edg/124.0.0.0&quot;, </span><br><span class="line">    &quot;X-Amzn-Trace-Id&quot;: &quot;Root=1-662f5c7d-78dad94c612f72536858fcd6&quot;</span><br><span class="line">  &#125;, </span><br><span class="line">  &quot;json&quot;: null, </span><br><span class="line">  &quot;origin&quot;: &quot;219.156.133.195&quot;, </span><br><span class="line">  &quot;url&quot;: &quot;http://www.httpbin.org/post&quot;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>
<p>成功设置了data,headers,method<br>通过add_header方法添加headers的方式</p>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">req=request.Request(url=url,data=data,method=method)</span><br><span class="line">req.add_header(&#x27;User-Agent&#x27;,&#x27;xxxxx&#x27;)</span><br></pre></td></tr></table></figure>
<h3 id="高级用法"><a href="#高级用法" class="headerlink" title="高级用法"></a>高级用法</h3><p>&emsp;&emsp;学会构建请求之后，还有一些高级操作如Cookie处理、代理设置等，我们需要用Handler。Handler可以理解为各种处理器，有专门处理登录验证的、处理Cookie的、处理代理设置的。利用Handler,几乎可以实现HTTP请求中所有的功能。<br>&emsp;&emsp;urllib.request模块的BaseHandler类是其他所有Handler类的父类。提供了最基本的方法，例如default_open，protocol_request等。<br>有各种Handler子类继承BaseHandler类</p>
<ul>
<li>HTTPDefaultErrorHandler用于处理HTTP响应错误，所有错误都会抛出HTTPError类型的异常。</li>
<li>HTTPRedirectHandler用于处理重定向。</li>
<li>HTTPCookieProcessor用于处理Cookie。</li>
<li>ProxyHandler用于设置代理，代理默认为空。</li>
<li>HTTPPasswordMgr用于管理密码，它维护着用户名密码的对照表。</li>
<li>HTTPBasicAuthHandle用于管理认证，如果一个链接在打开时需要认证，那么可以用这个类来解决认证问题。<br>还有一个重要的类OpenerDirector，我们可以称之为Opener。之前用过的urlopen方法，就是urllib库为我们提供的一个Opener。为了实现更高级的功能，前面使用的Request类和urlopen类相当于类库已经封装好的极其常用的方法，利用这两个类可以完成基本的请求，但现在要实现更高级的功能，就要深入一层进行配置，使用更底层的实例来完成操作，所以要用Opener。<br>Opener类提供open方法，该方法返回的响应类型和urlopen方法如出一辙。使用Handler类来构建Opener类。</li>
</ul>
<h3 id="验证"><a href="#验证" class="headerlink" title="验证"></a>验证</h3><p>&emsp;&emsp;在访问某些网站时，例如 <a target="_blank" rel="noopener" href="https://ssr3.scrape.center/">https://ssr3.scrape.center</a> ,可能会弹出认证窗口<br><img src="/2024/04/28/%E5%9F%BA%E6%9C%AC%E5%BA%93%E7%9A%84%E4%BD%BF%E7%94%A8/4.png"><br>&emsp;&emsp;这种情况是这个网站启用了基本身份认证，英文是HTTP Basic Access Authentication，一种登陆验证方式，允许网页浏览器或其他客户端程序在请求网站时提供用户名和口令形式的身份认证。<br>爬虫可以借助HTTPBasicAuthHandler模块完成。</p>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br></pre></td><td class="code"><pre><span class="line">from urllib.request import HTTPBasicAuthHandler,HTTPPasswordMgrWithDefaultRealm,build_opener</span><br><span class="line">from urllib.error import  URLError</span><br><span class="line"></span><br><span class="line">username = &#x27;admin&#x27;</span><br><span class="line">password = &#x27;admin&#x27;</span><br><span class="line">url=&quot;https://ssr3.scrape.center/&quot;</span><br><span class="line"></span><br><span class="line">p=HTTPPasswordMgrWithDefaultRealm()</span><br><span class="line">p.add_password(None,url,username,password)</span><br><span class="line">auth_handler = HTTPBasicAuthHandler(p)</span><br><span class="line">opener = build_opener(auth_handler)</span><br><span class="line"></span><br><span class="line">try :</span><br><span class="line">    result = opener.open(url)</span><br><span class="line">    html = result.read().decode(&#x27;utf-8&#x27;)</span><br><span class="line">    print(html)</span><br><span class="line">except URLError as e:</span><br><span class="line">    print(e.reason)</span><br></pre></td></tr></table></figure>
<p>&emsp;&emsp;这里首先实例化了一个HTTPBasicAuthHandler对象auth_handler，参数是HTTPPasswordMgrWithDefaultRealm对象，它利用add_passward方法添加用户名和密码，建立了一个用来处理验证的Handler类。<br>&emsp;&emsp;将对象auth_handler作为参数传递给build_opener方法，构建一个Opener，它在发送请求时就相当于验证成功了。<br>&emsp;&emsp;最后利用Opener类中的open方法打开连接，即可完成验证。这里获取的结果就是验证成功后的页面源码内容。  </p>
<h3 id="代理"><a href="#代理" class="headerlink" title="代理"></a>代理</h3><p>添加代理</p>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br></pre></td><td class="code"><pre><span class="line">from urllib.error import URLError</span><br><span class="line">from urllib.request import ProxyHandler , build_opener</span><br><span class="line"></span><br><span class="line">proxy_handler = ProxyHandler(</span><br><span class="line">    &#123;&#x27;http&#x27; : &#x27;http://127.0.0.1:8080&#x27;,</span><br><span class="line">     &#x27;https&#x27; : &#x27;https://127.0.0.1:8080&#x27;&#125;</span><br><span class="line">)</span><br><span class="line"></span><br><span class="line">opener = build_opener(proxy_handler)</span><br><span class="line"></span><br><span class="line">try:</span><br><span class="line">    response = opener.open(&#x27;https://www.baidu.com&#x27;)</span><br><span class="line">    print(response.read().decode(&#x27;utf-8&#x27;))</span><br><span class="line">except URLError as e:</span><br><span class="line">    print(e.reason)</span><br></pre></td></tr></table></figure>
<p>&emsp;&emsp;在本地搭建HTTP代理，让其运行在8080端口上。使用了ProxyHandler，其参数是一个字典，键名是协议类型，键值是代理链接，可以添加多个代理。利用这个Handler和build_opener方法构建了一个Opener，之后发送请求即可。  </p>
<h3 id="Cookie"><a href="#Cookie" class="headerlink" title="Cookie"></a>Cookie</h3><p>处理Cookie需要用到相关的Handler。<br>先用实例看看如何获取Cookie</p>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><span class="line">import http.cookiejar,urllib.request</span><br><span class="line"></span><br><span class="line">cookie = http.cookiejar.CookieJar()</span><br><span class="line">handler = urllib.request.HTTPCookieProcessor(cookie)</span><br><span class="line">opener = urllib.request.build_opener(handler)</span><br><span class="line">response = opener.open(&#x27;https://www.baidu.com&#x27;)</span><br><span class="line">for item in cookie:</span><br><span class="line">    print(item.name+&quot;=&quot;+item.value)</span><br></pre></td></tr></table></figure>
<p>&emsp;&emsp;先声明CookieJar对象。然后利用HTTPCookieProcessor构建一个Handler,再利用build_opener方法构建Opener，执行open函数即可。<br><img src="/2024/04/28/%E5%9F%BA%E6%9C%AC%E5%BA%93%E7%9A%84%E4%BD%BF%E7%94%A8/5.png"><br>&emsp;&emsp;分别输出了每个Cookie条目的名称和值。<br>&emsp;&emsp;输出文件格式的内容，Cookie实际上也是以文本形式保存的。  </p>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><span class="line">import urllib.request,http.cookiejar</span><br><span class="line"></span><br><span class="line">filename = &#x27;cookies.txt&#x27;</span><br><span class="line">cookie = http.cookiejar.MozillaCookieJar(filename)</span><br><span class="line">handler = urllib.request.HTTPCookieProcessor(cookie)</span><br><span class="line">opener = urllib.request.build_opener(handler)</span><br><span class="line">response = opener.open(&#x27;https://www.baidu.com&#x27;)</span><br><span class="line">cookie.save(ignore_discard=True,ignore_expires=True)</span><br></pre></td></tr></table></figure>
<p>&emsp;&emsp;这时将CookieJar换成MozillaCookieJar，它会在生成文件时用到，是CookieJar的子类，用来处理跟Cookie和文件相关的事件，如读取和保存Cookie可以将Cookie保存成Mozilla型浏览器的Cookie格式。<br><img src="/2024/04/28/%E5%9F%BA%E6%9C%AC%E5%BA%93%E7%9A%84%E4%BD%BF%E7%94%A8/6.png"><br>&emsp;&emsp;LWPCookieJar同样可以读取和保存Cookie，只是Cookie文件的保存格式和MozillaCookieJar不一样，它会保存成LWP(libwww-perl)格式。<br>&emsp;&emsp;要保存LWP格式的Cookie文件，只需在声明时修改：<br><code>cookie = http.cookiejar.LWPCookieJar(filename)</code><br><img src="/2024/04/28/%E5%9F%BA%E6%9C%AC%E5%BA%93%E7%9A%84%E4%BD%BF%E7%94%A8/7.png"><br>不同格式的Cookie有一定的差异。<br>生成Cookie之后，对其进行读取和利用<br>以LWPCookieJar格式为例：</p>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><span class="line">import urllib.request,http.cookiejar</span><br><span class="line"></span><br><span class="line">cookie = http.cookiejar.LWPCookieJar()</span><br><span class="line">cookie.load(&#x27;cookies.txt&#x27;,ignore_discard=True,ignore_expires=True)</span><br><span class="line">handler = urllib.request.HTTPCookieProcessor(cookie)</span><br><span class="line">opener = urllib.request.build_opener(handler)</span><br><span class="line">response = opener.open(&#x27;https://www.baidu.com&#x27;)</span><br><span class="line">print(response.read().decode(&#x27;utf-8&#x27;))</span><br></pre></td></tr></table></figure>
<p>这里调用load方法来读取本地的Cookie文件，获取Cookie的内容。这样做的前提是我们已经生成了LWPCookieJar格式的Cookie，并保存成了文件，读取了Cookie之后，用同样的方法构建Handler类和Opener类即可完成操作。  </p>
<h2 id="处理异常"><a href="#处理异常" class="headerlink" title="处理异常"></a>处理异常</h2><h3 id="URLError"><a href="#URLError" class="headerlink" title="URLError"></a>URLError</h3><p>&emsp;&emsp;URLError类来自urllib库的error模块，继承自OSError类，是error异常模块的基类，由request模块产生的异常可以通过捕获这个类来处理。<br>它的一个属性reaseon，返回错误的原因。<br>用一个实例来看看</p>
<figure class="highlight py"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">from</span> urllib <span class="keyword">import</span> request,error</span><br><span class="line"><span class="keyword">try</span>:</span><br><span class="line">    response = request.urlopen(<span class="string">&#x27;https://blttttt.com/404&#x27;</span>)</span><br><span class="line"><span class="keyword">except</span> error.URLError <span class="keyword">as</span> e:</span><br><span class="line">    <span class="built_in">print</span>(e.reason)</span><br><span class="line">&gt;&gt;&gt;</span><br><span class="line">[Errno <span class="number">11001</span>] getaddrinfo failed</span><br></pre></td></tr></table></figure>
<p>我们访问了一个不存在的页面，是会报错的，但是通过捕获URLError这个异常，没有直接报错，而是输出了报错的原因，可以避免程序异常终止。  </p>
<h3 id="HTTPError"><a href="#HTTPError" class="headerlink" title="HTTPError"></a>HTTPError</h3><p>HTTPError是URLError的子类，专门用来处理HTTP请求的错误，例如认证请求失败。<br>有三个属性：</p>
<ul>
<li>code<br>返回HTTP状态码  </li>
<li>reason<br>同父类，返回错误原因</li>
<li>headers<br>返回请求头</li>
</ul>
<p>实例</p>
<figure class="highlight py"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">from</span> urllib <span class="keyword">import</span> request,error</span><br><span class="line"><span class="keyword">try</span>:</span><br><span class="line">    response = request.urlopen(<span class="string">&#x27;https://cuiqingcai.com/404&#x27;</span>)</span><br><span class="line"><span class="keyword">except</span> error.HTTPError <span class="keyword">as</span> e:</span><br><span class="line">    <span class="built_in">print</span>(e.reason,e.code,e.headers,sep=<span class="string">&#x27;\n&#x27;</span>)</span><br></pre></td></tr></table></figure>
<p>运行结果<br><img src="/2024/04/28/%E5%9F%BA%E6%9C%AC%E5%BA%93%E7%9A%84%E4%BD%BF%E7%94%A8/8.png"><br>URLError是HTTPError的父类，所以可以先选择捕获子类的错误，再捕获父类的错误，代码写法可以改为：</p>
<figure class="highlight py"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">from</span> urllib <span class="keyword">import</span> request,error</span><br><span class="line"><span class="keyword">try</span> :</span><br><span class="line">    response = request.urlopen(<span class="string">&#x27;https://cuiqingcai.com/404&#x27;</span>)</span><br><span class="line"><span class="keyword">except</span> error.HTTPError <span class="keyword">as</span> e:</span><br><span class="line">    <span class="built_in">print</span>(e.reason,e.code,e.headers,sep=<span class="string">&#x27;\n&#x27;</span>)</span><br><span class="line"><span class="keyword">except</span> error.URLError <span class="keyword">as</span> e:</span><br><span class="line">    <span class="built_in">print</span>(e.reason)</span><br><span class="line"><span class="keyword">else</span>:</span><br><span class="line">    <span class="built_in">print</span>(<span class="string">&quot;Request succeeded&quot;</span>)</span><br></pre></td></tr></table></figure>
<p>这样可以先捕获HTTPError，捕获它的错误原因、状态码、请求头信息。如果不是HTTPError异常，就会捕获URLError异常，输出错误原因。最后用else语句来处理正常的逻辑。<br>有时reason属性返回的不一定是字符串，也可能是一个对象。</p>
<figure class="highlight py"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> socket</span><br><span class="line"><span class="keyword">import</span> urllib.request</span><br><span class="line"><span class="keyword">import</span> urllib.error</span><br><span class="line"><span class="keyword">try</span> :</span><br><span class="line">    response = urllib.request.urlopen(<span class="string">&#x27;https://cuiqingcai.com/404&#x27;</span>,timeout=<span class="number">0.01</span>)</span><br><span class="line"><span class="keyword">except</span> urllib.error.URLError <span class="keyword">as</span> e:</span><br><span class="line">    <span class="built_in">print</span>(<span class="built_in">type</span>(e.reason))</span><br><span class="line">    <span class="keyword">if</span> <span class="built_in">isinstance</span>(e.reason,socket.timeout):</span><br><span class="line">        <span class="built_in">print</span>(<span class="string">&#x27;TIME OUT&#x27;</span>)</span><br><span class="line">&gt;&gt;&gt;</span><br><span class="line">&lt;<span class="keyword">class</span> <span class="string">&#x27;socket.timeout&#x27;</span>&gt;</span><br><span class="line">TIME OUT</span><br></pre></td></tr></table></figure>
<p>reason的类型是socket.timeout类，这里使用了isinstance方法来判断它的类型。  </p>
<h2 id="解析链接"><a href="#解析链接" class="headerlink" title="解析链接"></a>解析链接</h2><p>urllib库里还提供了parse模块，这个模块定义了处理URL的标准接口，如实现URL各部分的抽取、合并以及转换。它支持如下协议处理：file、ftp、gopher、hdl、http、https、imap、mailto、mms、news、nntp、prospero、rsync、rtsp、rtspu、sftp、sip、sips、snews、svn、svn+ssh、telnet和wais。  </p>
<h3 id="urlparse"><a href="#urlparse" class="headerlink" title="urlparse"></a>urlparse</h3><p>这个方法可以实现url的识别和分段</p>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><span class="line">from urllib.parse import urlparse</span><br><span class="line"></span><br><span class="line">result = urlparse(&#x27;https://www.baidu.com/index.html;user?id=5#comment&#x27;)</span><br><span class="line">print(type(result))</span><br><span class="line">print(result)</span><br><span class="line">&gt;&gt;&gt;</span><br><span class="line">&lt;class &#x27;urllib.parse.ParseResult&#x27;&gt;</span><br><span class="line">ParseResult(scheme=&#x27;https&#x27;, netloc=&#x27;www.baidu.com&#x27;, path=&#x27;/index.html&#x27;, params=&#x27;user&#x27;, query=&#x27;id=5&#x27;, fragment=&#x27;comment&#x27;)</span><br></pre></td></tr></table></figure>
<p>解析的结果是ParseResult类型的对象，包含6部分，分别是scheme、netloc、path、params、query、fragment。<br>观察URL：<code>https://www.baidu.com/index.html;user?id=5#comment</code><br>urlparse方法在解析URL时有特定的分隔符。如<code>://</code>前面的内容就是scheme，代表协议。第一个<code>/</code>符号前面便是netloc，即域名；后面是path，访问路径。分号<code>;</code>后面是params，参数。问号<code>?</code>后面是查询条件query，一般用作GET类型的URL。井号<code>#</code>后面是锚点fragment，用于直接定位页面内部的下拉位置。<br>标准链接格式：<code>scheme://netloc/path;params?quert#fragment</code>，一个标准的URL都会符合这个规则，利用urlparse方法就可以将它拆分开来。<br>urlparse的API用法：<br><code>urllib.parse.urlparse(urlstring,scheme=&#39;&#39;,allow_fragment=True)</code></p>
<ul>
<li>urlstring:<br>这是必填项，即待解析的URL。</li>
<li>scheme:<br>这是默认的协议。如果待解析的URL没有带协议信息，就会将这个作为默认协议。</li>
<li>allow_fragment:<br>是否忽略fragment。如果此项被设置为False，那么fragment部分就会被忽略，它会被解析为path、params或者query的一部分，而fragment部分为空。</li>
</ul>
<h3 id="urlunparse"><a href="#urlunparse" class="headerlink" title="urlunparse"></a>urlunparse</h3><p>此方法用于构造URL。接收的参数是一个可迭代对象，其长度必须是6，否则会抛出参数数量不足或者过多的问题。<br>实例</p>
<figure class="highlight py"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">from</span> urllib.parse <span class="keyword">import</span> urlunparse</span><br><span class="line">data = [<span class="string">&#x27;https&#x27;</span>,<span class="string">&#x27;www.baidu.com&#x27;</span>,<span class="string">&#x27;index.html&#x27;</span>,<span class="string">&#x27;uesr&#x27;</span>,<span class="string">&#x27;a=6&#x27;</span>,<span class="string">&#x27;comment&#x27;</span>]</span><br><span class="line"><span class="built_in">print</span>(urlunparse((data)))</span><br><span class="line">&gt;&gt;&gt;</span><br><span class="line">https://www.baidu.com/index.html;uesr?a=<span class="number">6</span><span class="comment">#comment</span></span><br></pre></td></tr></table></figure>
<p>这里的参数使用的data列表类型。也可以使用其他类型，如元组或特定的数据结构。  </p>
<h3 id="urlsplit"><a href="#urlsplit" class="headerlink" title="urlsplit"></a>urlsplit</h3><p>此方法和urlparse类似，不过它不再单独解析params这部分（params会合并到path中），只返回5个结果。<br>实例</p>
<figure class="highlight py"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">from</span> urllib.parse <span class="keyword">import</span> urlsplit</span><br><span class="line">result = urlsplit(<span class="string">&#x27;https://www.baidu.com/index.html;user?id=5#comment&#x27;</span>)</span><br><span class="line"><span class="built_in">print</span>(result)</span><br><span class="line"><span class="built_in">print</span>(<span class="built_in">type</span>(result))</span><br><span class="line">&gt;&gt;&gt;</span><br><span class="line">SplitResult(scheme=<span class="string">&#x27;https&#x27;</span>, netloc=<span class="string">&#x27;www.baidu.com&#x27;</span>, path=<span class="string">&#x27;/index.html;user&#x27;</span>, query=<span class="string">&#x27;id=5&#x27;</span>, fragment=<span class="string">&#x27;comment&#x27;</span>)</span><br><span class="line">&lt;<span class="keyword">class</span> <span class="string">&#x27;urllib.parse.SplitResult&#x27;</span>&gt;</span><br></pre></td></tr></table></figure>
<p>返回的结果是SplitResult，其实也是元组，可以通过属性取其值，也可以通过索引取值。</p>
<figure class="highlight py"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">from</span> urllib.parse <span class="keyword">import</span> urlsplit</span><br><span class="line">result = urlsplit(<span class="string">&#x27;https://www.baidu.com/index.html;user?id=5#comment&#x27;</span>)</span><br><span class="line"><span class="built_in">print</span>(result.scheme,result[<span class="number">0</span>])</span><br><span class="line">&gt;&gt;&gt;</span><br><span class="line">https https</span><br></pre></td></tr></table></figure>
<h3 id="urlunsplit"><a href="#urlunsplit" class="headerlink" title="urlunsplit"></a>urlunsplit</h3><p>与urlunparse类似，也是将链接各部分组合成完整链接的方法，传入的参数是可迭代对象，如列表，元组，参数长度必须是5。<br>实例</p>
<figure class="highlight py"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">from</span> urllib.parse <span class="keyword">import</span> urlunsplit</span><br><span class="line">data=[<span class="string">&#x27;https&#x27;</span>,<span class="string">&#x27;www.baidu.com&#x27;</span>,<span class="string">&#x27;index.html&#x27;</span>,<span class="string">&#x27;a=6&#x27;</span>,<span class="string">&#x27;comment&#x27;</span>]</span><br><span class="line"><span class="built_in">print</span>(urlunsplit(data))</span><br><span class="line">&gt;&gt;&gt;</span><br><span class="line">https://www.baidu.com/index.html?a=<span class="number">6</span><span class="comment">#comment</span></span><br></pre></td></tr></table></figure>
<h3 id="urljoin"><a href="#urljoin" class="headerlink" title="urljoin"></a>urljoin</h3><p>&emsp;&emsp;urlunparse和urlunsplit方法都可以完成链接的合并，但前提是有特定的对象，链接的每一部分要清晰分开。<br>&emsp;&emsp;urljoin也可以生成链接。要提供base_url(基础链接)作为该方法的第一个参数，将新链接作为第二个参数。urljoin方法会分析base_url的scheme、netloc和path三个内容，并对新链接缺失的部分进行补充，最后返回结果。<br>实例</p>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br></pre></td><td class="code"><pre><span class="line">from urllib.parse import urljoin</span><br><span class="line"></span><br><span class="line">print(urljoin(&#x27;https://www.baidu.com&#x27;, &#x27;FAQ.html&#x27;))</span><br><span class="line">print(urljoin(&#x27;https://www.baidu.com&#x27;, &#x27;https://cuiqingcai.com/FAQ.html&#x27;))</span><br><span class="line">print(urljoin(&#x27;https://www.baidu.com/about.html&#x27;, &#x27;https://cuiqingcai.com/FAQ.html&#x27;))</span><br><span class="line">print(urljoin(&#x27;https://www.baidu.com/about.html&#x27;, &#x27;https://cuiqingcai.com/FAQ.html?question=2&#x27;))</span><br><span class="line">print(urljoin(&#x27;https://www.baidu.com?wd=abc&#x27;, &#x27;https://cuiqingcai.com/index,php&#x27;))</span><br><span class="line">print(urljoin(&#x27;https://www.baidu.com&#x27;, &#x27;?category=2#comment&#x27;))</span><br><span class="line">print(urljoin(&#x27;www.baidu.com&#x27;, &#x27;?category=2#comment&#x27;))</span><br><span class="line">print(urljoin(&#x27;www.baidu.com#comment&#x27;, &#x27;?caetgory=2&#x27;))</span><br><span class="line">&gt;&gt;&gt;</span><br><span class="line">https://www.baidu.com/FAQ.html</span><br><span class="line">https://cuiqingcai.com/FAQ.html</span><br><span class="line">https://cuiqingcai.com/FAQ.html</span><br><span class="line">https://cuiqingcai.com/FAQ.html?question=2</span><br><span class="line">https://cuiqingcai.com/index,php</span><br><span class="line">https://www.baidu.com?category=2#comment</span><br><span class="line">www.baidu.com?category=2#comment</span><br><span class="line">www.baidu.com?caetgory=2</span><br></pre></td></tr></table></figure>
<p>&emsp;&emsp;可以发现，base_url提供了scheme、netloc和path。如果新链接不存在这三项，就进行补充，如果存在，base_url是不起作用的。<br>&emsp;&emsp;通过urljoin方法，实现链接的解析、拼合与生成。  </p>
<h3 id="urlencode"><a href="#urlencode" class="headerlink" title="urlencode"></a>urlencode</h3><p>&emsp;&emsp;它在构造GET请求参数的时候非常有用。<br>实例：</p>
<figure class="highlight py"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">from</span> urllib.parse <span class="keyword">import</span> urlencode</span><br><span class="line">params=&#123;</span><br><span class="line">    <span class="string">&#x27;name&#x27;</span>:<span class="string">&#x27;germy&#x27;</span>,</span><br><span class="line">    <span class="string">&#x27;age&#x27;</span>:<span class="number">25</span></span><br><span class="line">&#125;</span><br><span class="line">base_url = <span class="string">&#x27;http://www.baidu.com?&#x27;</span></span><br><span class="line">url=base_url+urlencode(params)</span><br><span class="line"><span class="built_in">print</span>(url)</span><br><span class="line">&gt;&gt;&gt;</span><br><span class="line">http://www.baidu.com?name=germy&amp;age=<span class="number">25</span></span><br></pre></td></tr></table></figure>
<p>&emsp;&emsp;先声明了一个字典params，用于将参数显示出来，然后调用urlencode方法将params序列化为GET请求的参数。<br>&emsp;&emsp;urlencode方法很常用，有时为了更方便地构造参数。我们会先用字典将参数表示出来，然后将字典转化为URL的参数时，调用该方法即可。  </p>
<h3 id="parse-qs"><a href="#parse-qs" class="headerlink" title="parse_qs"></a>parse_qs</h3><p>&emsp;&emsp;有了序列化就会有反序列化。利用parse_qs方法，可以将一串GET请求参数转回字典。<br>实例：</p>
<figure class="highlight py"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">from</span> urllib.parse <span class="keyword">import</span> parse_qs</span><br><span class="line">query = <span class="string">&#x27;name=germy&amp;age=25&#x27;</span></span><br><span class="line"><span class="built_in">print</span>(parse_qs(query))</span><br><span class="line">&gt;&gt;&gt;</span><br><span class="line">&#123;<span class="string">&#x27;name&#x27;</span>: [<span class="string">&#x27;germy&#x27;</span>], <span class="string">&#x27;age&#x27;</span>: [<span class="string">&#x27;25&#x27;</span>]&#125;</span><br></pre></td></tr></table></figure>
<p>可以看到URL的参数车公共转换为了字典类型。</p>
<blockquote>
<p>查询字符串是 URL 中 ? 符号后的部分，通常用于在 HTTP 请求中传递参数。<br>输出结果会是一个字典，其中每个键对应一个参数名，每个值是一个列表，包含该参数名对应的所有值。  </p>
</blockquote>
<h3 id="parse-qsl"><a href="#parse-qsl" class="headerlink" title="parse_qsl"></a>parse_qsl</h3><p>&emsp;&emsp;此方法用于将参数转化为由元组组成的列表：</p>
<figure class="highlight py"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">from</span> urllib.parse <span class="keyword">import</span> parse_qsl</span><br><span class="line">query = <span class="string">&#x27;name=germy&amp;age=25&#x27;</span></span><br><span class="line"><span class="built_in">print</span>(parse_qsl(query))</span><br><span class="line">&gt;&gt;&gt;</span><br><span class="line">[(<span class="string">&#x27;name&#x27;</span>, <span class="string">&#x27;germy&#x27;</span>), (<span class="string">&#x27;age&#x27;</span>, <span class="string">&#x27;25&#x27;</span>)]</span><br></pre></td></tr></table></figure>
<p>&emsp;&emsp;运行结果是一个列表，该列表中的每一个元素是一个元组，元组第一个内容是参数名，第二个内容是参数值。  </p>
<h3 id="quote"><a href="#quote" class="headerlink" title="quote"></a>quote</h3><p>&emsp;&emsp;此方法可以将内容转化为URL编码的格式。当URL中带有中文参数的时候，有可能导致乱码，使用quote可以将中文字符转化为URL编码。  </p>
<figure class="highlight py"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">from</span> urllib.parse <span class="keyword">import</span> quote</span><br><span class="line">keyword = <span class="string">&#x27;壁纸&#x27;</span></span><br><span class="line">url=<span class="string">&#x27;http://www.baidu.com/s?wd=&#x27;</span>+quote(keyword)</span><br><span class="line"><span class="built_in">print</span>(url)</span><br><span class="line">&gt;&gt;&gt;</span><br><span class="line">http://www.baidu.com/s?wd=%E5%A3%<span class="number">81</span>%E7%BA%B8</span><br></pre></td></tr></table></figure>
<h3 id="unquote"><a href="#unquote" class="headerlink" title="unquote"></a>unquote</h3><p>它可以进行URL解码。<br>实例：</p>
<figure class="highlight py"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">from</span> urllib.parse <span class="keyword">import</span> unquote</span><br><span class="line">url = <span class="string">&#x27;http://www.baidu.com/s?wd=%E5%A3%81%E7%BA%B8&#x27;</span></span><br><span class="line"><span class="built_in">print</span>(unquote(url))</span><br><span class="line">&gt;&gt;&gt;</span><br><span class="line">http://www.baidu.com/s?wd=壁纸</span><br></pre></td></tr></table></figure>
<h2 id="分析Robots协议"><a href="#分析Robots协议" class="headerlink" title="分析Robots协议"></a>分析Robots协议</h2><p>利用urllib库的robotparser模块，可以分析网站的Robots协议。</p>
<h3 id="Robots协议"><a href="#Robots协议" class="headerlink" title="Robots协议"></a>Robots协议</h3><p>&emsp;&emsp;该协议也称爬虫协议或机器人协议，全名是网络爬虫排除标准。来告诉爬虫和搜索引擎哪些页面可以抓取，哪些不可以。通常有一个叫robots.txt的文本文件，一般在网站的根目录下。<br>&emsp;&emsp;爬虫在访问一个站点时，首先检查这个站点的根目录下是否存在robots.txt文件，如果存在，会根据其中定义的爬取范围来爬取。若没有这个文件，搜索爬虫会访问所有可直接访问的页面。  </p>
<h3 id="robotparser"><a href="#robotparser" class="headerlink" title="robotparser"></a>robotparser</h3><p>&emsp;&emsp;使用robotparser模块来解析robots.txt文件。该模块提供了一个RobotFileParser，它可以根据某网站的robots.txt文件判断一个爬取爬虫是否有权限爬取这个网页。<br>&emsp;&emsp;这个类的使用只需在构造方法里传入robots.txt文件的链接即可。<br>&emsp;&emsp;看它的声明<code>urllib.robotparser.RobotFileParser(url=&#39;&#39;)</code><br>&emsp;&emsp;也可以不在声明时传入robots.txt文件的链接，让其默认为空，最后使用set_url()方法去设置也可以。</p>
<ul>
<li>set_url:<br>用来设置robots.txt文件的链接。如果在创建RobotFileParser对象时传入了链接，就不需要使用这个方法设置了。</li>
<li>read:<br>读取robots.txt文件并进行分析。这个方法执行读取和分析操作，如果不调用这个方法，接下来接下来的判断都会为False，所以一定要调用这个方法。这个方法虽然不返回内容，但是执行了读取操作。</li>
<li>parse:<br>用来解析robots.txt文件，传入其中的参数是robots.txt文件中某些行的内容，会按照robots.txt的语法规则来分析这些内容。  </li>
<li>can_fetch:<br>该方法有两个参数，第一个是User-Agent，第二个是要抓取的URL。返回结果是True或False，表示搜索引擎是否可以抓取这个URL。</li>
<li>mtime:<br>返回上次抓取和分析robots.txt文件的时间，这对于长时间分析和抓取robots.txt文件的搜索爬虫很有必要，可能需要定期检查以抓取最新的robots.txt文件。</li>
<li>modified:<br>同样对长时间分析和抓取的搜索爬虫很有帮助，可以将时间设置为上次抓取和分析robots.txt文件的时间。<br>实例  <blockquote>
<p>首先创建一个RobotFileParser对象rp，然后通过set_url方法设置robots.txt文件的链接。利用can_fetch方法判断网页是否可以被抓取。  </p>
</blockquote>
<figure class="highlight py"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">from</span> urllib.robotparser <span class="keyword">import</span> RobotFileParser</span><br><span class="line">rp = RobotFileParser()</span><br><span class="line">rp.set_url(<span class="string">&quot;https://www.baidu.com/robots.txt&quot;</span>)</span><br><span class="line">rp.read()</span><br><span class="line"><span class="built_in">print</span>(rp.can_fetch(<span class="string">&#x27;Baiduspider&#x27;</span>,<span class="string">&#x27;https://www.baidu.com&#x27;</span>))</span><br><span class="line"><span class="built_in">print</span>(rp.can_fetch(<span class="string">&#x27;Baiduspider&#x27;</span>,<span class="string">&#x27;https://www.baidu.com/homepage/&#x27;</span>))</span><br><span class="line"><span class="built_in">print</span>(rp.can_fetch(<span class="string">&#x27;Googlebot&#x27;</span>,<span class="string">&#x27;https://www.baidu.com/homepage/&#x27;</span>))</span><br><span class="line">&gt;&gt;&gt;</span><br><span class="line"><span class="literal">True</span></span><br><span class="line"><span class="literal">True</span></span><br><span class="line"><span class="literal">False</span></span><br></pre></td></tr></table></figure>
&emsp;&emsp;利用Baiduspider就可以抓取百度的首页以及homepage页面，但是Googlebot就不能抓取homepage页面。<br>&emsp;&emsp;还可以用parse方法执行对robots.txt文件的读取和分析。<br>实例<figure class="highlight py"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">from</span> urllib.request <span class="keyword">import</span> urlopen</span><br><span class="line"><span class="keyword">from</span> urllib.robotparser <span class="keyword">import</span> RobotFileParser</span><br><span class="line"></span><br><span class="line">rp = RobotFileParser()</span><br><span class="line">rp.parse(urlopen(<span class="string">&#x27;https://www.baidu.com/robots.txt&#x27;</span>).read().decode(<span class="string">&#x27;utf-8&#x27;</span>).split(<span class="string">&#x27;\n&#x27;</span>))</span><br><span class="line"><span class="built_in">print</span>(rp.can_fetch(<span class="string">&#x27;Baiduspider&#x27;</span>,<span class="string">&#x27;https://www.baidu.com&#x27;</span>))</span><br><span class="line"><span class="built_in">print</span>(rp.can_fetch(<span class="string">&#x27;Baiduspider&#x27;</span>,<span class="string">&#x27;https://www.baidu.com/homepage/&#x27;</span>))</span><br><span class="line"><span class="built_in">print</span>(rp.can_fetch(<span class="string">&#x27;Googlebot&#x27;</span>,<span class="string">&#x27;https://www.baidu.com/homepage/&#x27;</span>))</span><br><span class="line">&gt;&gt;&gt;</span><br><span class="line"><span class="literal">True</span></span><br><span class="line"><span class="literal">True</span></span><br><span class="line"><span class="literal">False</span></span><br></pre></td></tr></table></figure>
运行结果一样</li>
</ul>
<h1 id="requests的使用"><a href="#requests的使用" class="headerlink" title="requests的使用"></a>requests的使用</h1><p>requests相比于urllib更加方便。  </p>
<h2 id="准备"><a href="#准备" class="headerlink" title="准备"></a>准备</h2><p>首先确保安装了requests库，若未安装，使用pip3安装：<br><code>pip3 install requests</code></p>
<h2 id="实例"><a href="#实例" class="headerlink" title="实例"></a>实例</h2><p>&emsp;&emsp;在urllib库中的urlopen方法实际上是以GET请求方式请求网页的，requests库中相应的方法就是get方法</p>
<figure class="highlight py"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> requests</span><br><span class="line"></span><br><span class="line">response = requests.get(<span class="string">&#x27;http://www.baidu.com&#x27;</span>)</span><br><span class="line"><span class="built_in">print</span>(<span class="built_in">type</span>(response))</span><br><span class="line"><span class="built_in">print</span>(response.status_code)</span><br><span class="line"><span class="built_in">print</span>(<span class="built_in">type</span>(response.text))</span><br><span class="line"><span class="built_in">print</span>(response.text[:<span class="number">100</span>])</span><br><span class="line"><span class="built_in">print</span>(response.cookies)</span><br><span class="line">&gt;&gt;&gt;</span><br><span class="line">&lt;<span class="keyword">class</span> <span class="string">&#x27;requests.models.Response&#x27;</span>&gt;</span><br><span class="line"><span class="number">200</span></span><br><span class="line">&lt;<span class="keyword">class</span> <span class="string">&#x27;str&#x27;</span>&gt;</span><br><span class="line">&lt;!DOCTYPE html&gt;&lt;!--STATUS OK--&gt;&lt;html&gt;&lt;head&gt;&lt;meta http-equiv=<span class="string">&quot;Content-Type&quot;</span> content=<span class="string">&quot;text/html;charse</span></span><br><span class="line"><span class="string">&lt;RequestsCookieJar[&lt;Cookie BAIDUID=1C74878EE150BFF62131B72042A1460F:FG=1 for .baidu.com/&gt;, &lt;Cookie BAIDUID_BFESS=1C74878EE150BFF62131B72042A1460F:FG=1 for .baidu.com/&gt;, &lt;Cookie BIDUPSID=1C74878EE150BFF62131B72042A1460F for .baidu.com/&gt;, &lt;Cookie H_PS_PSSID=40304_40499_40446_40080 for .baidu.com/&gt;, &lt;Cookie PSTM=1714996509 for .baidu.com/&gt;]&gt;  </span></span><br></pre></td></tr></table></figure>
<p>&emsp;&emsp;这里调用get方法实现了与urlopen方法相同的操作，返回一个Response对象，将其存放在变量response中，然后输出响应的类型，状态码，响应体的类型，内容，以及Cookie。  </p>
<h2 id="GET请求"><a href="#GET请求" class="headerlink" title="GET请求"></a>GET请求</h2><p>利用requests库构建GET请求的方法。  </p>
<h3 id="基本实例"><a href="#基本实例" class="headerlink" title="基本实例"></a>基本实例</h3><p>&emsp;&emsp;构建简单GET请求，请求链接为<code>https://www.httpbin.org/get</code>，该网站会判断客户端发起的请求是否为get请求，如果是，那么它将返回相应的请求信息：</p>
<figure class="highlight py"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> requests</span><br><span class="line"></span><br><span class="line">url = <span class="string">&#x27;https://www.httpbin.org/get&#x27;</span></span><br><span class="line">r=requests.get(url)</span><br><span class="line"><span class="built_in">print</span>(r.text)</span><br><span class="line">&gt;&gt;&gt;</span><br><span class="line">&#123;</span><br><span class="line">  <span class="string">&quot;args&quot;</span>: &#123;&#125;, </span><br><span class="line">  <span class="string">&quot;headers&quot;</span>: &#123;</span><br><span class="line">    <span class="string">&quot;Accept&quot;</span>: <span class="string">&quot;*/*&quot;</span>, </span><br><span class="line">    <span class="string">&quot;Accept-Encoding&quot;</span>: <span class="string">&quot;gzip, deflate&quot;</span>, </span><br><span class="line">    <span class="string">&quot;Host&quot;</span>: <span class="string">&quot;www.httpbin.org&quot;</span>, </span><br><span class="line">    <span class="string">&quot;User-Agent&quot;</span>: <span class="string">&quot;python-requests/2.31.0&quot;</span>, </span><br><span class="line">    <span class="string">&quot;X-Amzn-Trace-Id&quot;</span>: <span class="string">&quot;Root=1-6638c88c-656171973be54b8465b565d1&quot;</span></span><br><span class="line">  &#125;, </span><br><span class="line">  <span class="string">&quot;origin&quot;</span>: <span class="string">&quot;219.156.133.195&quot;</span>, </span><br><span class="line">  <span class="string">&quot;url&quot;</span>: <span class="string">&quot;https://www.httpbin.org/get&quot;</span></span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>
<p>&emsp;&emsp;成功发起了GET请求，返回结果中包含请求头、URL、IP等信息。<br>&emsp;&emsp;对于GET请求，可以添加附加信息，如加两个参数name和age，其中name是germy、age是25，那么URL就可以写为<code>https://www.httpbin.org/get?name=germy&amp;age=25</code>，但是这样写稍有麻烦，其实可以利用params参数直接传递这种信息：</p>
<figure class="highlight py"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> requests</span><br><span class="line">data=&#123;</span><br><span class="line">    <span class="string">&#x27;name&#x27;</span>:<span class="string">&#x27;germy&#x27;</span>,</span><br><span class="line">    <span class="string">&#x27;age&#x27;</span>:<span class="number">25</span></span><br><span class="line">&#125;</span><br><span class="line">url = <span class="string">&#x27;https://www.httpbin.org/get&#x27;</span></span><br><span class="line">response = requests.get(url, params=data)</span><br><span class="line"><span class="built_in">print</span>(response.text)</span><br><span class="line">&gt;&gt;&gt;</span><br><span class="line">&#123;</span><br><span class="line">  <span class="string">&quot;args&quot;</span>: &#123;</span><br><span class="line">    <span class="string">&quot;age&quot;</span>: <span class="string">&quot;25&quot;</span>, </span><br><span class="line">    <span class="string">&quot;name&quot;</span>: <span class="string">&quot;germy&quot;</span></span><br><span class="line">  &#125;, </span><br><span class="line">  <span class="string">&quot;headers&quot;</span>: &#123;</span><br><span class="line">    <span class="string">&quot;Accept&quot;</span>: <span class="string">&quot;*/*&quot;</span>, </span><br><span class="line">    <span class="string">&quot;Accept-Encoding&quot;</span>: <span class="string">&quot;gzip, deflate&quot;</span>, </span><br><span class="line">    <span class="string">&quot;Host&quot;</span>: <span class="string">&quot;www.httpbin.org&quot;</span>, </span><br><span class="line">    <span class="string">&quot;User-Agent&quot;</span>: <span class="string">&quot;python-requests/2.31.0&quot;</span>, </span><br><span class="line">    <span class="string">&quot;X-Amzn-Trace-Id&quot;</span>: <span class="string">&quot;Root=1-6638ca91-53eeec34391520830e2e1c8a&quot;</span></span><br><span class="line">  &#125;, </span><br><span class="line">  <span class="string">&quot;origin&quot;</span>: <span class="string">&quot;219.156.133.195&quot;</span>, </span><br><span class="line">  <span class="string">&quot;url&quot;</span>: <span class="string">&quot;https://www.httpbin.org/get?name=germy&amp;age=25&quot;</span></span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>
<p>&emsp;&emsp;这样就把URL参数以字典的形式传给get方法的params参数，通过返回信息可以判断，请求链接自动被构造成了<code>https://www.httpbin.org/get?name=germy&amp;age=25</code>,就不用自己构造URL了。<br>&emsp;&emsp;网页返回的类型虽然是str类型，但是它很特殊，是JSON格式的。如果想直接解析返回结果，得到一个JSON格式的数据，可以直接调用json方法：</p>
<figure class="highlight py"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> requests</span><br><span class="line">data=&#123;</span><br><span class="line">    <span class="string">&#x27;name&#x27;</span>:<span class="string">&#x27;germy&#x27;</span>,</span><br><span class="line">    <span class="string">&#x27;age&#x27;</span>:<span class="number">25</span></span><br><span class="line">&#125;</span><br><span class="line">url = <span class="string">&#x27;https://www.httpbin.org/get&#x27;</span></span><br><span class="line">response = requests.get(url, params=data)</span><br><span class="line"><span class="built_in">print</span>(<span class="built_in">type</span>(response.json()))</span><br><span class="line"><span class="built_in">print</span>(response.json())</span><br><span class="line">&gt;&gt;&gt;</span><br><span class="line">&lt;<span class="keyword">class</span> <span class="string">&#x27;dict&#x27;</span>&gt;</span><br><span class="line">&#123;<span class="string">&#x27;args&#x27;</span>: &#123;<span class="string">&#x27;age&#x27;</span>: <span class="string">&#x27;25&#x27;</span>, <span class="string">&#x27;name&#x27;</span>: <span class="string">&#x27;germy&#x27;</span>&#125;, <span class="string">&#x27;headers&#x27;</span>: &#123;<span class="string">&#x27;Accept&#x27;</span>: <span class="string">&#x27;*/*&#x27;</span>, <span class="string">&#x27;Accept-Encoding&#x27;</span>: <span class="string">&#x27;gzip, deflate&#x27;</span>, <span class="string">&#x27;Host&#x27;</span>: <span class="string">&#x27;www.httpbin.org&#x27;</span>, <span class="string">&#x27;User-Agent&#x27;</span>: <span class="string">&#x27;python-requests/2.31.0&#x27;</span>, <span class="string">&#x27;X-Amzn-Trace-Id&#x27;</span>: <span class="string">&#x27;Root=1-6638cd0c-14525f1e05304540064c24db&#x27;</span>&#125;, <span class="string">&#x27;origin&#x27;</span>: <span class="string">&#x27;219.156.133.195&#x27;</span>, <span class="string">&#x27;url&#x27;</span>: <span class="string">&#x27;https://www.httpbin.org/get?name=germy&amp;age=25&#x27;</span>&#125;</span><br></pre></td></tr></table></figure>
<p>&emsp;&emsp;调用json方法可以将结果（JSON格式字符串）转化为字典。<br>&emsp;&emsp;但是如果返回结果不是JSON格式，就会出现解析错误，出现异常。</p>
<blockquote>
<p>对于上面的请求得到的响应，如果直接用print将响应的对象打印输出，得到的会是什么，我试了下，请求成功<code>&lt;Response [200]&gt;</code>，请求失败<code>&lt;Response [404]&gt;</code></p>
</blockquote>
<h3 id="抓取网页"><a href="#抓取网页" class="headerlink" title="抓取网页"></a>抓取网页</h3><p>以<code>https://ssr1.scrape.center</code>为例，往代码里加一点提取信息的逻辑：</p>
<figure class="highlight py"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> re</span><br><span class="line"><span class="keyword">import</span> requests</span><br><span class="line"></span><br><span class="line">url = <span class="string">&#x27;https://ssr1.scrape.center&#x27;</span></span><br><span class="line"></span><br><span class="line">response = requests.get(url)</span><br><span class="line">pattern = re.<span class="built_in">compile</span>(<span class="string">&#x27;&lt;h2.*?&gt;(.*?)&lt;/h2&gt;&#x27;</span>,re.S)</span><br><span class="line">titles = re.findall(pattern,response.text)</span><br><span class="line"><span class="built_in">print</span>(titles)</span><br><span class="line">&gt;&gt;&gt;</span><br><span class="line">[<span class="string">&#x27;霸王别姬 - Farewell My Concubine&#x27;</span>, <span class="string">&#x27;这个杀手不太冷 - Léon&#x27;</span>, <span class="string">&#x27;肖申克的救赎 - The Shawshank Redemption&#x27;</span>, <span class="string">&#x27;泰坦尼克号 - Titanic&#x27;</span>, <span class="string">&#x27;罗马假日 - Roman Holiday&#x27;</span>, <span class="string">&#x27;唐伯虎点秋香 - Flirting Scholar&#x27;</span>, <span class="string">&#x27;乱世佳人 - Gone with the Wind&#x27;</span>, <span class="string">&#x27;喜剧之王 - The King of Comedy&#x27;</span>, <span class="string">&#x27;楚门的世界 - The Truman Show&#x27;</span>, <span class="string">&#x27;狮子王 - The Lion King&#x27;</span>]</span><br></pre></td></tr></table></figure>
<p>&emsp;&emsp;这里提取出了所有的电影标题，只需一个最基本的抓取和提取流程即可完成。  </p>
<h3 id="抓取二进制数据"><a href="#抓取二进制数据" class="headerlink" title="抓取二进制数据"></a>抓取二进制数据</h3><p>&emsp;&emsp;上面例子抓取的是网站页面，返回的是HTML文档。<br>而图片、音频、视频这些文件本质上都是由二进制编码组成的，有特定的保存格式和对应的解析方式，我们能看到各种形色的多媒体，抓取他们要拿到他们的二进制数。  </p>
<figure class="highlight py"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> requests</span><br><span class="line">url = <span class="string">&#x27;https://scrape.center/favicon.ico&#x27;</span></span><br><span class="line">response = requests.get(url)</span><br><span class="line"><span class="built_in">print</span>(response.text)</span><br><span class="line"><span class="built_in">print</span>(response.content)</span><br></pre></td></tr></table></figure>
<p>&emsp;&emsp;这里<code>response.text</code>的运行结果是一串乱码，而<code>response.content</code>的运行结果是bytes类型的数据。<br><img src="/2024/04/28/%E5%9F%BA%E6%9C%AC%E5%BA%93%E7%9A%84%E4%BD%BF%E7%94%A8/9.png">    </p>
<blockquote>
<p>bytes类型的数据前面会带有一个b，也称为“字节串”的格式，字节串是由一系列字节组成的，每个字节可以表示一个字符或者数据的一部分。在Python中，字节串通常用单引号或双引号括起来，并且字节串中的每个字节用两个十六进制数表示。  </p>
</blockquote>
<p>&emsp;&emsp;可以将图片的二进制数据保存下来。  </p>
<figure class="highlight py"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> requests</span><br><span class="line">url = <span class="string">&#x27;https://scrape.center/favicon.ico&#x27;</span></span><br><span class="line">response = requests.get(url)</span><br><span class="line"><span class="keyword">with</span> <span class="built_in">open</span>(<span class="string">&#x27;favicon.ico&#x27;</span>, <span class="string">&#x27;wb&#x27;</span>) <span class="keyword">as</span> f:</span><br><span class="line">    f.write(response.content)</span><br></pre></td></tr></table></figure>
<p>&emsp;&emsp;使用open方法，以二进制写的形式打开文件，向文件里写入二进制数据。</p>
<h3 id="添加请求头"><a href="#添加请求头" class="headerlink" title="添加请求头"></a>添加请求头</h3><p>&emsp;&emsp;在发起HTTP请求的时候，会有一个请求头Request Headers，我们可以通过设置headers参数完成。<br>&emsp;&emsp;上面的实例中，是没有设置请求头信息的，某些网站会发现这不是一个正常浏览器发起的请求，可能返回异常结果，导致网页抓取失败。<br>&emsp;&emsp;添加请求信息，如添加一个User-Agent字段：</p>
<figure class="highlight py"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> requests</span><br><span class="line">headers=&#123;</span><br><span class="line">    <span class="string">&#x27;User-Agent&#x27;</span>: <span class="string">&#x27;Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/124.0.0.0 Safari/537.36&#x27;</span></span><br><span class="line">&#125;</span><br><span class="line">url = <span class="string">&#x27;https://ssr1.scrape.center/&#x27;</span></span><br><span class="line">response = requests.get(url, headers=headers)</span><br><span class="line"><span class="built_in">print</span>(response.text)</span><br></pre></td></tr></table></figure>
<p>&emsp;&emsp;也可以在headers参数中添加其他字段信息。  </p>
<h2 id="POST请求"><a href="#POST请求" class="headerlink" title="POST请求"></a>POST请求</h2><p>实例</p>
<figure class="highlight py"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> requests</span><br><span class="line">data=&#123;</span><br><span class="line">    <span class="string">&#x27;name&#x27;</span>:<span class="string">&#x27;admin&#x27;</span>,</span><br><span class="line">    <span class="string">&#x27;age&#x27;</span>:<span class="number">25</span></span><br><span class="line">&#125;</span><br><span class="line">url = <span class="string">&#x27;https://httpbin.org/post&#x27;</span></span><br><span class="line">response = requests.post(url,data=data)</span><br><span class="line"><span class="built_in">print</span>(response.text)</span><br><span class="line">&gt;&gt;&gt;</span><br><span class="line">&#123;</span><br><span class="line">  <span class="string">&quot;args&quot;</span>: &#123;&#125;, </span><br><span class="line">  <span class="string">&quot;data&quot;</span>: <span class="string">&quot;&quot;</span>, </span><br><span class="line">  <span class="string">&quot;files&quot;</span>: &#123;&#125;, </span><br><span class="line">  <span class="string">&quot;form&quot;</span>: &#123;</span><br><span class="line">    <span class="string">&quot;age&quot;</span>: <span class="string">&quot;25&quot;</span>, </span><br><span class="line">    <span class="string">&quot;name&quot;</span>: <span class="string">&quot;admin&quot;</span></span><br><span class="line">  &#125;, </span><br><span class="line">  <span class="string">&quot;headers&quot;</span>: &#123;</span><br><span class="line">    <span class="string">&quot;Accept&quot;</span>: <span class="string">&quot;*/*&quot;</span>, </span><br><span class="line">    <span class="string">&quot;Accept-Encoding&quot;</span>: <span class="string">&quot;gzip, deflate&quot;</span>, </span><br><span class="line">    <span class="string">&quot;Content-Length&quot;</span>: <span class="string">&quot;17&quot;</span>, </span><br><span class="line">    <span class="string">&quot;Content-Type&quot;</span>: <span class="string">&quot;application/x-www-form-urlencoded&quot;</span>, </span><br><span class="line">    <span class="string">&quot;Host&quot;</span>: <span class="string">&quot;httpbin.org&quot;</span>, </span><br><span class="line">    <span class="string">&quot;User-Agent&quot;</span>: <span class="string">&quot;python-requests/2.31.0&quot;</span>, </span><br><span class="line">    <span class="string">&quot;X-Amzn-Trace-Id&quot;</span>: <span class="string">&quot;Root=1-6639927b-00d275d24c6775e71e150ebb&quot;</span></span><br><span class="line">  &#125;, </span><br><span class="line">  <span class="string">&quot;json&quot;</span>: null, </span><br><span class="line">  <span class="string">&quot;origin&quot;</span>: <span class="string">&quot;219.156.133.195&quot;</span>, </span><br><span class="line">  <span class="string">&quot;url&quot;</span>: <span class="string">&quot;https://httpbin.org/post&quot;</span></span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>
<p>form部分就是提交的数据，证明POST请求成功发送。  </p>
<h2 id="响应"><a href="#响应" class="headerlink" title="响应"></a>响应</h2><p>&emsp;&emsp;请求发送后会得到响应。上面实例中，用的是text和content获取了响应的内容，还有很多属性<br>和方法用来获取其他信息，如状态码、响应头、Cookie等。<br>实例</p>
<figure class="highlight py"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> requests</span><br><span class="line">url = (<span class="string">&#x27;http://ssr1.scrape.center/&#x27;</span>)</span><br><span class="line">response = requests.get(url)</span><br><span class="line"><span class="built_in">print</span>(<span class="built_in">type</span>(response.status_code),response.status_code)  //状态码</span><br><span class="line"><span class="built_in">print</span>(<span class="built_in">type</span>(response.headers),response.headers)  //响应头</span><br><span class="line"><span class="built_in">print</span>(<span class="built_in">type</span>(response.cookies),response.cookies)  //cookies</span><br><span class="line"><span class="built_in">print</span>(<span class="built_in">type</span>(response.url),response.url)  //URL</span><br><span class="line"><span class="built_in">print</span>(<span class="built_in">type</span>(response.history),response.history)  //请求历史</span><br><span class="line">&gt;&gt;&gt;</span><br><span class="line">&lt;<span class="keyword">class</span> <span class="string">&#x27;int&#x27;</span>&gt; <span class="number">200</span></span><br><span class="line">&lt;<span class="keyword">class</span> <span class="string">&#x27;requests.structures.CaseInsensitiveDict&#x27;</span>&gt; &#123;<span class="string">&#x27;Date&#x27;</span>: <span class="string">&#x27;Tue, 07 May 2024 02:40:47 GMT&#x27;</span>, <span class="string">&#x27;Content-Type&#x27;</span>: <span class="string">&#x27;text/html; charset=utf-8&#x27;</span>, <span class="string">&#x27;X-Frame-Options&#x27;</span>: <span class="string">&#x27;DENY&#x27;</span>, <span class="string">&#x27;X-Content-Type-Options&#x27;</span>: <span class="string">&#x27;nosniff&#x27;</span>, <span class="string">&#x27;Expires&#x27;</span>: <span class="string">&#x27;Tue, 07 May 2024 02:45:59 GMT&#x27;</span>, <span class="string">&#x27;Strict-Transport-Security&#x27;</span>: <span class="string">&#x27;max-age=15724800; includeSubDomains&#x27;</span>, <span class="string">&#x27;Server&#x27;</span>: <span class="string">&#x27;Lego Server&#x27;</span>, <span class="string">&#x27;X-Cache-Lookup&#x27;</span>: <span class="string">&#x27;Cache Miss, Cache Miss&#x27;</span>, <span class="string">&#x27;Cache-Control&#x27;</span>: <span class="string">&#x27;max-age=600&#x27;</span>, <span class="string">&#x27;Age&#x27;</span>: <span class="string">&#x27;0&#x27;</span>, <span class="string">&#x27;Content-Length&#x27;</span>: <span class="string">&#x27;41667&#x27;</span>, <span class="string">&#x27;X-NWS-LOG-UUID&#x27;</span>: <span class="string">&#x27;10330054940065279203&#x27;</span>, <span class="string">&#x27;Connection&#x27;</span>: <span class="string">&#x27;keep-alive&#x27;</span>&#125;</span><br><span class="line">&lt;<span class="keyword">class</span> <span class="string">&#x27;requests.cookies.RequestsCookieJar&#x27;</span>&gt; &lt;RequestsCookieJar[]&gt;</span><br><span class="line">&lt;<span class="keyword">class</span> <span class="string">&#x27;str&#x27;</span>&gt; https://ssr1.scrape.center/</span><br><span class="line">&lt;<span class="keyword">class</span> <span class="string">&#x27;list&#x27;</span>&gt; [&lt;Response [<span class="number">302</span>]&gt;]</span><br></pre></td></tr></table></figure>
<p>&emsp;&emsp;这里面headers和cookies这两个属性的结果分别是<code>CaseInsensitiveDict</code>和<code>RequestsCookieJar</code>类型的对象。<br>&emsp;&emsp;状态码<code>200</code>表示响应没有问题，而我们可以通过判断这个数字来确认爬虫是否爬取成功。<br>&emsp;&emsp;requests库还提供了一个内置的状态码查询对象requests.codes<br>实例</p>
<figure class="highlight py"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> requests</span><br><span class="line">url = <span class="string">&#x27;https://ssr1.scrape.center/&#x27;</span></span><br><span class="line">response = requests.get(url)</span><br><span class="line">exit() <span class="keyword">if</span> <span class="keyword">not</span> response.status_code == requests.codes.ok <span class="keyword">else</span> <span class="built_in">print</span>(<span class="string">&#x27;Request Successfully&#x27;</span>)</span><br><span class="line">&gt;&gt;&gt;</span><br><span class="line">Request Successfully</span><br></pre></td></tr></table></figure>
<p>&emsp;&emsp;通过比较返回码和内置的表示成功的状态码，保证得到了正常响应，如果是就输出请求成功的消息，否则程序终止运行，用requests.codes.ok得到的成功状态码是200。  </p>
<h2 id="高级用法-1"><a href="#高级用法-1" class="headerlink" title="高级用法"></a>高级用法</h2><h3 id="文件上传"><a href="#文件上传" class="headerlink" title="文件上传"></a>文件上传</h3><p>&emsp;&emsp;requests库可以模拟提交一些数据。也可以上传文件。<br>实例  </p>
<figure class="highlight py"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> requests</span><br><span class="line">url = <span class="string">&#x27;https://httpbin.org/post&#x27;</span></span><br><span class="line">file = &#123;<span class="string">&#x27;favicon.ico&#x27;</span>: <span class="built_in">open</span>(<span class="string">&#x27;favicon.ico&#x27;</span>,<span class="string">&#x27;rb&#x27;</span>)&#125;</span><br><span class="line">reponse = requests.post(url, files=file)</span><br><span class="line"><span class="built_in">print</span>(reponse.text)</span><br><span class="line">&gt;&gt;&gt;</span><br><span class="line">&#123;</span><br><span class="line">  <span class="string">&quot;args&quot;</span>: &#123;&#125;, </span><br><span class="line">  <span class="string">&quot;data&quot;</span>: <span class="string">&quot;&quot;</span>, </span><br><span class="line">  <span class="string">&quot;files&quot;</span>: &#123;</span><br><span class="line">    <span class="string">&quot;favicon.ico&quot;</span>: <span class="string">&quot;data:application/octet-stream;base64,AAAB...&quot;</span></span><br><span class="line">  &#125;, </span><br><span class="line">  <span class="string">&quot;form&quot;</span>: &#123;&#125;, </span><br><span class="line">  <span class="string">&quot;headers&quot;</span>: &#123;</span><br><span class="line">    <span class="string">&quot;Accept&quot;</span>: <span class="string">&quot;*/*&quot;</span>, </span><br><span class="line">    <span class="string">&quot;Accept-Encoding&quot;</span>: <span class="string">&quot;gzip, deflate&quot;</span>, </span><br><span class="line">    <span class="string">&quot;Content-Length&quot;</span>: <span class="string">&quot;4440&quot;</span>, </span><br><span class="line">    <span class="string">&quot;Content-Type&quot;</span>: <span class="string">&quot;multipart/form-data; boundary=5dc8b6473c0c05dc184c55876cebce25&quot;</span>, </span><br><span class="line">    <span class="string">&quot;Host&quot;</span>: <span class="string">&quot;httpbin.org&quot;</span>, </span><br><span class="line">    <span class="string">&quot;User-Agent&quot;</span>: <span class="string">&quot;python-requests/2.31.0&quot;</span>, </span><br><span class="line">    <span class="string">&quot;X-Amzn-Trace-Id&quot;</span>: <span class="string">&quot;Root=1-66399eb4-6599df655be9cf0a6a9ac001&quot;</span></span><br><span class="line">  &#125;, </span><br><span class="line">  <span class="string">&quot;json&quot;</span>: null, </span><br><span class="line">  <span class="string">&quot;origin&quot;</span>: <span class="string">&quot;219.156.133.195&quot;</span>, </span><br><span class="line">  <span class="string">&quot;url&quot;</span>: <span class="string">&quot;https://httpbin.org/post&quot;</span></span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>
<p>&emsp;&emsp;上传文件后，网站返回响应，响应中包含files字段和form字段，而form字段是空的，说明文件上传部分会单独用一个files字段来标识。  </p>
<h3 id="Cookie设置"><a href="#Cookie设置" class="headerlink" title="Cookie设置"></a>Cookie设置</h3><p>&emsp;&emsp;前面使用过urllib库处理Cookie，写法较为复杂，而用requests库，获取和设置Cookie只需一部即可完成。<br>实例</p>
<figure class="highlight py"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> requests</span><br><span class="line">url = <span class="string">&#x27;https://www.baidu.com/&#x27;</span></span><br><span class="line">response = requests.get(url)</span><br><span class="line"><span class="built_in">print</span>(response.cookies)</span><br><span class="line"><span class="keyword">for</span> key, value <span class="keyword">in</span> response.cookies.items():</span><br><span class="line">    <span class="built_in">print</span>(key+<span class="string">&#x27;=&#x27;</span>+value)</span><br><span class="line">&gt;&gt;&gt;</span><br><span class="line">&lt;RequestsCookieJar[&lt;Cookie BAIDUID=0F50E1FD0E238874BD72A86D327F65CF:FG=<span class="number">1</span> <span class="keyword">for</span> .baidu.com/&gt;, &lt;Cookie BAIDUID_BFESS=0F50E1FD0E238874BD72A86D327F65CF:FG=<span class="number">1</span> <span class="keyword">for</span> .baidu.com/&gt;, &lt;Cookie H_PS_PSSID=<span class="number">40300_40079_40463_60174</span> <span class="keyword">for</span> .baidu.com/&gt;, &lt;Cookie PSTM=<span class="number">1715055521</span> <span class="keyword">for</span> .baidu.com/&gt;, &lt;Cookie BD_NOT_HTTPS=<span class="number">1</span> <span class="keyword">for</span> www.baidu.com/&gt;]&gt;</span><br><span class="line">BAIDUID=070B4458EAF888B767A06FC7F52B6DE1:FG=<span class="number">1</span></span><br><span class="line">BAIDUID_BFESS=070B4458EAF888B767A06FC7F52B6DE1:FG=<span class="number">1</span></span><br><span class="line">H_PS_PSSID=<span class="number">40303_40080_60174</span></span><br><span class="line">PSTM=<span class="number">1715054787</span></span><br><span class="line">BD_NOT_HTTPS=<span class="number">1</span></span><br></pre></td></tr></table></figure>
<p>&emsp;&emsp;这里先调用cookies属性，成功得到Cookie，发现它属于RequestsCookieJar类型。然后调用items方法将Cookie转化为由元组组成的列表，遍历输出每一个Cookie条目的名称和值，实现对Cookie的遍历和解析。<br>&emsp;&emsp;我们也可以直接用Cookie来维持登录状态。以Github为例，我们先登录Github,然后将请求头中的Cookie内容复制下来<br><img src="/2024/04/28/%E5%9F%BA%E6%9C%AC%E5%BA%93%E7%9A%84%E4%BD%BF%E7%94%A8/10.png"><br>&emsp;&emsp;将Cookie添加到请求头里，然后发送请求。<br>实例  </p>
<figure class="highlight py"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> requests</span><br><span class="line">headers = &#123;</span><br><span class="line">    <span class="string">&#x27;Cookie&#x27;</span>: <span class="string">&#x27;_octo=GH1.1.1020779121.1709874774; _device_id=e5c616e3c9807057e79266383da8cd61; saved_user_sessions=155144413%3AetQxEGfBoTvbxcQPOuPZ0WyHSskj5GsMfXA2k8JqGVOz4pv0; user_session=etQxEGfBoTvbxcQPOuPZ0WyHSskj5GsMfXA2k8JqGVOz4pv0; __Host-user_session_same_site=etQxEGfBoTvbxcQPOuPZ0WyHSskj5GsMfXA2k8JqGVOz4pv0; logged_in=yes; dotcom_user=blttttt; has_recent_activity=1; color_mode=%7B%22color_mode%22%3A%22auto%22%2C%22light_theme%22%3A%7B%22name%22%3A%22light%22%2C%22color_mode%22%3A%22light%22%7D%2C%22dark_theme%22%3A%7B%22name%22%3A%22dark%22%2C%22color_mode%22%3A%22dark%22%7D%7D; preferred_color_mode=light; tz=Asia%2FShanghai; _gh_sess=cscvLFSGJcf6AUPq6fzQjoyyl0O89JlmxwVkS1%2BaEXzwsS5bDp%2FKxbAGbdkfKGN2PeDMiLcD2%2BnXCaJiYkjRjAncjALoQ0yR0XMFkNlI08mU2uBAY%2BO0eHJ78vnRiiRutYVCkBPJAQF0oKQ8P46vwW1esYkG3X7Hfs2scPXyDm5sUkW0AC%2B%2B4Xg4yyjQ2VMD96gSpB0SUk8CMugT4GaYSQ4jWzSys%2BffoehUzl0fC76EAhvzT7X7fw5Oa4c7NKoGSz4xn6DXMl8ZaoK%2F7a32i%2FPwYplpfpcd8k%2By36Q%2F533m23P8dVyB3SMO2UZRdIFTUbTi7wFomjcSvbIFJ8lW%2BNMtuYrrz9TZ8X90K1gdDzi4KcmCSXzXcbY25E%2BDMlco%2BmfPtPPax1%2BIaoGu5HYTpM2IAkRDheNvdzHE4pbBPQbdJlx4p6V1R9MWYVm3iKM1Jx6gkPfm92%2BcbznQ4AItn4%2BV40z7Aj4QnV7gG6c8lkyeOhRjVls9s%2F%2BrhDwkHMPjxDpRzruGN6YEPo%2F7X8gt4uIspHA0d4z%2BnNbR1mloiPXw%2B1EJ6gqCog%3D%3D--8YfmSu%2BHtVUuakSu--kppLpsZaDcPEkIPLocCpMA%3D%3D&#x27;</span></span><br><span class="line">&#125;</span><br><span class="line">url = (<span class="string">&#x27;https://github.com/&#x27;</span>)</span><br><span class="line">response = requests.get(url,headers=headers)</span><br><span class="line"><span class="built_in">print</span>(response.text)</span><br></pre></td></tr></table></figure>
<p>&emsp;&emsp;结果中包含了登录才能有的信息，其中包含用户名信息。<br>&emsp;&emsp;得到此结果说明Cookie成功模拟了登录状态，这样就能爬取登录之后才能看到的页面了。<br>&emsp;&emsp;也可以通过cookies参数来设置Cookie的信息，这里构造一个RequestsCookieJar对象，然后对刚才复制的Cookie进行处理以及赋值。<br>实例</p>
<figure class="highlight py"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> requests</span><br><span class="line">cookies = <span class="string">&#x27;_octo=GH1.1.1020779121.1709874774; _device_id=e5c616e3c9807057e79266383da8cd61; saved_user_sessions=155144413%3AetQxEGfBoTvbxcQPOuPZ0WyHSskj5GsMfXA2k8JqGVOz4pv0; user_session=etQxEGfBoTvbxcQPOuPZ0WyHSskj5GsMfXA2k8JqGVOz4pv0; __Host-user_session_same_site=etQxEGfBoTvbxcQPOuPZ0WyHSskj5GsMfXA2k8JqGVOz4pv0; logged_in=yes; dotcom_user=blttttt; has_recent_activity=1; color_mode=%7B%22color_mode%22%3A%22auto%22%2C%22light_theme%22%3A%7B%22name%22%3A%22light%22%2C%22color_mode%22%3A%22light%22%7D%2C%22dark_theme%22%3A%7B%22name%22%3A%22dark%22%2C%22color_mode%22%3A%22dark%22%7D%7D; preferred_color_mode=light; tz=Asia%2FShanghai; _gh_sess=blrf5T4EJEg6JJ50EPrvPOFUhj0hBcd1MbdCCIQc8MS8Fe03V4WSkKIGvUY08G9m%2B4kh1Ir1%2F5fjayGfZFgwiz34xSZrtbWxNO6%2BVWD8yVdBxZYwmE%2ByX1ckwBVTlhVzWjuIg4cRuFc7Cf2a%2BEjNyWZ1SGWiEFqkBI4%2BNBFFWjySrCv8%2Bprb%2FGE6PYp5yMj5ErqhtrIprbxYYLeZujlF2xZKEtO5HEE%2FML%2BkhPd3zelGQ1xfsQCDZCtGMKzBl096OovkfZGd2neqwvadIN5BFbtXr%2F0Qrg0l5%2FgKZivdxH0FcFWjbbh6vBYC1Wdq6ddLAmtoK5tkzf65szPCym4wJWK8BHH5ui1g6gR2jCrKggtk7XDYJ%2FgfRdRFFhOFodqyUTAwliztTaEUqsZx10xvLGSzOa76fWfqN5DOktKMUqEVPi6RwRejzb2hf6CxNQ4fr1kR348GyuyNrQGpAi0%2BFod2%2B7r4VtIRXPmLX%2FhMLZqC4%2BMRiM1S9i3IeQdhj%2BF%2Bu1VBLuvVlg0hKX3xxm449GFW0tBXk345dTu89MLffCFI29X%2BUXV8jA%3D%3D--L4G%2B56Y9nCQAoeYL--nvFWkVjJSzdHJtWrW%2FW%2F4Q%3D%3D&#x27;</span></span><br><span class="line">jar = requests.cookies.RequestsCookieJar()</span><br><span class="line">headers = &#123;</span><br><span class="line">    <span class="string">&#x27;User-Agent&#x27;</span>: <span class="string">&#x27;Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/124.0.0.0 Safari/537.36&#x27;</span></span><br><span class="line">&#125;</span><br><span class="line">url = (<span class="string">&#x27;https://github.com/&#x27;</span>)</span><br><span class="line"></span><br><span class="line"><span class="keyword">for</span> cookie <span class="keyword">in</span> cookies.split(<span class="string">&#x27;;&#x27;</span>):</span><br><span class="line">    key, value = cookie.split(<span class="string">&#x27;=&#x27;</span>,<span class="number">1</span>)   //这里 split 方法的第二个参数 <span class="number">1</span> 表示最多分割一次，确保即使 value 中包含等号 = 也不会继续分割。</span><br><span class="line">    jar.<span class="built_in">set</span>(key, value)</span><br><span class="line">response = requests.get(url,cookies=jar,headers=headers)</span><br><span class="line"><span class="built_in">print</span>(response.text)</span><br></pre></td></tr></table></figure>
<p>&emsp;&emsp;这里首先创建了一个RequestCookieJar对象，然后利用split方法对复制下来的Cookie内容做分割，接着利用set方法设置好每个Cookie条目的键名和键值，最后把RequestCookieJar对象通过cookies参数传递，调用requests库的get方法，即可获取登录后的页面。<br>&emsp;&emsp;也可以正常登录。  </p>
<h3 id="Session维持"><a href="#Session维持" class="headerlink" title="Session维持"></a>Session维持</h3><p>&emsp;&emsp;直接利用request库中的get或post方法的确可以做到模拟网页的请求，但这两种方法实际上相当于不同的Session，或者说是两个浏览器打开了不同的页面。<br>&emsp;&emsp;有一个场景，第一个请求利用requests库的post方法登录了某个网站，第二次想获取成功登录后的自己的个人信息，于是又用了一次requests库的get方法去请求个人信息页面。实际上这相当于打开了两个浏览器，是两个完全独立的操作，对应两个完全不相关Session，并不能获取个人信息。<br>&emsp;&emsp;如果在两次请求时设置一样的Cookie，这样做可以，但是过于繁琐。<br>&emsp;&emsp;解决这个问题的方法是维持同一个Session,也就是第二次请求时打开一个新的浏览器选项卡而不是打开一个新的浏览器。利用Session对象，可以方便地维护一个Session，而且不用担心Cookie的问题，它会自动帮我们处理好。<br>实例：沿用之前写法</p>
<figure class="highlight py"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> requests</span><br><span class="line">requests.get(<span class="string">&#x27;https://httpbin.org/cookies/set/number/123456789&#x27;</span>)</span><br><span class="line">r = requests.get(<span class="string">&#x27;https://httpbin.org/cookies&#x27;</span>)</span><br><span class="line"><span class="built_in">print</span>(r.text)</span><br><span class="line">&gt;&gt;&gt;</span><br><span class="line">&#123;</span><br><span class="line">  <span class="string">&quot;cookies&quot;</span>: &#123;&#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>
<p>&emsp;&emsp;这里请求测试网址<code>https://httpbin.org/cookies/set/number/123456789</code>。在请求时设置了一个Cookie条目，名称是number，内容是123456789。然后请求<code>https://httpbin.org/cookies</code>，获取当前Cookie信息。并不能成功获取设置的Cookie。<br>&emsp;&emsp;然后用Session试试看</p>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br></pre></td><td class="code"><pre><span class="line">import requests</span><br><span class="line">S=requests.Session()</span><br><span class="line">S.get(&#x27;https://httpbin.org/cookies/set/number/123456789&#x27;)</span><br><span class="line">r = S.get(&#x27;https://httpbin.org/cookies&#x27;)</span><br><span class="line">print(r.text)</span><br><span class="line">&gt;&gt;&gt;</span><br><span class="line">&#123;</span><br><span class="line">  &quot;cookies&quot;: &#123;</span><br><span class="line">    &quot;number&quot;: &quot;123456789&quot;</span><br><span class="line">  &#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>
<p>&emsp;&emsp;这样就能获取到设置的Cookie了。  </p>
<h3 id="SSL证书验证"><a href="#SSL证书验证" class="headerlink" title="SSL证书验证"></a>SSL证书验证</h3><p>&emsp;&emsp;现在很多网站要求使用HTTPS协议，但有些网站可能并没有设置好HTTPS证书，或者网站的HTTPS证书可能不被CA机构认可，这些网站就可能出现SSL证书错误的提示。<br>&emsp;&emsp;例如我们访问网站<code>https://ssr2.scrape.center/</code>，如果用Chrome浏览器打开它，会有如下提示：<br><img src="/2024/04/28/%E5%9F%BA%E6%9C%AC%E5%BA%93%E7%9A%84%E4%BD%BF%E7%94%A8/11.png"><br>&emsp;&emsp;其实可以在浏览器中通过一些设置来忽略证书的验证。<br>但如果是使用requests库来请求这类网站</p>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">import requests</span><br><span class="line">url = &#x27;https://ssr2.scrape.center&#x27;</span><br><span class="line">response = requests.get(url)</span><br><span class="line">print(response.text)</span><br></pre></td></tr></table></figure>
<p>&emsp;&emsp;会出现SSLError错误，因为我们请求的URL的证书是无效的。<br>&emsp;&emsp;但如果还是想爬这个网站的话，可以使用verify参数控制是否验证证书，将此参数设置为False，那么在请求时就不会再验证证书是否有效。不设置verify参数，其默认值是True，会自动验证。  </p>
<figure class="highlight py"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> requests</span><br><span class="line">url = <span class="string">&#x27;https://ssr2.scrape.center/&#x27;</span></span><br><span class="line">response = requests.get(url,verify=<span class="literal">False</span>)</span><br><span class="line"><span class="built_in">print</span>(response.status_code)</span><br><span class="line">&gt;&gt;&gt;</span><br><span class="line">E:\JetBrains\python386\lib\site-packages\urllib3\connectionpool.py:<span class="number">1103</span>: InsecureRequestWarning: Unverified HTTPS request <span class="keyword">is</span> being made to host <span class="string">&#x27;ssr2.scrape.center&#x27;</span>. Adding certificate verification <span class="keyword">is</span> strongly advised. See: https://urllib3.readthedocs.io/en/latest/advanced-usage.html<span class="comment">#tls-warnings</span></span><br><span class="line">  warnings.warn(</span><br><span class="line"><span class="number">200</span></span><br></pre></td></tr></table></figure>
<p>&emsp;&emsp;不过有警告，建议我们给它指定证书，我们可以通过设置忽略警告的方式来屏蔽这个警告。  </p>
<figure class="highlight py"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> requests</span><br><span class="line"><span class="keyword">import</span> urllib3</span><br><span class="line">url = <span class="string">&#x27;https://ssr2.scrape.center/&#x27;</span></span><br><span class="line">urllib3.disable_warnings()</span><br><span class="line">response = requests.get(url,verify=<span class="literal">False</span>)</span><br><span class="line"><span class="built_in">print</span>(response.status_code)</span><br><span class="line">&gt;&gt;&gt;</span><br><span class="line"><span class="number">200</span></span><br></pre></td></tr></table></figure>
<p>&emsp;&emsp;或捕获警告到日志的方式忽略警告：</p>
<figure class="highlight py"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> requests</span><br><span class="line"><span class="keyword">import</span> logging</span><br><span class="line">url = <span class="string">&#x27;https://ssr2.scrape.center/&#x27;</span></span><br><span class="line">logging.captureWarnings(<span class="literal">True</span>)</span><br><span class="line">response = requests.get(url,verify=<span class="literal">False</span>)</span><br><span class="line"><span class="built_in">print</span>(response.status_code)</span><br></pre></td></tr></table></figure>
<p>&emsp;&emsp;也可以指定一个本地证书作为客户端证书，可以是单个文件（包含密钥和证书）或一个包含两个文件路径的元组： </p>
<figure class="highlight py"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> requests </span><br><span class="line">response = requests.get(<span class="string">&#x27;https://ssr2.scrape.center/&#x27;</span>,cert=(<span class="string">&#x27;/path/server.crt&#x27;</span>,<span class="string">&#x27;/path/server.key&#x27;</span>))</span><br><span class="line"><span class="built_in">print</span>(response.status_code)</span><br></pre></td></tr></table></figure>
<p>&emsp;&emsp;这里需要有crt和key文件，并指定他们的路径，而且本地私有证书的key必须是解密状态加密状态的key是不支持的。  </p>
<h3 id="超时设置"><a href="#超时设置" class="headerlink" title="超时设置"></a>超时设置</h3><p>&emsp;&emsp;在本机网络状况不好或服务器网络响应太慢甚至无响应时，我们可能会等待特别久的时间才能收到响应，甚至到最后因为接收不到响应而报错。为了防止服务器不能及时响应，可以设置一个超时时间，如果超过了这个时间还没有得到响应，就报错。<br>&emsp;&emsp;这里要用到timeout参数，其值是从发出请求到服务器返回响应的时间。<br>实例  </p>
<figure class="highlight py"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> requests</span><br><span class="line">url = <span class="string">&#x27;https://httpbin.org/get&#x27;</span></span><br><span class="line">response = requests.get(url,timeout=<span class="number">1</span>)</span><br><span class="line"><span class="built_in">print</span>(response.status_code)</span><br><span class="line">&gt;&gt;&gt;</span><br><span class="line"><span class="number">200</span></span><br></pre></td></tr></table></figure>
<p>&emsp;&emsp;如果在相应时间内没有响应就会抛出异常。<br>&emsp;&emsp;请求分为两个阶段：连接(connect)和读取(read)。如果想要分别指定连接和读取的timeout，则可以传入一个元组<code>response = requests.get(url,timeout=(5,30))</code>  </p>
<h3 id="身份认证"><a href="#身份认证" class="headerlink" title="身份认证"></a>身份认证</h3><p>&emsp;&emsp;在访问启用了基本身份认证的网站时，首先会弹出一个认证窗口，如前面遇到的<code>https://ssr3.scrape.center</code>。<br>&emsp;&emsp;可以使用requests库自带的身份认证功能，通过auth参数即可设置。<br>实例</p>
<figure class="highlight py"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> requests</span><br><span class="line"><span class="keyword">from</span> requests.auth <span class="keyword">import</span> HTTPBasicAuth</span><br><span class="line">url = <span class="string">&#x27;https://ssr3.scrape.center&#x27;</span></span><br><span class="line">response = requests.get(url,auth=HTTPBasicAuth(<span class="string">&#x27;admin&#x27;</span>,<span class="string">&#x27;admin&#x27;</span>))</span><br><span class="line"><span class="built_in">print</span>(response.status_code)</span><br><span class="line">&gt;&gt;&gt;</span><br><span class="line"><span class="number">200</span></span><br></pre></td></tr></table></figure>
<p>&emsp;&emsp;这个实例网站的用户名和密码都是admin，这里可以直接设置。<br>&emsp;&emsp;如果用户名和密码正确，那么请求时就会自动认证成功，返回200状态码；如果认证失败，则返回401状态码。<br>&emsp;&emsp;参数都传一个HTTPBasicAuth类，显得有些繁琐，有更简单的写法，直接传一个元组，它会默认使用HTTPBasicAuth这个类来认证。  </p>
<figure class="highlight py"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> requests</span><br><span class="line"></span><br><span class="line">url = <span class="string">&#x27;https://ssr3.scrape.center&#x27;</span></span><br><span class="line">response = requests.get(url,auth=(<span class="string">&#x27;admin&#x27;</span>,<span class="string">&#x27;admin&#x27;</span>))</span><br><span class="line"><span class="built_in">print</span>(response.status_code)</span><br></pre></td></tr></table></figure>
<p>&emsp;&emsp;还有其他认证方式，如OAuth认证，这个需要安装oauth包<code>pip3 install requests_oauthlib</code><br>实例</p>
<figure class="highlight py"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> requests</span><br><span class="line"><span class="keyword">from</span> requests_oauthlib <span class="keyword">import</span> OAuth1</span><br><span class="line"></span><br><span class="line">url = <span class="string">&#x27;https://ssr3.scrape.center&#x27;</span></span><br><span class="line">auth = OAuth1(<span class="string">&#x27;YOUR_API_KEY&#x27;</span>, <span class="string">&#x27;YOUR_API_SECRET&#x27;</span>,</span><br><span class="line">              <span class="string">&#x27;USER_OAUTH_TOKEN&#x27;</span>,<span class="string">&#x27;USER_OAUTH_TOKEN_SECRET&#x27;</span>)</span><br><span class="line">response=requests.get(url,auth=auth)</span><br><span class="line"><span class="built_in">print</span>(response.status_code)</span><br></pre></td></tr></table></figure>
<h3 id="代理设置"><a href="#代理设置" class="headerlink" title="代理设置"></a>代理设置</h3><p>&emsp;&emsp;某些网站在测试的时候请求几次，都能正常获取内容。但是一旦大规模且频繁的请求时，这些网站就可能弹出验证码，或者跳转到登录认证界面，甚至会直接封禁客户端的IP，导致在一定时间内无法访问。<br>&emsp;&emsp;为了防止这种情况发生，我们需要设置代理来解决这个问题，要用到proxies参数。可以用如下方式设置：</p>
<figure class="highlight py"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> requests</span><br><span class="line">proxies = &#123;</span><br><span class="line">    <span class="string">&#x27;http&#x27;</span>: <span class="string">&#x27;http://10.10.10.10:1080&#x27;</span>,</span><br><span class="line">    <span class="string">&#x27;https&#x27;</span>: <span class="string">&#x27;http://10.10.10.10:1080&#x27;</span></span><br><span class="line">&#125;</span><br><span class="line">url = <span class="string">&#x27;https://www.baidu.com&#x27;</span></span><br><span class="line">requests.get(url, proxies=proxies)</span><br></pre></td></tr></table></figure>
<p>&emsp;&emsp;这里直接运行是不可以的，因为这个代理无效，可以找有效的代理替换试验。<br>&emsp;&emsp;如果代理需要使用身份认证，可以使用类似<code>https://user:password@host:port</code>这样的语法来设置代理：</p>
<figure class="highlight py"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> requests</span><br><span class="line">proxies = &#123;</span><br><span class="line">    <span class="string">&#x27;https&#x27;</span>: <span class="string">&#x27;http://user:password@10.10.10.10:1080&#x27;</span>,</span><br><span class="line">&#125;</span><br><span class="line">requests.get(<span class="string">&#x27;https://www.httpbin.org/get&#x27;</span>, proxies=proxies)</span><br></pre></td></tr></table></figure>
<p>&emsp;&emsp;除了基本的HTTP代理外，requsts库还支持SOCKS协议的代理。<br>&emsp;&emsp;首先要安装socks这个库:<code>pip3 install &quot;requests[socks]&quot;</code><br>&emsp;&emsp;然后就可以使用SOCKS协议代理了<br>实例</p>
<figure class="highlight py"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> requests</span><br><span class="line">proxies =&#123;</span><br><span class="line">    <span class="string">&#x27;http&#x27;</span>: <span class="string">&#x27;socks5://user:password@host:port&#x27;</span>,</span><br><span class="line">    <span class="string">&#x27;https&#x27;</span>: <span class="string">&#x27;socks5://user:password@host:&#x27;</span></span><br><span class="line">&#125;</span><br><span class="line">requests.get(<span class="string">&#x27;http://www.httpbin.org/get&#x27;</span>, proxies=proxies)</span><br></pre></td></tr></table></figure>

<h3 id="Prepared-Request"><a href="#Prepared-Request" class="headerlink" title="Prepared Request"></a>Prepared Request</h3><p>&emsp;&emsp;我们可以直接使用requests库的get和post方法的请求，但是这个请求在requests内部是怎么实现的。<br>&emsp;&emsp;实际上，requests在发送请求的时候，是在内部构造了一个Request对象，并给这个对象赋予了各种参数，包括url、headers、data等，然后直接把这个Request对象发出去，请求成功后会再得到一个Response对象，解析这个对象即可。而对于Request对象，其实它就是Prepared Request。<br>&emsp;&emsp;不使用get，而是直接构造一个Prepared Request对象来试试：</p>
<figure class="highlight py"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">from</span> requests <span class="keyword">import</span> Request,Session</span><br><span class="line"></span><br><span class="line">url = <span class="string">&#x27;https://www.httpbin.org/post&#x27;</span></span><br><span class="line">data = &#123;</span><br><span class="line">    <span class="string">&#x27;name&#x27;</span>:<span class="string">&#x27;germy&#x27;</span></span><br><span class="line">&#125;</span><br><span class="line">headers = &#123;</span><br><span class="line">    <span class="string">&#x27;User-Agent&#x27;</span>: <span class="string">&#x27;Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/124.0.0.0 Safari/537.36 Edg/124.0.0.0&#x27;</span></span><br><span class="line">&#125;</span><br><span class="line">s = Session()</span><br><span class="line">req = Request(<span class="string">&#x27;POST&#x27;</span>,url,data=data,headers=headers)</span><br><span class="line">prepped = s.prepare_request(req)</span><br><span class="line">response = s.send(prepped)</span><br><span class="line"><span class="built_in">print</span>(response.text)</span><br><span class="line">&gt;&gt;&gt;</span><br><span class="line">&#123;</span><br><span class="line">  <span class="string">&quot;args&quot;</span>: &#123;&#125;, </span><br><span class="line">  <span class="string">&quot;data&quot;</span>: <span class="string">&quot;&quot;</span>, </span><br><span class="line">  <span class="string">&quot;files&quot;</span>: &#123;&#125;, </span><br><span class="line">  <span class="string">&quot;form&quot;</span>: &#123;</span><br><span class="line">    <span class="string">&quot;name&quot;</span>: <span class="string">&quot;germy&quot;</span></span><br><span class="line">  &#125;, </span><br><span class="line">  <span class="string">&quot;headers&quot;</span>: &#123;</span><br><span class="line">    <span class="string">&quot;Accept&quot;</span>: <span class="string">&quot;*/*&quot;</span>, </span><br><span class="line">    <span class="string">&quot;Accept-Encoding&quot;</span>: <span class="string">&quot;gzip, deflate&quot;</span>, </span><br><span class="line">    <span class="string">&quot;Content-Length&quot;</span>: <span class="string">&quot;10&quot;</span>, </span><br><span class="line">    <span class="string">&quot;Content-Type&quot;</span>: <span class="string">&quot;application/x-www-form-urlencoded&quot;</span>, </span><br><span class="line">    <span class="string">&quot;Host&quot;</span>: <span class="string">&quot;www.httpbin.org&quot;</span>, </span><br><span class="line">    <span class="string">&quot;User-Agent&quot;</span>: <span class="string">&quot;Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/124.0.0.0 Safari/537.36 Edg/124.0.0.0&quot;</span>, </span><br><span class="line">    <span class="string">&quot;X-Amzn-Trace-Id&quot;</span>: <span class="string">&quot;Root=1-663a1529-42d19be2162994625bf75e11&quot;</span></span><br><span class="line">  &#125;, </span><br><span class="line">  <span class="string">&quot;json&quot;</span>: null, </span><br><span class="line">  <span class="string">&quot;origin&quot;</span>: <span class="string">&quot;219.156.133.195&quot;</span>, </span><br><span class="line">  <span class="string">&quot;url&quot;</span>: <span class="string">&quot;https://www.httpbin.org/post&quot;</span></span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>
<p>&emsp;&emsp;我们引入了Request类，然后用url、data和headers参数构造了一个Request对象，这时调用Session类的prepare_request方法将其转换为Prepared Request对象，再调用send方法发送，即可达到和POST请求同样的效果。<br>&emsp;&emsp;有了Request这个对象，就可以将请求当作独立的对象来看待，这里可以直接操作这个Request对象，更灵活地实现请求的调度和各种操作。  </p>
<h1 id="正则表达式"><a href="#正则表达式" class="headerlink" title="正则表达式"></a>正则表达式</h1><p>实现字符串的检索、替换、匹配验证。</p>
<h2 id="实例引入"><a href="#实例引入" class="headerlink" title="实例引入"></a>实例引入</h2><p><a target="_blank" rel="noopener" href="https://tool.oschina.net/regex">正则表达式测试工具</a><br>&emsp;&emsp;在这里输入待匹配的文字，然后选择常用的正则表达式，就可以得出相应的匹配结果。</p>
<blockquote>
<p>Hello, my phone number is 010-86432100 and email is <a href="mailto:&#99;&#113;&#x63;&#64;&#99;&#x75;&#x69;&#x71;&#105;&#x6e;&#x67;&#99;&#x61;&#x69;&#x2e;&#x63;&#111;&#x6d;">&#99;&#113;&#x63;&#64;&#99;&#x75;&#x69;&#x71;&#105;&#x6e;&#x67;&#99;&#x61;&#x69;&#x2e;&#x63;&#111;&#x6d;</a>,    and my website is <a target="_blank" rel="noopener" href="https://cuiqigncai.com/">https://cuiqigncai.com</a></p>
</blockquote>
<p>&emsp;&emsp;这段字符串包含一个电话号码，一个Email地址和一个URL，然后用正则表达式将这些内容提取出来。<br>&emsp;&emsp;在网页右侧选择“匹配Email地址”，就可以看到出现文本中的E-mail。选择“匹配网址URL”可以看到文本中的URL。<br>&emsp;&emsp;这里面用的就是正则表达式匹配，用一定的规则将特定的文本提取出来。<br>&emsp;&emsp;E-mail地址开头是一段字符串，然后一个@符号，后面跟上的是域名。对于URL，开头是协议类型，然后是冒号加两个斜杠，最后是域名加路径。<br>&emsp;&emsp;对于URL可以是<code>[a-zA-z]+://[^\s]*</code>，<code>a-z</code>代表匹配任意的小写字母，<code>\s</code>代表匹配任意的空白字符，<code>*</code>代表匹配前面任意多个字符。<br><strong>常用匹配规则</strong>  </p>
<table>
<thead>
<tr>
<th>模式</th>
<th>描述</th>
</tr>
</thead>
<tbody><tr>
<td>\w</td>
<td>匹配字母数字下划线</td>
</tr>
<tr>
<td>\W</td>
<td>匹配非字母数字下划线</td>
</tr>
<tr>
<td>\s</td>
<td>匹配任意空白字符，[\t\n\r\f]</td>
</tr>
<tr>
<td>\S</td>
<td>匹配任意非空字符</td>
</tr>
<tr>
<td>\d</td>
<td>匹配任意数字，等价于[0-9]</td>
</tr>
<tr>
<td>\D</td>
<td>匹配任意非数字</td>
</tr>
<tr>
<td>\A</td>
<td>匹配字符串开头</td>
</tr>
<tr>
<td>\Z</td>
<td>匹配字符串结尾。如果有换行，只匹配到换行前的结束字符串</td>
</tr>
<tr>
<td>\z</td>
<td>匹配字符串结尾。如果有换行，同时还会匹配换行符</td>
</tr>
<tr>
<td>\G</td>
<td>匹配最后匹配完成的位置</td>
</tr>
<tr>
<td>\n</td>
<td>匹配一个换行符</td>
</tr>
<tr>
<td>\t</td>
<td>匹配一个制表符</td>
</tr>
<tr>
<td>^</td>
<td>匹配一行字符串的开头</td>
</tr>
<tr>
<td>$</td>
<td>匹配一行字符串的结尾</td>
</tr>
<tr>
<td>.</td>
<td>匹配任意字符，除了换行符。当re.DOTALL标记被指定时，可以匹配包括换行符在内的任意字符</td>
</tr>
<tr>
<td>[…]</td>
<td>用来表示一组字符，单独列出，如[amk]匹配a、m、k</td>
</tr>
<tr>
<td>[^…]</td>
<td>匹配不在[]中的字符</td>
</tr>
<tr>
<td>*</td>
<td>匹配0个或多个表达式</td>
</tr>
<tr>
<td>+</td>
<td>匹配一个或多个表达式</td>
</tr>
<tr>
<td>?</td>
<td>匹配0个或1个前面的正则表达式定义的片段，非贪婪方式</td>
</tr>
<tr>
<td>{n}</td>
<td>精确匹配n个前面的表达式</td>
</tr>
<tr>
<td>{n,m}</td>
<td>匹配n到m次由前面正则表达式顶一个的片段，贪婪方式</td>
</tr>
<tr>
<td>a|b</td>
<td>匹配a或b</td>
</tr>
<tr>
<td>()</td>
<td>匹配括号内的表达式，也表示一个组</td>
</tr>
</tbody></table>
<p>&emsp;&emsp;正则表达式并非Python独有，但再Python的re库提供了正正则表达式的实现，利用这个库，可以再Python中方便地使用正则表达式。  </p>
<h2 id="match"><a href="#match" class="headerlink" title="match"></a>match</h2><p>&emsp;&emsp;这是一个常用的匹配方法，match方法会尝试从字符串的起始位置开始匹配正则表达式，如果匹配就返回成功的结果；否则返回None。<br>实例  </p>
<figure class="highlight py"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> re</span><br><span class="line"></span><br><span class="line">content = <span class="string">&#x27;Hello 123 4567 World_This is a Regex Demo&#x27;</span></span><br><span class="line"><span class="built_in">print</span>(<span class="built_in">len</span>(content))</span><br><span class="line">result = re.<span class="keyword">match</span>(<span class="string">&#x27;^Hello\s\d\d\d\s\d&#123;4&#125;\s\w&#123;10&#125;&#x27;</span>,content)</span><br><span class="line"><span class="built_in">print</span>(result)</span><br><span class="line"><span class="built_in">print</span>(result.group())</span><br><span class="line"><span class="built_in">print</span>(result.span())</span><br><span class="line">&gt;&gt;&gt;</span><br><span class="line"><span class="number">41</span></span><br><span class="line">&lt;re.Match <span class="built_in">object</span>; span=(<span class="number">0</span>, <span class="number">25</span>), <span class="keyword">match</span>=<span class="string">&#x27;Hello 123 4567 World_This&#x27;</span>&gt;</span><br><span class="line">Hello <span class="number">123</span> <span class="number">4567</span> World_This</span><br><span class="line">(<span class="number">0</span>, <span class="number">25</span>)</span><br></pre></td></tr></table></figure>
<p>&emsp;&emsp;首先声明字符串，<code>^Hello\s\d\d\d\s\d&#123;4&#125;\s\w&#123;10&#125;</code>是一个正则表达式。在match方法里，第一个参数是传入了正则表达式，第二个参数是要匹配的字符串。结果是<code>re.Match</code>对象，匹配成功了。对象包含两个方法：group方法可以输出匹配到的内容；span方法可以输出匹配的范围，结果是(0,25)，是匹配到的结果字符串在原字符串中的位置范围。  </p>
<h3 id="匹配目标"><a href="#匹配目标" class="headerlink" title="匹配目标"></a>匹配目标</h3><p>&emsp;&emsp;match实现匹配目标，对于从字符串中提取内容，可以用括号()将想提取的子字符串括起来。()实际上标记了一个子表达式的开始和结束位置，被标记的子表达式依次对应每一个分组，通过调用group方法传入分组索引获取提取结果。<br>实例</p>
<figure class="highlight py"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> re</span><br><span class="line"></span><br><span class="line">content = <span class="string">&#x27;Hello 123 4567 World_This is a Regex Demo&#x27;</span></span><br><span class="line">result = re.<span class="keyword">match</span>(<span class="string">&#x27;^Hello\s(\d\d\d\s\d&#123;4&#125;)\s(\w&#123;10&#125;)&#x27;</span>,content)</span><br><span class="line"><span class="built_in">print</span>(result)</span><br><span class="line"><span class="built_in">print</span>(result.group())</span><br><span class="line"><span class="built_in">print</span>(result.group(<span class="number">1</span>))</span><br><span class="line"><span class="built_in">print</span>(result.span())</span><br><span class="line">&gt;&gt;&gt;</span><br><span class="line">&lt;re.Match <span class="built_in">object</span>; span=(<span class="number">0</span>, <span class="number">25</span>), <span class="keyword">match</span>=<span class="string">&#x27;Hello 123 4567 World_This&#x27;</span>&gt;</span><br><span class="line">Hello <span class="number">123</span> <span class="number">4567</span> World_This</span><br><span class="line"><span class="number">123</span> <span class="number">4567</span></span><br><span class="line">(<span class="number">0</span>, <span class="number">25</span>)</span><br></pre></td></tr></table></figure>
<p>&emsp;&emsp;把字符串中的<code>123 4567</code>提取出来了。group方法会输出完整的匹配结果。group(1)输出第一个被()包围的匹配结果，以此类推。  </p>
<h3 id="通用匹配"><a href="#通用匹配" class="headerlink" title="通用匹配"></a>通用匹配</h3><p>&emsp;&emsp;有一个万能匹配符，<code>*</code>。它可以匹配任意字符（除了换行符），<code>*</code>代表匹配前面的字符无限次，组合在一起就可以匹配任意字符了。<br>实例</p>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br></pre></td><td class="code"><pre><span class="line">import re</span><br><span class="line"></span><br><span class="line">content = &#x27;Hello 123 4567 World_This is a Regex Demo&#x27;</span><br><span class="line">result = re.match(&#x27;^Hello.*Demo$&#x27;,content)</span><br><span class="line">print(result)</span><br><span class="line">print(result.group())</span><br><span class="line">print(result.span())</span><br><span class="line">&gt;&gt;&gt;</span><br><span class="line">&lt;re.Match object; span=(0, 41), match=&#x27;Hello 123 4567 World_This is a Regex Demo&#x27;&gt;</span><br><span class="line">Hello 123 4567 World_This is a Regex Demo</span><br><span class="line">(0, 41)</span><br></pre></td></tr></table></figure>
<p>&emsp;&emsp;中间部分全用<code>.*</code>代替，最后加一个结尾字符串。  </p>
<h3 id="贪婪与非贪婪"><a href="#贪婪与非贪婪" class="headerlink" title="贪婪与非贪婪"></a>贪婪与非贪婪</h3><p>&emsp;&emsp;通用匹配<code>.*</code>匹配到的内容有时并不是我们想要的结果。<br>实例</p>
<figure class="highlight py"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> re</span><br><span class="line">content = <span class="string">&#x27;Hello 1234567 World_This is a Regex Demo&#x27;</span></span><br><span class="line">result = re.<span class="keyword">match</span>(<span class="string">&#x27;^H.*(\d+).*Demo$&#x27;</span>,content)</span><br><span class="line"><span class="built_in">print</span>(result)</span><br><span class="line"><span class="built_in">print</span>(result.group(<span class="number">1</span>))</span><br><span class="line">&gt;&gt;&gt;</span><br><span class="line">&lt;re.Match <span class="built_in">object</span>; span=(<span class="number">0</span>, <span class="number">40</span>), <span class="keyword">match</span>=<span class="string">&#x27;Hello 1234567 World_This is a Regex Demo&#x27;</span>&gt;</span><br><span class="line"><span class="number">7</span></span><br></pre></td></tr></table></figure>
<p>&emsp;&emsp;目标是匹配字符串中间的数字，用<code>(\d+)</code>来匹配数字的两侧都用<code>.*</code>，而结果只得到了7。<br>&emsp;&emsp;这是因为在贪婪匹配下，<code>.*</code>会匹配尽可能多的字符，.*后面是<code>\d+</code>，至少匹配一个数字，但是这里并没有指定具体几个数字，<code>.*</code>就把123456都匹配了，只给<code>\d+</code>留下一个满足条件的7。<br>&emsp;&emsp;使用非贪婪匹配来解决这个问题。非贪婪匹配写法是<code>.*?</code>。  </p>
<figure class="highlight py"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> re</span><br><span class="line">content = <span class="string">&#x27;Hello 1234567 World_This is a Regex Demo&#x27;</span></span><br><span class="line">result = re.<span class="keyword">match</span>(<span class="string">&#x27;^H.*?(\d+).*Demo$&#x27;</span>,content)</span><br><span class="line"><span class="built_in">print</span>(result)</span><br><span class="line"><span class="built_in">print</span>(result.group(<span class="number">1</span>))</span><br><span class="line">&gt;&gt;&gt;</span><br><span class="line">&lt;re.Match <span class="built_in">object</span>; span=(<span class="number">0</span>, <span class="number">40</span>), <span class="keyword">match</span>=<span class="string">&#x27;Hello 1234567 World_This is a Regex Demo&#x27;</span>&gt;</span><br><span class="line"><span class="number">1234567</span></span><br></pre></td></tr></table></figure>
<p>&emsp;&emsp;此时成功获取了1234567。贪婪匹配是匹配尽可能多的字符，非贪婪匹配是匹配尽可能少的字符。<br>&emsp;&emsp;有一种情况，当匹配的结果在字符串的结尾，<code>.*?</code>有可能匹配不到任何内容。<br>实例</p>
<figure class="highlight py"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> re</span><br><span class="line">content = <span class="string">&#x27;Http://www.baidu.com/comment&#x27;</span></span><br><span class="line">result1 = re.<span class="keyword">match</span>(<span class="string">&#x27;Http://www.baidu.com/(.*?)&#x27;</span>,content)</span><br><span class="line">result2 = re.<span class="keyword">match</span>(<span class="string">&#x27;Http://www.baidu.com/(.*)&#x27;</span>,content)</span><br><span class="line"><span class="built_in">print</span>(<span class="string">&#x27;result1&#x27;</span>,result1.group(<span class="number">1</span>))</span><br><span class="line"><span class="built_in">print</span>(<span class="string">&#x27;result2&#x27;</span>,result2.group(<span class="number">1</span>))</span><br><span class="line">&gt;&gt;&gt;</span><br><span class="line">result1 </span><br><span class="line">result2 comment</span><br></pre></td></tr></table></figure>
<h3 id="修饰符"><a href="#修饰符" class="headerlink" title="修饰符"></a>修饰符</h3><p>&emsp;&emsp;在正则表达式里，可以用一些可选标志来控制匹配的模式。修饰符被指定为一个可选的标志。<br>实例</p>
<figure class="highlight py"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> re</span><br><span class="line">content = <span class="string">&#x27;&#x27;&#x27;Hello 1234567 World_This</span></span><br><span class="line"><span class="string">is a Regex Demo&#x27;&#x27;&#x27;</span></span><br><span class="line">result1 = re.<span class="keyword">match</span>(<span class="string">&#x27;^He.*?(\d+).*?Demo$&#x27;</span>,content)</span><br><span class="line"><span class="built_in">print</span>(<span class="string">&#x27;result1&#x27;</span>,result1.group(<span class="number">1</span>))</span><br></pre></td></tr></table></figure>
<p>&emsp;&emsp;由于字符串里加了换行符，报错了<code>AttributeError: &#39;NoneType&#39; object has no attribute &#39;group&#39;</code>。表明正则表达式没有匹配到这个字符串，结果是None。调用group方法，导致了Attribute。<br>&emsp;&emsp;因为匹配的内容是除了换行符之外的任意字符，当遇到换行符时，<code>.*?</code>就不能匹配了。<br>&emsp;&emsp;解决这个问题的方式是加上一个修饰符re.S，就可以了。  </p>
<figure class="highlight py"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> re</span><br><span class="line">content = <span class="string">&#x27;&#x27;&#x27;Hello 1234567 World_This</span></span><br><span class="line"><span class="string">is a Regex Demo&#x27;&#x27;&#x27;</span></span><br><span class="line">result1 = re.<span class="keyword">match</span>(<span class="string">&#x27;^He.*?(\d+).*?Demo$&#x27;</span>,content,re.S)</span><br><span class="line"><span class="built_in">print</span>(result1.group(<span class="number">1</span>))</span><br><span class="line">&gt;&gt;&gt;</span><br><span class="line"><span class="number">1234567</span></span><br></pre></td></tr></table></figure>
<p>&emsp;&emsp;re.S在网页匹配中经常用到。因为HTML节点经常会有换行，加上它，就可以匹配节点与节点之间的换行了。<br>&emsp;&emsp;还有一些修饰符，在必要的情况下可以使用。  </p>
<table>
<thead>
<tr>
<th>修饰符</th>
<th>描述</th>
</tr>
</thead>
<tbody><tr>
<td>re.I</td>
<td>使匹配对大小写不敏感</td>
</tr>
<tr>
<td>re.L</td>
<td>实现本地化识别(local-aware)匹配</td>
</tr>
<tr>
<td>re.M</td>
<td>多行匹配，影响^和$</td>
</tr>
<tr>
<td>re.S</td>
<td>使匹配内容包括换行符在内的所有字符</td>
</tr>
<tr>
<td>re.U</td>
<td>根据Unicode字符集解析字符。会影响\w、\W、\b、\B</td>
</tr>
<tr>
<td>re.X</td>
<td>给予灵活的格式，将正则表达式书写的更易于理解</td>
</tr>
</tbody></table>
<h3 id="转义匹配"><a href="#转义匹配" class="headerlink" title="转义匹配"></a>转义匹配</h3><p>&emsp;&emsp;<code>.</code>用于匹配除换行符以外的任意字符，但目标字符串中可能包含<code>.</code>这个字符。<br>&emsp;&emsp;我们需要用到转义匹配。<br>实例</p>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line">import re</span><br><span class="line">content = &#x27;&#x27;&#x27;(百度)www.baidu.com&#x27;&#x27;&#x27;</span><br><span class="line">result1 = re.match(&#x27;\(百度\)www\.baidu\.com&#x27;,content,re.S)</span><br><span class="line">print(result1)</span><br><span class="line">&gt;&gt;&gt;</span><br><span class="line">&lt;re.Match object; span=(0, 17), match=&#x27;(百度)www.baidu.com&#x27;&gt;</span><br></pre></td></tr></table></figure>
<p>在特殊字符前加<code>\</code>转义一下即可。  </p>
<h2 id="search"><a href="#search" class="headerlink" title="search"></a>search</h2><p>&emsp;&emsp;match方法是从字符串的开头开始匹配的，一旦开头不匹配，整个匹配就会失败。<br>实例</p>
<figure class="highlight py"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> re</span><br><span class="line">content = <span class="string">&#x27;Extra strings Hello 1234567 World_THis is a Regax Demo Strings&#x27;</span></span><br><span class="line">result = re.<span class="keyword">match</span>(<span class="string">&#x27;Hello.*?(\d+).*?Demo&#x27;</span>,content)</span><br><span class="line"><span class="built_in">print</span>(result)</span><br><span class="line">&gt;&gt;&gt;</span><br><span class="line"><span class="literal">None</span></span><br></pre></td></tr></table></figure>
<p>&emsp;&emsp;match方法在使用时需要考虑目标字符串开头的内容，它更适合检测某个字符串是否符合某个正则表达式的规则。<br>&emsp;&emsp;使用search方法在匹配时会扫描整个字符串，然后返回第一个匹配成功的结果。在匹配时，search会依次以每个字符作为开头扫描字符串，知道找到符合规则的字符串，然后返回匹配的内容；如果没有找到，就返回None。  </p>
<figure class="highlight py"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> re</span><br><span class="line">content = <span class="string">&#x27;Extra strings Hello 1234567 World_THis is a Regax Demo Strings&#x27;</span></span><br><span class="line">result = re.search(<span class="string">&#x27;Hello.*?(\d+).*?Demo&#x27;</span>,content)</span><br><span class="line"><span class="built_in">print</span>(result)</span><br><span class="line"><span class="built_in">print</span>(result.group(<span class="number">1</span>))</span><br><span class="line">&gt;&gt;&gt;</span><br><span class="line">&lt;re.Match <span class="built_in">object</span>; span=(<span class="number">14</span>, <span class="number">54</span>), <span class="keyword">match</span>=<span class="string">&#x27;Hello 1234567 World_THis is a Regax Demo&#x27;</span>&gt;</span><br><span class="line"><span class="number">1234567</span></span><br></pre></td></tr></table></figure>
<p>其他实例<br>有一段HTML文本，用几个正则表达式实例实现相应信息的提取：</p>
<figure class="highlight html"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br></pre></td><td class="code"><pre><span class="line">html = &#x27;&#x27;&#x27;<span class="tag">&lt;<span class="name">div</span> <span class="attr">id</span>=<span class="string">&quot;songs-list&quot;</span>&gt;</span></span><br><span class="line"><span class="tag">&lt;<span class="name">h2</span> <span class="attr">class</span>=<span class="string">&quot;title&quot;</span>&gt;</span> 经典老歌 <span class="tag">&lt;/<span class="name">h2</span>&gt;</span></span><br><span class="line"><span class="tag">&lt;<span class="name">p</span> <span class="attr">class</span>=<span class="string">&quot;introduction&quot;</span>&gt;</span></span><br><span class="line">经典老歌列表</span><br><span class="line"><span class="tag">&lt;/<span class="name">p</span>&gt;</span></span><br><span class="line"><span class="tag">&lt;<span class="name">ul</span> <span class="attr">id</span>=<span class="string">&quot;list&quot;</span> <span class="attr">class</span>=<span class="string">&quot;list-group&quot;</span>&gt;</span></span><br><span class="line"><span class="tag">&lt;<span class="name">li</span> <span class="attr">data-view</span>=<span class="string">&quot;2&quot;</span>&gt;</span> 一路上有你 <span class="tag">&lt;/<span class="name">li</span>&gt;</span></span><br><span class="line"><span class="tag">&lt;<span class="name">li</span> <span class="attr">data-view</span>=<span class="string">&quot;7&quot;</span>&gt;</span></span><br><span class="line"><span class="tag">&lt;<span class="name">a</span> <span class="attr">href</span>=<span class="string">&quot;/2.mp3&quot;</span> <span class="attr">singer</span>=<span class="string">&quot;任贤齐&quot;</span>&gt;</span> 沧海一声笑 <span class="tag">&lt;/<span class="name">a</span>&gt;</span></span><br><span class="line"><span class="tag">&lt;/<span class="name">li</span>&gt;</span></span><br><span class="line"><span class="tag">&lt;<span class="name">li</span> <span class="attr">data-view</span>=<span class="string">&quot;4&quot;</span> <span class="attr">class</span>=<span class="string">&quot;active&quot;</span>&gt;</span></span><br><span class="line"><span class="tag">&lt;<span class="name">a</span> <span class="attr">href</span>=<span class="string">&quot;/3.mp3&quot;</span> <span class="attr">singer</span>=<span class="string">&quot;齐秦&quot;</span>&gt;</span> 往事随风 <span class="tag">&lt;/<span class="name">a</span>&gt;</span></span><br><span class="line"><span class="tag">&lt;/<span class="name">li</span>&gt;</span></span><br><span class="line"><span class="tag">&lt;<span class="name">li</span> <span class="attr">data-view</span>=<span class="string">&quot;6&quot;</span>&gt;</span><span class="tag">&lt;<span class="name">a</span> <span class="attr">href</span>=<span class="string">&quot;/4.mp3&quot;</span> <span class="attr">singer</span>=<span class="string">&quot;beyond&quot;</span>&gt;</span> 光辉岁月 <span class="tag">&lt;/<span class="name">a</span>&gt;</span><span class="tag">&lt;/<span class="name">li</span>&gt;</span></span><br><span class="line"><span class="tag">&lt;<span class="name">li</span> <span class="attr">data-view</span>=<span class="string">&quot;5&quot;</span>&gt;</span><span class="tag">&lt;<span class="name">a</span> <span class="attr">href</span>=<span class="string">&quot;/5.mp3&quot;</span> <span class="attr">singer</span>=<span class="string">&quot;陈慧琳&quot;</span>&gt;</span> 记事本 <span class="tag">&lt;/<span class="name">a</span>&gt;</span><span class="tag">&lt;/<span class="name">li</span>&gt;</span></span><br><span class="line"><span class="tag">&lt;<span class="name">li</span> <span class="attr">data-view</span>=<span class="string">&quot;5&quot;</span>&gt;</span></span><br><span class="line"><span class="tag">&lt;<span class="name">a</span> <span class="attr">href</span>=<span class="string">&quot;/6.mp3&quot;</span> <span class="attr">singer</span>=<span class="string">&quot;邓丽君&quot;</span>&gt;</span> 但愿人长久 <span class="tag">&lt;/<span class="name">a</span>&gt;</span></span><br><span class="line"><span class="tag">&lt;/<span class="name">li</span>&gt;</span></span><br><span class="line"><span class="tag">&lt;/<span class="name">ul</span>&gt;</span></span><br><span class="line"><span class="tag">&lt;/<span class="name">div</span>&gt;</span>&#x27;&#x27;&#x27;</span><br></pre></td></tr></table></figure>
<p>&emsp;&emsp;ul节点里有很多li节点，li节点里有点包含a节点，有的不包含a节点。a节点还有一些相应的属性——超链接和歌手名。<br>&emsp;&emsp;我们尝试提取 class 为 active 的 li 节点内部的超链接包含的歌手名和歌名，此时需要提取第三个 li 节点下 a 节点的 singer 属性和文本。<br>&emsp;&emsp;正则表达式以li开头，然后找标志符active，中间部分用.*?来匹配。然后提取singer的值，可以使用<code>singer=&quot;(.*?)&quot;</code>。接下来匹配a节点的文本，左边界是&gt;，右边界是&lt;&#x2F;a&gt;，目标用<code>(.*?)</code>匹配。正则表达式就是：<code>li.*?active.*?singer=&quot;(.*?)&quot;&gt;(.*?)&lt;/a&gt;</code>。由于代码中有换行，所以search方法的第三个参数要传入re.S。：</p>
<figure class="highlight py"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> re</span><br><span class="line">html = <span class="string">&#x27;&#x27;&#x27;&lt;div id=&quot;songs-list&quot;&gt;</span></span><br><span class="line"><span class="string">&lt;h2 class=&quot;title&quot;&gt; 经典老歌 &lt;/h2&gt;</span></span><br><span class="line"><span class="string">&lt;p class=&quot;introduction&quot;&gt;</span></span><br><span class="line"><span class="string">经典老歌列表</span></span><br><span class="line"><span class="string">&lt;/p&gt;</span></span><br><span class="line"><span class="string">&lt;ul id=&quot;list&quot; class=&quot;list-group&quot;&gt;</span></span><br><span class="line"><span class="string">&lt;li data-view=&quot;2&quot;&gt; 一路上有你 &lt;/li&gt;</span></span><br><span class="line"><span class="string">&lt;li data-view=&quot;7&quot;&gt;</span></span><br><span class="line"><span class="string">&lt;a href=&quot;/2.mp3&quot; singer=&quot;任贤齐&quot;&gt; 沧海一声笑 &lt;/a&gt;</span></span><br><span class="line"><span class="string">&lt;/li&gt;</span></span><br><span class="line"><span class="string">&lt;li data-view=&quot;4&quot; class=&quot;active&quot;&gt;</span></span><br><span class="line"><span class="string">&lt;a href=&quot;/3.mp3&quot; singer=&quot;齐秦&quot;&gt; 往事随风 &lt;/a&gt;</span></span><br><span class="line"><span class="string">&lt;/li&gt;</span></span><br><span class="line"><span class="string">&lt;li data-view=&quot;6&quot;&gt;&lt;a href=&quot;/4.mp3&quot; singer=&quot;beyond&quot;&gt; 光辉岁月 &lt;/a&gt;&lt;/li&gt;</span></span><br><span class="line"><span class="string">&lt;li data-view=&quot;5&quot;&gt;&lt;a href=&quot;/5.mp3&quot; singer=&quot;陈慧琳&quot;&gt; 记事本 &lt;/a&gt;&lt;/li&gt;</span></span><br><span class="line"><span class="string">&lt;li data-view=&quot;5&quot;&gt;</span></span><br><span class="line"><span class="string">&lt;a href=&quot;/6.mp3&quot; singer=&quot;邓丽君&quot;&gt; 但愿人长久 &lt;/a&gt;</span></span><br><span class="line"><span class="string">&lt;/li&gt;</span></span><br><span class="line"><span class="string">&lt;/ul&gt;</span></span><br><span class="line"><span class="string">&lt;/div&gt;&#x27;&#x27;&#x27;</span></span><br><span class="line">result = re.search(<span class="string">&#x27;li.*?active.*?singer=&quot;(.*?)&quot;&gt;(.*?)&lt;/a&gt;&#x27;</span>,html,re.S)</span><br><span class="line"><span class="built_in">print</span>(result.group(<span class="number">1</span>),result.group(<span class="number">2</span>))</span><br><span class="line">&gt;&gt;&gt;</span><br><span class="line">齐秦  往事随风 </span><br></pre></td></tr></table></figure>
<p>&emsp;&emsp;这样就获取到了class为active的歌手名和歌曲名。<br>&emsp;&emsp;试一下表达式不加active会如何  </p>
<figure class="highlight py"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">result = re.search(<span class="string">&#x27;li.*?singer=&quot;(.*?)&quot;&gt;(.*?)&lt;/a&gt;&#x27;</span>,html,re.S)</span><br><span class="line"><span class="built_in">print</span>(result.group(<span class="number">1</span>),result.group(<span class="number">2</span>))</span><br><span class="line">&gt;&gt;&gt;</span><br><span class="line">任贤齐  沧海一声笑 </span><br></pre></td></tr></table></figure>
<p>&emsp;&emsp;会返回第一个符合条件的匹配目标<br>&emsp;&emsp;接下来，把search方法的第三个参数re.S去掉</p>
<figure class="highlight py"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">result = re.search(<span class="string">&#x27;li.*?singer=&quot;(.*?)&quot;&gt;(.*?)&lt;/a&gt;&#x27;</span>,html)</span><br><span class="line"><span class="built_in">print</span>(result.group(<span class="number">1</span>),result.group(<span class="number">2</span>))</span><br><span class="line">&gt;&gt;&gt;</span><br><span class="line">beyond  光辉岁月 </span><br></pre></td></tr></table></figure>
<p>绝大部分HTML文本包含换行符，所以要尽量加上re.s修饰符，避免出现匹配不到的问题。  </p>
<h2 id="findall"><a href="#findall" class="headerlink" title="findall"></a>findall</h2><p>&emsp;&emsp;对于search方法，它返回与正则表达式相匹配的第一个字符串。如果想获取与正则表达式相匹配的所有字符串，要使用findall方法了。<br>实例</p>
<figure class="highlight py"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> re</span><br><span class="line">html = <span class="string">&#x27;&#x27;&#x27;&lt;div id=&quot;songs-list&quot;&gt;</span></span><br><span class="line"><span class="string">&lt;h2 class=&quot;title&quot;&gt; 经典老歌 &lt;/h2&gt;</span></span><br><span class="line"><span class="string">&lt;p class=&quot;introduction&quot;&gt;</span></span><br><span class="line"><span class="string">经典老歌列表</span></span><br><span class="line"><span class="string">&lt;/p&gt;</span></span><br><span class="line"><span class="string">&lt;ul id=&quot;list&quot; class=&quot;list-group&quot;&gt;</span></span><br><span class="line"><span class="string">&lt;li data-view=&quot;2&quot;&gt; 一路上有你 &lt;/li&gt;</span></span><br><span class="line"><span class="string">&lt;li data-view=&quot;7&quot;&gt;</span></span><br><span class="line"><span class="string">&lt;a href=&quot;/2.mp3&quot; singer=&quot;任贤齐&quot;&gt; 沧海一声笑 &lt;/a&gt;</span></span><br><span class="line"><span class="string">&lt;/li&gt;</span></span><br><span class="line"><span class="string">&lt;li data-view=&quot;4&quot; class=&quot;active&quot;&gt;</span></span><br><span class="line"><span class="string">&lt;a href=&quot;/3.mp3&quot; singer=&quot;齐秦&quot;&gt; 往事随风 &lt;/a&gt;</span></span><br><span class="line"><span class="string">&lt;/li&gt;</span></span><br><span class="line"><span class="string">&lt;li data-view=&quot;6&quot;&gt;&lt;a href=&quot;/4.mp3&quot; singer=&quot;beyond&quot;&gt; 光辉岁月 &lt;/a&gt;&lt;/li&gt;</span></span><br><span class="line"><span class="string">&lt;li data-view=&quot;5&quot;&gt;&lt;a href=&quot;/5.mp3&quot; singer=&quot;陈慧琳&quot;&gt; 记事本 &lt;/a&gt;&lt;/li&gt;</span></span><br><span class="line"><span class="string">&lt;li data-view=&quot;5&quot;&gt;</span></span><br><span class="line"><span class="string">&lt;a href=&quot;/6.mp3&quot; singer=&quot;邓丽君&quot;&gt; 但愿人长久 &lt;/a&gt;</span></span><br><span class="line"><span class="string">&lt;/li&gt;</span></span><br><span class="line"><span class="string">&lt;/ul&gt;</span></span><br><span class="line"><span class="string">&lt;/div&gt;&#x27;&#x27;&#x27;</span></span><br><span class="line">result = re.findall(<span class="string">&#x27;li.*?href=&quot;(.*?)&quot;.*?singer=&quot;(.*?)&quot;&gt;(.*?)&lt;/a&gt;&#x27;</span>,html,re.S)</span><br><span class="line"><span class="built_in">print</span>(result)</span><br><span class="line"><span class="built_in">print</span>(<span class="built_in">type</span>(result))</span><br><span class="line"><span class="keyword">for</span> i <span class="keyword">in</span> result:</span><br><span class="line">    <span class="built_in">print</span>(i)</span><br><span class="line">    <span class="built_in">print</span>(i[<span class="number">0</span>],i[<span class="number">1</span>],i[<span class="number">2</span>])</span><br><span class="line">&gt;&gt;&gt;</span><br><span class="line">[(<span class="string">&#x27;/2.mp3&#x27;</span>, <span class="string">&#x27;任贤齐&#x27;</span>, <span class="string">&#x27; 沧海一声笑 &#x27;</span>), (<span class="string">&#x27;/3.mp3&#x27;</span>, <span class="string">&#x27;齐秦&#x27;</span>, <span class="string">&#x27; 往事随风 &#x27;</span>), (<span class="string">&#x27;/4.mp3&#x27;</span>, <span class="string">&#x27;beyond&#x27;</span>, <span class="string">&#x27; 光辉岁月 &#x27;</span>), (<span class="string">&#x27;/5.mp3&#x27;</span>, <span class="string">&#x27;陈慧琳&#x27;</span>, <span class="string">&#x27; 记事本 &#x27;</span>), (<span class="string">&#x27;/6.mp3&#x27;</span>, <span class="string">&#x27;邓丽君&#x27;</span>, <span class="string">&#x27; 但愿人长久 &#x27;</span>)]</span><br><span class="line">&lt;<span class="keyword">class</span> <span class="string">&#x27;list&#x27;</span>&gt;</span><br><span class="line">(<span class="string">&#x27;/2.mp3&#x27;</span>, <span class="string">&#x27;任贤齐&#x27;</span>, <span class="string">&#x27; 沧海一声笑 &#x27;</span>)</span><br><span class="line">/<span class="number">2.</span>mp3 任贤齐  沧海一声笑 </span><br><span class="line">(<span class="string">&#x27;/3.mp3&#x27;</span>, <span class="string">&#x27;齐秦&#x27;</span>, <span class="string">&#x27; 往事随风 &#x27;</span>)</span><br><span class="line">/<span class="number">3.</span>mp3 齐秦  往事随风 </span><br><span class="line">(<span class="string">&#x27;/4.mp3&#x27;</span>, <span class="string">&#x27;beyond&#x27;</span>, <span class="string">&#x27; 光辉岁月 &#x27;</span>)</span><br><span class="line">/<span class="number">4.</span>mp3 beyond  光辉岁月 </span><br><span class="line">(<span class="string">&#x27;/5.mp3&#x27;</span>, <span class="string">&#x27;陈慧琳&#x27;</span>, <span class="string">&#x27; 记事本 &#x27;</span>)</span><br><span class="line">/<span class="number">5.</span>mp3 陈慧琳  记事本 </span><br><span class="line">(<span class="string">&#x27;/6.mp3&#x27;</span>, <span class="string">&#x27;邓丽君&#x27;</span>, <span class="string">&#x27; 但愿人长久 &#x27;</span>)</span><br><span class="line">/<span class="number">6.</span>mp3 邓丽君  但愿人长久 </span><br></pre></td></tr></table></figure>
<p>&emsp;&emsp;返回列表中的每个元素都是元组类型，可以用索引依次取出每个条目。当正则表达式中只有一个分组时，列表中的元组没有<code>()</code>。  </p>
<h2 id="sub"><a href="#sub" class="headerlink" title="sub"></a>sub</h2><p>&emsp;&emsp;正则表达式除了提取信息，还可以修改文本。如把字符串的所有数字都去掉，如果只用字符串的replace方法，显得繁琐，这里可以借助sub方法。<br>实例</p>
<figure class="highlight py"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> re</span><br><span class="line">content = <span class="string">&#x27;54aK54yr5oiR54ix5L2g&#x27;</span></span><br><span class="line">content = re.sub(<span class="string">&#x27;\d+&#x27;</span>,<span class="string">&#x27;&#x27;</span>,content)</span><br><span class="line"><span class="built_in">print</span>(content)</span><br><span class="line">&gt;&gt;&gt;</span><br><span class="line">aKyroiRixLg</span><br></pre></td></tr></table></figure>
<p>&emsp;&emsp;这里sub的第一个参数<code>\d+</code>用来匹配所有的数字，第二个参数是把数字替换成的字符串，第三个参数是原字符串。<br>&emsp;&emsp;获取前面HTML文本中所有li节点的歌名。<br>实例</p>
<figure class="highlight py"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br></pre></td><td class="code"><pre><span class="line">result = re.findall(<span class="string">&#x27;&lt;li.*?&gt;\s?(&lt;a.*?&gt;)?\s(\w+)\s(&lt;/a&gt;)?\s?&lt;/li&gt;&#x27;</span>,html,re.S)</span><br><span class="line"><span class="keyword">for</span> i <span class="keyword">in</span> result:</span><br><span class="line">    <span class="built_in">print</span>(i[<span class="number">1</span>])</span><br><span class="line">&gt;&gt;&gt;</span><br><span class="line">一路上有你</span><br><span class="line">沧海一声笑</span><br><span class="line">往事随风</span><br><span class="line">光辉岁月</span><br><span class="line">记事本</span><br><span class="line">但愿人长久</span><br></pre></td></tr></table></figure>
<p>&emsp;&emsp;显得有些繁琐，但此时用sub就很简单了。用sub把a节点去掉，只留下文本，然后用findall提取：</p>
<figure class="highlight py"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br></pre></td><td class="code"><pre><span class="line">html = re.sub(<span class="string">&#x27;&lt;a.*?&gt;|&lt;/a&gt;&#x27;</span>,<span class="string">&#x27;&#x27;</span>,html)</span><br><span class="line">result = re.findall(<span class="string">&#x27;&lt;li.*?&gt;(.*?)&lt;/li&gt;&#x27;</span>,html,re.S)</span><br><span class="line"><span class="built_in">print</span>(result)</span><br><span class="line"><span class="keyword">for</span> i <span class="keyword">in</span> result:</span><br><span class="line">    <span class="built_in">print</span>(i.strip())</span><br><span class="line">&gt;&gt;&gt;</span><br><span class="line">一路上有你</span><br><span class="line">沧海一声笑</span><br><span class="line">往事随风</span><br><span class="line">光辉岁月</span><br><span class="line">记事本</span><br><span class="line">但愿人长久</span><br></pre></td></tr></table></figure>
<p>&emsp;&emsp;经过sub方法处理过之后，a节点就没有了，然后通过findall方法直接提取。  </p>
<h2 id="compile"><a href="#compile" class="headerlink" title="compile"></a>compile</h2><p>&emsp;&emsp;这个方法可以将字符串编译成正则表达式对象，实现在后面的代码中复用。<br>实例</p>
<figure class="highlight py"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> re</span><br><span class="line">content1 = <span class="string">&#x27;2019-12-15 12:00&#x27;</span></span><br><span class="line">content2 = <span class="string">&#x27;2019-12-17 12:55&#x27;</span></span><br><span class="line">content3 = <span class="string">&#x27;2019-12-19 13:14&#x27;</span></span><br><span class="line">pattern = re.<span class="built_in">compile</span>(<span class="string">&#x27;\d&#123;2&#125;:\d&#123;2&#125;&#x27;</span>)</span><br><span class="line">result1 = re.sub(pattern, <span class="string">&#x27;&#x27;</span>, content1)</span><br><span class="line">result2 = re.sub(pattern, <span class="string">&#x27;&#x27;</span>, content2)</span><br><span class="line">result3 = re.sub(pattern, <span class="string">&#x27;&#x27;</span>, content3)</span><br><span class="line"><span class="built_in">print</span>(result1,result2,result3)</span><br><span class="line">&gt;&gt;&gt;</span><br><span class="line"><span class="number">2019</span>-<span class="number">12</span>-<span class="number">15</span>  <span class="number">2019</span>-<span class="number">12</span>-<span class="number">17</span>  <span class="number">2019</span>-<span class="number">12</span>-<span class="number">19</span></span><br></pre></td></tr></table></figure>
<p>&emsp;&emsp;这里有三个日期，我们用sub方法把日期中的时间去掉，用compile方法将正则表达式编译成一个正则表达式对象，实现复用。<br>&emsp;&emsp;compile中还可以传递修饰符，如re.S，在search，findall方法中就不需要再传了。  </p>
<h1 id="httpx的使用"><a href="#httpx的使用" class="headerlink" title="httpx的使用"></a>httpx的使用</h1><p>&emsp;&emsp;urllib库和requests库已经可以爬取绝大多数网站的数据，但是由于有些网站强制使用HTTP&#x2F;2.0协议访问，而urllib和requests只支持HTTP&#x2F;1.1，不支持HTTP2.0，就无法爬取数据。<br>&emsp;&emsp;这时使用支持HTTP&#x2F;2.0的请求库就可以了，有代表性的是hyper和httpx。httpx使用起来更方便，功能也强大，requests库已有的功能它也几乎都支持。  </p>
<h2 id="示例"><a href="#示例" class="headerlink" title="示例"></a>示例</h2><p>&emsp;&emsp;<code>https://spa16.scrape.center/</code>就是一个强制使用HTTP&#x2F;2.0访问的一个网站。这个网站用requests库是无法爬取的：</p>
<figure class="highlight py"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> requests</span><br><span class="line">url = <span class="string">&#x27;https://ssr1.scrape.center/&#x27;</span></span><br><span class="line">response = requests.get(url)</span><br><span class="line"><span class="built_in">print</span>(response)</span><br><span class="line">```  </span><br><span class="line">&amp;emsp;&amp;emsp;会抛出错误，错误信息很多，真实原因就是requests这个库是使用HTTP/<span class="number">1.1</span>访问的目标网站，而目标网站会检测请求使用的协议是不是HTTP/<span class="number">2.0</span>，如果不是就拒绝返回任何请求。  </span><br><span class="line"><span class="comment">## 安装</span></span><br><span class="line">&amp;emsp;&amp;emsp;httpx可以使用pip3工具直接安装，需要的Python版本是<span class="number">3.6</span>及以上</span><br></pre></td></tr></table></figure>
<p>pip3 install httpx</p>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">但是这样安装完的httpx是不支持HTTP/2.0的，如果想支持，可以这样安装  </span><br></pre></td></tr></table></figure>
<p>pip3 install “httpx[http2]”</p>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br></pre></td><td class="code"><pre><span class="line">就既安装了httpx，又安装了httpx对HTTP/2.0的支持模块。 </span><br><span class="line">## 基本使用</span><br><span class="line">&amp;emsp;&amp;emsp;基本GET请求用法</span><br><span class="line">```py</span><br><span class="line">import httpx</span><br><span class="line">response = httpx.get(&#x27;https://httpbin.org/get&#x27;)</span><br><span class="line">print(response.status_code)</span><br><span class="line">print(response.headers)</span><br><span class="line">print(response.text)</span><br><span class="line">&gt;&gt;&gt;</span><br><span class="line">200</span><br><span class="line">Headers(&#123;&#x27;date&#x27;: &#x27;Mon, 13 May 2024 03:59:08 GMT&#x27;, &#x27;content-type&#x27;: &#x27;application/json&#x27;, &#x27;content-length&#x27;: &#x27;306&#x27;, &#x27;connection&#x27;: &#x27;keep-alive&#x27;, &#x27;server&#x27;: &#x27;gunicorn/19.9.0&#x27;, &#x27;access-control-allow-origin&#x27;: &#x27;*&#x27;, &#x27;access-control-allow-credentials&#x27;: &#x27;true&#x27;&#125;)</span><br><span class="line">&#123;</span><br><span class="line">  &quot;args&quot;: &#123;&#125;, </span><br><span class="line">  &quot;headers&quot;: &#123;</span><br><span class="line">    &quot;Accept&quot;: &quot;*/*&quot;, </span><br><span class="line">    &quot;Accept-Encoding&quot;: &quot;gzip, deflate&quot;, </span><br><span class="line">    &quot;Host&quot;: &quot;httpbin.org&quot;, </span><br><span class="line">    &quot;User-Agent&quot;: &quot;python-httpx/0.27.0&quot;, </span><br><span class="line">    &quot;X-Amzn-Trace-Id&quot;: &quot;Root=1-6641900c-25c2aaf626555ab94c910e08&quot;</span><br><span class="line">  &#125;, </span><br><span class="line">  &quot;origin&quot;: &quot;219.156.133.195&quot;, </span><br><span class="line">  &quot;url&quot;: &quot;https://httpbin.org/get&quot;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>
<p>&emsp;&emsp;输出有状态码，响应头，响应体。，可以在响应体里看到User-Agent是python-httpx&#x2F;0.27.0，代表是httpx请求的。<br>&emsp;&emsp;接下来更改User-Agent再请求一次。  </p>
<figure class="highlight py"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> httpx</span><br><span class="line">headers = &#123;</span><br><span class="line">    <span class="string">&#x27;User-Agent&#x27;</span>: <span class="string">&#x27;Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/124.0.0.0 Safari/537.36 Edg/124.0.0.0&#x27;</span></span><br><span class="line">&#125;</span><br><span class="line">response = httpx.get(<span class="string">&#x27;https://www.httpbin.org/get&#x27;</span>,headers=headers)</span><br><span class="line"><span class="built_in">print</span>(response.text)</span><br><span class="line">&gt;&gt;&gt;</span><br><span class="line">&#123;</span><br><span class="line">  <span class="string">&quot;args&quot;</span>: &#123;&#125;, </span><br><span class="line">  <span class="string">&quot;headers&quot;</span>: &#123;</span><br><span class="line">    <span class="string">&quot;Accept&quot;</span>: <span class="string">&quot;*/*&quot;</span>, </span><br><span class="line">    <span class="string">&quot;Accept-Encoding&quot;</span>: <span class="string">&quot;gzip, deflate&quot;</span>, </span><br><span class="line">    <span class="string">&quot;Host&quot;</span>: <span class="string">&quot;www.httpbin.org&quot;</span>, </span><br><span class="line">    <span class="string">&quot;User-Agent&quot;</span>: <span class="string">&quot;Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/124.0.0.0 Safari/537.36 Edg/124.0.0.0&quot;</span>, </span><br><span class="line">    <span class="string">&quot;X-Amzn-Trace-Id&quot;</span>: <span class="string">&quot;Root=1-664192eb-4fab4c527b08851818d180d9&quot;</span></span><br><span class="line">  &#125;, </span><br><span class="line">  <span class="string">&quot;origin&quot;</span>: <span class="string">&quot;219.156.133.195&quot;</span>, </span><br><span class="line">  <span class="string">&quot;url&quot;</span>: <span class="string">&quot;https://www.httpbin.org/get&quot;</span></span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>
<p>&emsp;&emsp;接下来使用httpx请求<code>https://spa16.scrape.center/</code>这个http&#x2F;2.0的网站</p>
<figure class="highlight py"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> httpx</span><br><span class="line">url = <span class="string">&#x27;https://spa16.scrape.center/&#x27;</span></span><br><span class="line">response = httpx.get(url)</span><br><span class="line"><span class="built_in">print</span>(response.text)</span><br></pre></td></tr></table></figure>
<p>&emsp;&emsp;还是会抛出错误，其实httpx默认是不会开启对HTTP&#x2F;2.0的支持的，默认使用的是HTTP&#x2F;1.1，需要手动声明一下才能使用HTTP&#x2F;2.0</p>
<figure class="highlight py"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> httpx</span><br><span class="line">client = httpx.Client(http2=<span class="literal">True</span>)</span><br><span class="line">url = <span class="string">&#x27;https://spa16.scrape.center/&#x27;</span></span><br><span class="line">response = client.get(url)</span><br><span class="line"><span class="built_in">print</span>(response.text)</span><br></pre></td></tr></table></figure>
<p><img src="/2024/04/28/%E5%9F%BA%E6%9C%AC%E5%BA%93%E7%9A%84%E4%BD%BF%E7%94%A8/12.png"><br>&emsp;&emsp;这里声明了一个Client对象，赋值为client变量，将http2参数设置为True，就开启了对HTTP&#x2F;2.0的支持。然后成功获取了HTML代码。印证了这个示例网站只能使用HTTP&#x2F;2.0访问。<br>&emsp;&emsp;httpx和requests有很多相似的API，对于POST请求、PUT请求和DELETE请求来说，实现方式是类似的：</p>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line">import httpx</span><br><span class="line"></span><br><span class="line">r=httpx.get(&#x27;https://spa16.scrape.center/get&#x27;,params=&#123;&#x27;name&#x27;,&#x27;germy&#x27;&#125;)</span><br><span class="line">r=httpx.post(&#x27;https://spa16.scrape.center/post&#x27;,data=&#123;&#x27;name&#x27;,&#x27;germy&#x27;&#125;)</span><br><span class="line">r=httpx.put(&#x27;https://spa16.scrape.center/put&#x27;)</span><br><span class="line">r=httpx.delete(&#x27;https://spa16.scrape.center/delete&#x27;)</span><br><span class="line">r=httpx.patch(&#x27;https://spa16.scrape.center/patch&#x27;)</span><br></pre></td></tr></table></figure>
<p>&emsp;&emsp;根据得到的Response对象，使用以下属性和方法获取想要的内容。  </p>
<ul>
<li>status_code：状态码</li>
<li>text： 响应体的文本内容</li>
<li>content： 响应体的二进制内容</li>
<li>headers： 响应头，是headers对象，可以用像获取字典中的内容一样获取其中某个Header的值。  </li>
<li>json：方法，调用此方法将文本结果转化为JSON对象。</li>
</ul>
<p>可以参考<a target="_blank" rel="noopener" href="https://www.python-httpx.org/quickstart/">httpx官方文档</a></p>
<h2 id="Client对象"><a href="#Client对象" class="headerlink" title="Client对象"></a>Client对象</h2><p>&emsp;&emsp;httpx中有一个Client对象，可以和requests中的Session对象类比学习。<br>&emsp;&emsp;官方较推荐的使用Client对象的方式是with as语句：</p>
<figure class="highlight py"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> httpx</span><br><span class="line"></span><br><span class="line"><span class="keyword">with</span> httpx.Client() <span class="keyword">as</span> client:</span><br><span class="line">    response = client.get(<span class="string">&#x27;https://www.httpbin.org/get&#x27;</span>)</span><br><span class="line">    <span class="built_in">print</span>(response)</span><br><span class="line">&gt;&gt;&gt;</span><br><span class="line">&lt;Response [<span class="number">200</span> OK]&gt;</span><br></pre></td></tr></table></figure>
<p>这种用法等价于：</p>
<figure class="highlight py"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> httpx</span><br><span class="line"></span><br><span class="line">client = httpx.Client()</span><br><span class="line"><span class="keyword">try</span>:</span><br><span class="line">    response = client.get(<span class="string">&#x27;http://httpbin.org&#x27;</span>)</span><br><span class="line"><span class="keyword">finally</span>:</span><br><span class="line">    client.close()</span><br><span class="line">    <span class="built_in">print</span>(response)</span><br></pre></td></tr></table></figure>
<p>&emsp;&emsp;这两种方式的运行结果一样，这里需要在最后显式调用close方法来关闭Client对象。<br>&emsp;&emsp;在声明Client对象时可以指定一些参数，headers，这样使用该对象发起的所有请求都会默认带上这些参数设置。<br>示例：</p>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><span class="line">import httpx</span><br><span class="line">url = &#x27;https://www.httpbin.org/headers&#x27;</span><br><span class="line">headers = &#123;&#x27;User-Agent&#x27;: &#x27;my-app/0.0.1&#x27;&#125;</span><br><span class="line">with httpx.Client(headers=headers) as client:</span><br><span class="line">    r=client.get(url)</span><br><span class="line">    print(r.json()[&#x27;headers&#x27;][&#x27;User-Agent&#x27;])</span><br><span class="line">&gt;&gt;&gt;</span><br><span class="line">my-app/0.0.1</span><br></pre></td></tr></table></figure>
<p>&emsp;&emsp;这里声明了headers变量，内容为User-Agent属性，然后将此变量传递给headers参数初始化一个Client对象，赋值给client变量，用client变量请求测试网站，打印User-Agent内容。<br>更多用法参考<a target="_blank" rel="noopener" href="https://www.python-httpx.org/advanced/clients/">官方文档</a>。  </p>
<h2 id="支持HTTP-2-0"><a href="#支持HTTP-2-0" class="headerlink" title="支持HTTP&#x2F;2.0"></a>支持HTTP&#x2F;2.0</h2><p>&emsp;&emsp;要声明Client对象，然后将http2参数设置为True，如果不设置，就默认支持HTTP&#x2F;1.1，是不能开启对HTTP&#x2F;2.0的支持。<br>示例：</p>
<figure class="highlight py"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> httpx</span><br><span class="line">client = httpx.Client(http2=<span class="literal">True</span>)</span><br><span class="line">url = <span class="string">&#x27;https://www.httpbin.org/get&#x27;</span></span><br><span class="line">response = client.get(url)</span><br><span class="line"><span class="built_in">print</span>(response.text)</span><br><span class="line"><span class="built_in">print</span>(response.http_version)</span><br><span class="line">&gt;&gt;&gt;</span><br><span class="line">&#123;</span><br><span class="line">  <span class="string">&quot;args&quot;</span>: &#123;&#125;, </span><br><span class="line">  <span class="string">&quot;headers&quot;</span>: &#123;</span><br><span class="line">    <span class="string">&quot;Accept&quot;</span>: <span class="string">&quot;*/*&quot;</span>, </span><br><span class="line">    <span class="string">&quot;Accept-Encoding&quot;</span>: <span class="string">&quot;gzip, deflate&quot;</span>, </span><br><span class="line">    <span class="string">&quot;Host&quot;</span>: <span class="string">&quot;www.httpbin.org&quot;</span>, </span><br><span class="line">    <span class="string">&quot;User-Agent&quot;</span>: <span class="string">&quot;python-httpx/0.27.0&quot;</span>, </span><br><span class="line">    <span class="string">&quot;X-Amzn-Trace-Id&quot;</span>: <span class="string">&quot;Root=1-6641b1c0-03c067ce641ca1223c927bb3&quot;</span></span><br><span class="line">  &#125;, </span><br><span class="line">  <span class="string">&quot;origin&quot;</span>: <span class="string">&quot;219.156.133.195&quot;</span>, </span><br><span class="line">  <span class="string">&quot;url&quot;</span>: <span class="string">&quot;https://www.httpbin.org/get&quot;</span></span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line">HTTP/<span class="number">2</span></span><br></pre></td></tr></table></figure>
<p>&emsp;&emsp;我们输出了response变量的http_version属性，这时request中不存在的属性。<br>&emsp;&emsp;这里输出的http_version属性值是HTTP&#x2F;2，表明使用了HTTP&#x2F;2.0协议传输。 </p>
<blockquote>
<p>客户端的httpx上启用对HTTP&#x2F;2.0的支持并不意味着请求和响应都通过HTTP&#x2F;2.0传输。需要客户端和服务端都支持HTTP&#x2F;2.0才可以。  </p>
</blockquote>
<h2 id="支持异步请求"><a href="#支持异步请求" class="headerlink" title="支持异步请求"></a>支持异步请求</h2><p>&emsp;&emsp;httpx还支持异步客户端请求（AsyncClient），支持Python的async请求模式。<br>写法：</p>
<figure class="highlight py"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> httpx</span><br><span class="line"><span class="keyword">import</span> asyncio</span><br><span class="line"></span><br><span class="line"><span class="keyword">async</span> <span class="keyword">def</span> <span class="title function_">fetch</span>(<span class="params">url</span>):</span><br><span class="line">    <span class="keyword">async</span> <span class="keyword">with</span> httpx.AsyncClient(http2=<span class="literal">True</span>) <span class="keyword">as</span> client:</span><br><span class="line">        response = <span class="keyword">await</span> client.get(url)</span><br><span class="line">        <span class="built_in">print</span>(response.text)</span><br><span class="line"></span><br><span class="line"><span class="keyword">if</span> __name__ == <span class="string">&#x27;__main__&#x27;</span>:</span><br><span class="line">    asyncio.get_event_loop().run_until_complete(fetch(<span class="string">&#x27;https://www.httpbin.org/get&#x27;</span>))</span><br><span class="line">&gt;&gt;&gt;</span><br><span class="line">&#123;</span><br><span class="line">  <span class="string">&quot;args&quot;</span>: &#123;&#125;, </span><br><span class="line">  <span class="string">&quot;headers&quot;</span>: &#123;</span><br><span class="line">    <span class="string">&quot;Accept&quot;</span>: <span class="string">&quot;*/*&quot;</span>, </span><br><span class="line">    <span class="string">&quot;Accept-Encoding&quot;</span>: <span class="string">&quot;gzip, deflate&quot;</span>, </span><br><span class="line">    <span class="string">&quot;Host&quot;</span>: <span class="string">&quot;www.httpbin.org&quot;</span>, </span><br><span class="line">    <span class="string">&quot;User-Agent&quot;</span>: <span class="string">&quot;python-httpx/0.27.0&quot;</span>, </span><br><span class="line">    <span class="string">&quot;X-Amzn-Trace-Id&quot;</span>: <span class="string">&quot;Root=1-6641b38e-6d8e79bb0d6cf9c909831d2f&quot;</span></span><br><span class="line">  &#125;, </span><br><span class="line">  <span class="string">&quot;origin&quot;</span>: <span class="string">&quot;219.156.133.195&quot;</span>, </span><br><span class="line">  <span class="string">&quot;url&quot;</span>: <span class="string">&quot;https://www.httpbin.org/get&quot;</span></span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>
<p>这里了解即可。详细可参考<a target="_blank" rel="noopener" href="https://www.python-httpx.org/async/">官方文档</a><br>。</p>
<h1 id="基础实例案例"><a href="#基础实例案例" class="headerlink" title="基础实例案例"></a>基础实例案例</h1><figure class="highlight py"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br><span class="line">63</span><br><span class="line">64</span><br><span class="line">65</span><br><span class="line">66</span><br><span class="line">67</span><br><span class="line">68</span><br><span class="line">69</span><br><span class="line">70</span><br><span class="line">71</span><br><span class="line">72</span><br><span class="line">73</span><br><span class="line">74</span><br><span class="line">75</span><br><span class="line">76</span><br><span class="line">77</span><br><span class="line">78</span><br><span class="line">79</span><br><span class="line">80</span><br><span class="line">81</span><br><span class="line">82</span><br><span class="line">83</span><br><span class="line">84</span><br><span class="line">85</span><br><span class="line">86</span><br><span class="line">87</span><br><span class="line">88</span><br><span class="line">89</span><br><span class="line">90</span><br><span class="line">91</span><br><span class="line">92</span><br><span class="line">93</span><br><span class="line">94</span><br><span class="line">95</span><br><span class="line">96</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> requests</span><br><span class="line"><span class="keyword">import</span> logging <span class="comment"># logging库用来输出信息</span></span><br><span class="line"><span class="keyword">import</span> re</span><br><span class="line"><span class="keyword">from</span> urllib.parse <span class="keyword">import</span> urljoin</span><br><span class="line"><span class="keyword">import</span> json</span><br><span class="line"><span class="keyword">from</span> os <span class="keyword">import</span>  makedirs</span><br><span class="line"><span class="keyword">from</span> os.path <span class="keyword">import</span> exists</span><br><span class="line"></span><br><span class="line">RESULT_DIR = <span class="string">&#x27;result&#x27;</span>  <span class="comment"># 定义保存数据文件夹</span></span><br><span class="line">exists(RESULT_DIR) <span class="keyword">or</span> makedirs(RESULT_DIR)</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="comment"># 定义日志输出级别和输出格式</span></span><br><span class="line"><span class="comment"># logging.basicConfig(level=logging.INFO,format=&#x27;%(asctime)s-%(levelname)s: %(message)s&#x27;)</span></span><br><span class="line"></span><br><span class="line">BASE_URL = <span class="string">&#x27;https://ssr1.scrape.center&#x27;</span></span><br><span class="line">TOTAL_PAGE = <span class="number">10</span></span><br><span class="line">detail_url=[]</span><br><span class="line"></span><br><span class="line"><span class="comment"># 列表页爬取，返回html</span></span><br><span class="line"><span class="keyword">def</span> <span class="title function_">scrape_page</span>(<span class="params">url</span>):</span><br><span class="line">    logging.info(<span class="string">&#x27;scraping %s ...&#x27;</span>,url)</span><br><span class="line">    <span class="keyword">try</span>:</span><br><span class="line">        response = requests.get(url)</span><br><span class="line">        <span class="keyword">if</span> response.status_code == <span class="number">200</span>:</span><br><span class="line">            <span class="keyword">return</span> response.text</span><br><span class="line">        <span class="keyword">else</span>:</span><br><span class="line">            logging.error(<span class="string">&#x27;get invalid status code %s while scraping %s&#x27;</span>,response.status_code,url)</span><br><span class="line">    <span class="keyword">except</span> requests.RequestException:</span><br><span class="line">        logging.error(<span class="string">&#x27;error occurred while scraping %s&#x27;</span>,url,exc_info=<span class="literal">True</span>)</span><br><span class="line"></span><br><span class="line"><span class="comment"># 获取带有页码的url</span></span><br><span class="line"><span class="keyword">def</span> <span class="title function_">scrape_idnex</span>(<span class="params">page</span>):</span><br><span class="line">    index_url = urljoin(BASE_URL,<span class="string">&quot;/page/&quot;</span>+<span class="built_in">str</span>(page))</span><br><span class="line">    <span class="keyword">return</span> index_url</span><br><span class="line"></span><br><span class="line"><span class="comment"># 获取详情页URL</span></span><br><span class="line"><span class="keyword">def</span> <span class="title function_">parse_index</span>(<span class="params">html</span>):</span><br><span class="line">    pattern = re.<span class="built_in">compile</span>(<span class="string">&#x27;&lt;a.*?href=&quot;(.*?)&quot; class=&quot;name&quot;&gt;&#x27;</span>)</span><br><span class="line">    items = re.findall(pattern,html)</span><br><span class="line">    <span class="keyword">if</span> <span class="keyword">not</span> items:</span><br><span class="line">        <span class="keyword">return</span> []</span><br><span class="line">    <span class="keyword">else</span>:</span><br><span class="line">        <span class="keyword">for</span> item <span class="keyword">in</span> items:</span><br><span class="line">            detail_url.append(urljoin(BASE_URL,item))</span><br><span class="line">            logging.info(<span class="string">&#x27;get detail url %s&#x27;</span>,urljoin(BASE_URL,item))</span><br><span class="line"></span><br><span class="line"><span class="comment"># 详情页爬取</span></span><br><span class="line"><span class="keyword">def</span> <span class="title function_">scrape_detail</span>(<span class="params">url</span>):</span><br><span class="line">    <span class="keyword">return</span> scrape_page(url)</span><br><span class="line"></span><br><span class="line"><span class="comment"># 详情页解析</span></span><br><span class="line"><span class="keyword">def</span> <span class="title function_">parse_detail</span>(<span class="params">html</span>):</span><br><span class="line">    cover_pattern = re.<span class="built_in">compile</span>(<span class="string">&#x27;id=&quot;detail&quot;.*?&lt;a.*?&gt;.*?&lt;img.*?&quot;&quot;.*?src=&quot;(.*?)&quot;.*?class=&quot;cover&quot;&gt;.*?&lt;/a&gt;&#x27;</span>, re.S)</span><br><span class="line">    name_pattern = re.<span class="built_in">compile</span>(<span class="string">&#x27;&lt;h2.*?&gt;(.*?)&lt;/h2&gt;&#x27;</span>)</span><br><span class="line">    categories_pattern = re.<span class="built_in">compile</span>(<span class="string">&#x27;&lt;button.*?category.*?&gt;.*?&lt;span&gt;(.*?)&lt;/span&gt;.*?&lt;/button&gt;&#x27;</span>, re.S)</span><br><span class="line">    info_time_pattern = re.<span class="built_in">compile</span>(<span class="string">&#x27;&lt;div.*?info&quot;&gt;.*?&lt;span.*?&gt;(\d&#123;4&#125;-\d&#123;2&#125;-\d&#123;2&#125;) 上映&lt;/span&gt;.*?&lt;/div&gt;&#x27;</span>,re.S)</span><br><span class="line">    score_pattern = re.<span class="built_in">compile</span>(<span class="string">&#x27;&lt;p.*?score.*?&gt;(.*?)&lt;/p&gt;&#x27;</span>,re.S)</span><br><span class="line">    drama_pattern = re.<span class="built_in">compile</span>(<span class="string">&#x27;&lt;div.*?class=&quot;drama&quot;&gt;&lt;h3.*?&gt;剧情简介&lt;/h3&gt;.*?&lt;p.*?&gt;(.*?)&lt;/p&gt;&lt;/div&gt;&#x27;</span>,re.S)</span><br><span class="line">    cover = re.search(cover_pattern,html).group(<span class="number">1</span>).strip() <span class="keyword">if</span> re.search(cover_pattern,html) <span class="keyword">else</span> <span class="literal">None</span></span><br><span class="line">    name = re.search(name_pattern,html).group(<span class="number">1</span>).strip() <span class="keyword">if</span> re.search(name_pattern,html) <span class="keyword">else</span> <span class="literal">None</span></span><br><span class="line">    categories = re.findall(categories_pattern,html) <span class="keyword">if</span> re.findall(categories_pattern,html) <span class="keyword">else</span> <span class="literal">None</span></span><br><span class="line">    info_time = re.search(info_time_pattern,html).group(<span class="number">1</span>) <span class="keyword">if</span> re.search(info_time_pattern,html) <span class="keyword">else</span> <span class="literal">None</span></span><br><span class="line">    score = re.search(score_pattern,html).group(<span class="number">1</span>).strip() <span class="keyword">if</span> re.search(score_pattern,html) <span class="keyword">else</span> <span class="literal">None</span></span><br><span class="line">    drama = re.search(drama_pattern,html).group(<span class="number">1</span>).strip() <span class="keyword">if</span> re.search(drama_pattern,html) <span class="keyword">else</span> <span class="literal">None</span></span><br><span class="line"></span><br><span class="line">    <span class="keyword">return</span> &#123;</span><br><span class="line">        <span class="string">&#x27;cover&#x27;</span>:cover,</span><br><span class="line">        <span class="string">&#x27;name&#x27;</span>: name,</span><br><span class="line">        <span class="string">&#x27;categories&#x27;</span>: categories,</span><br><span class="line">        <span class="string">&#x27;info_time&#x27;</span>: info_time,</span><br><span class="line">        <span class="string">&#x27;score&#x27;</span>: score,</span><br><span class="line">        <span class="string">&#x27;drama&#x27;</span>:drama</span><br><span class="line">    &#125;</span><br><span class="line"></span><br><span class="line"><span class="comment"># 保存数据</span></span><br><span class="line"><span class="keyword">def</span> <span class="title function_">save_data</span>(<span class="params">data</span>):</span><br><span class="line">    name = data.get(<span class="string">&#x27;name&#x27;</span>)</span><br><span class="line">    data_path = <span class="string">f&#x27;<span class="subst">&#123;RESULT_DIR&#125;</span>/<span class="subst">&#123;name&#125;</span>.json&#x27;</span></span><br><span class="line">    json.dump(data,<span class="built_in">open</span>(data_path,<span class="string">&#x27;w&#x27;</span>,encoding=<span class="string">&#x27;utf-8&#x27;</span>),ensure_ascii=<span class="literal">False</span>,indent=<span class="number">2</span>)</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="keyword">def</span> <span class="title function_">main</span>():</span><br><span class="line">    <span class="keyword">for</span> page <span class="keyword">in</span> <span class="built_in">range</span>(<span class="number">1</span>,TOTAL_PAGE+<span class="number">1</span>):</span><br><span class="line">        page_index=scrape_idnex(page)</span><br><span class="line">        html=scrape_page(page_index)</span><br><span class="line">        parse_index(html)</span><br><span class="line">    <span class="keyword">for</span> items <span class="keyword">in</span> detail_url:</span><br><span class="line">        <span class="built_in">print</span>(items)</span><br><span class="line">        html_detail=scrape_page(items)</span><br><span class="line">        detail=parse_detail(html_detail)</span><br><span class="line">        save_data(detail)</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="keyword">if</span> __name__ == <span class="string">&#x27;__main__&#x27;</span>:</span><br><span class="line">    main()</span><br></pre></td></tr></table></figure>















































    </div>

    
    
    

    <footer class="post-footer">
          <div class="post-tags">
              <a href="/tags/%E5%9F%BA%E6%9C%AC%E5%BA%93%E7%9A%84%E4%BD%BF%E7%94%A8/" rel="tag">基本库的使用</a>
          </div>

        

          <div class="post-nav">
            <div class="post-nav-item">
                <a href="/2024/04/25/%E7%88%AC%E8%99%AB%E5%9F%BA%E7%A1%80/" rel="prev" title="爬虫基础">
                  <i class="fa fa-angle-left"></i> 爬虫基础
                </a>
            </div>
            <div class="post-nav-item">
                <a href="/2024/05/06/%E5%9C%A8kali%E5%88%9B%E5%BB%BA%E5%A5%BD%E4%B9%8B%E5%90%8E%E8%BF%9B%E8%A1%8C%E7%9A%84%E4%B8%80%E7%B3%BB%E5%88%97%E4%BF%AE%E6%94%B9/" rel="next" title="kali相关配置">
                  kali相关配置 <i class="fa fa-angle-right"></i>
                </a>
            </div>
          </div>
    </footer>
  </article>
</div>






</div>
  </main>

  <footer class="footer">
    <div class="footer-inner">

  <div class="copyright">
    &copy; 
    <span itemprop="copyrightYear">2024</span>
    <span class="with-love">
      <i class="fa fa-heart"></i>
    </span>
    <span class="author" itemprop="copyrightHolder">白乐天</span>
  </div>

<span id="timeDate">载入天数...</span>
<span id="times">载入时分秒...</span>
<script>
    var now = new Date();
    function createtime() {
        var grt= new Date("01/01/2024 00:00:00"); //修改为你的网站开始运行的时间
        now.setTime(now.getTime()+250);
        days = (now - grt ) / 1000 / 60 / 60 / 24; dnum = Math.floor(days);
        hours = (now - grt ) / 1000 / 60 / 60 - (24 * dnum); hnum = Math.floor(hours);
        if(String(hnum).length ==1 ){hnum = "0" + hnum;} minutes = (now - grt ) / 1000 /60 - (24 * 60 * dnum) - (60 * hnum);
        mnum = Math.floor(minutes); if(String(mnum).length ==1 ){mnum = "0" + mnum;}
        seconds = (now - grt ) / 1000 - (24 * 60 * 60 * dnum) - (60 * 60 * hnum) - (60 * mnum);
        snum = Math.round(seconds); if(String(snum).length ==1 ){snum = "0" + snum;}
        document.getElementById("timeDate").innerHTML = "本站已安全运行 "+dnum+" 天 ";
        document.getElementById("times").innerHTML = hnum + " 小时 " + mnum + " 分 " + snum + " 秒.";
    }
setInterval("createtime()",250);
</script>
    </div>
  </footer>

  

<noscript>
  <div class="noscript-warning">Theme NexT works best with JavaScript enabled</div>
</noscript>


  
  <script src="https://cdnjs.cloudflare.com/ajax/libs/animejs/3.2.1/anime.min.js" integrity="sha256-XL2inqUJaslATFnHdJOi9GfQ60on8Wx1C2H8DYiN1xY=" crossorigin="anonymous"></script>
<script src="/js/comments.js"></script><script src="/js/utils.js"></script><script src="/js/motion.js"></script><script src="/js/next-boot.js"></script>

  






  





</body>
</html>
