<!DOCTYPE html>
<html lang="zh-CN">
<head>
  <meta charset="UTF-8">
<meta name="viewport" content="width=device-width">
<meta name="theme-color" content="#222"><meta name="generator" content="Hexo 7.1.1">

  <link rel="apple-touch-icon" sizes="180x180" href="/images/apple-touch-icon-next.png">
  <link rel="icon" type="image/png" sizes="32x32" href="/images/favicon-32x32-next.png">
  <link rel="icon" type="image/png" sizes="16x16" href="/images/favicon-16x16-next.png">
  <link rel="mask-icon" href="/images/logo.svg" color="#222">

<link rel="stylesheet" href="/css/main.css">



<link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/6.5.1/css/all.min.css" integrity="sha256-wiz7ZSCn/btzhjKDQBms9Hx4sSeUYsDrTLg7roPstac=" crossorigin="anonymous">
  <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/animate.css/3.1.1/animate.min.css" integrity="sha256-PR7ttpcvz8qrF57fur/yAx1qXMFJeJFiA6pSzWi0OIE=" crossorigin="anonymous">

<script class="next-config" data-name="main" type="application/json">{"hostname":"example.com","root":"/","images":"/images","scheme":"Pisces","darkmode":false,"version":"8.19.2","exturl":false,"sidebar":{"position":"left","display":"post","padding":18,"offset":12},"copycode":{"enable":true,"style":"mac"},"fold":{"enable":true,"height":500},"bookmark":{"enable":false,"color":"#222","save":"auto"},"mediumzoom":false,"lazyload":false,"pangu":false,"comments":{"style":"tabs","active":null,"storage":true,"lazyload":false,"nav":null},"stickytabs":false,"motion":{"enable":true,"async":false,"transition":{"menu_item":"fadeInDown","post_block":"fadeIn","post_header":"fadeInDown","post_body":"fadeInDown","coll_header":"fadeInLeft","sidebar":"fadeInUp"}},"i18n":{"placeholder":"搜索...","empty":"没有找到任何搜索结果：${query}","hits_time":"找到 ${hits} 个搜索结果（用时 ${time} 毫秒）","hits":"找到 ${hits} 个搜索结果"}}</script><script src="/js/config.js"></script>

    <meta name="description" content="urlliburllib是一个Python库，利用它可以实现HTTP请求的发送，我们要做的是指定请求的URL、请求头、请求体等信息。此外urllib还可以把服务器返回的对象转化为Python对象，通过该对象可以方便地获取响应的相关信息，如响应状态码、响应头、响应体。urllib库包含如下4个模块。  request: 最基本的http请求模块，可以模拟请求的发送。 error: 异常处理模块。 p">
<meta property="og:type" content="article">
<meta property="og:title" content="基本库的使用">
<meta property="og:url" content="http://example.com/2024/04/28/%E5%9F%BA%E6%9C%AC%E5%BA%93%E7%9A%84%E4%BD%BF%E7%94%A8/index.html">
<meta property="og:site_name" content="白乐天">
<meta property="og:description" content="urlliburllib是一个Python库，利用它可以实现HTTP请求的发送，我们要做的是指定请求的URL、请求头、请求体等信息。此外urllib还可以把服务器返回的对象转化为Python对象，通过该对象可以方便地获取响应的相关信息，如响应状态码、响应头、响应体。urllib库包含如下4个模块。  request: 最基本的http请求模块，可以模拟请求的发送。 error: 异常处理模块。 p">
<meta property="og:locale" content="zh_CN">
<meta property="og:image" content="http://example.com/2024/04/28/%E5%9F%BA%E6%9C%AC%E5%BA%93%E7%9A%84%E4%BD%BF%E7%94%A8/1.png">
<meta property="og:image" content="http://example.com/2024/04/28/%E5%9F%BA%E6%9C%AC%E5%BA%93%E7%9A%84%E4%BD%BF%E7%94%A8/2.png">
<meta property="og:image" content="http://example.com/2024/04/28/%E5%9F%BA%E6%9C%AC%E5%BA%93%E7%9A%84%E4%BD%BF%E7%94%A8/3.png">
<meta property="og:image" content="http://example.com/2024/04/28/%E5%9F%BA%E6%9C%AC%E5%BA%93%E7%9A%84%E4%BD%BF%E7%94%A8/4.png">
<meta property="og:image" content="http://example.com/2024/04/28/%E5%9F%BA%E6%9C%AC%E5%BA%93%E7%9A%84%E4%BD%BF%E7%94%A8/5.png">
<meta property="og:image" content="http://example.com/2024/04/28/%E5%9F%BA%E6%9C%AC%E5%BA%93%E7%9A%84%E4%BD%BF%E7%94%A8/6.png">
<meta property="og:image" content="http://example.com/2024/04/28/%E5%9F%BA%E6%9C%AC%E5%BA%93%E7%9A%84%E4%BD%BF%E7%94%A8/7.png">
<meta property="og:image" content="http://example.com/2024/04/28/%E5%9F%BA%E6%9C%AC%E5%BA%93%E7%9A%84%E4%BD%BF%E7%94%A8/8.png">
<meta property="article:published_time" content="2024-04-28T12:40:18.000Z">
<meta property="article:modified_time" content="2024-05-06T09:02:25.322Z">
<meta property="article:author" content="白乐天">
<meta property="article:tag" content="基本库的使用">
<meta name="twitter:card" content="summary">
<meta name="twitter:image" content="http://example.com/2024/04/28/%E5%9F%BA%E6%9C%AC%E5%BA%93%E7%9A%84%E4%BD%BF%E7%94%A8/1.png">


<link rel="canonical" href="http://example.com/2024/04/28/%E5%9F%BA%E6%9C%AC%E5%BA%93%E7%9A%84%E4%BD%BF%E7%94%A8/">



<script class="next-config" data-name="page" type="application/json">{"sidebar":"","isHome":false,"isPost":true,"lang":"zh-CN","comments":true,"permalink":"http://example.com/2024/04/28/%E5%9F%BA%E6%9C%AC%E5%BA%93%E7%9A%84%E4%BD%BF%E7%94%A8/","path":"2024/04/28/基本库的使用/","title":"基本库的使用"}</script>

<script class="next-config" data-name="calendar" type="application/json">""</script>
<title>基本库的使用 | 白乐天</title>
  








  <noscript>
    <link rel="stylesheet" href="/css/noscript.css">
  </noscript>
</head>

<body itemscope itemtype="http://schema.org/WebPage" class="use-motion">
  <div class="headband"></div>

  <main class="main">
    <div class="column">
      <header class="header" itemscope itemtype="http://schema.org/WPHeader"><div class="site-brand-container">
  <div class="site-nav-toggle">
    <div class="toggle" aria-label="切换导航栏" role="button">
        <span class="toggle-line"></span>
        <span class="toggle-line"></span>
        <span class="toggle-line"></span>
    </div>
  </div>

  <div class="site-meta">

    <a href="/" class="brand" rel="start">
      <i class="logo-line"></i>
      <p class="site-title">白乐天</p>
      <i class="logo-line"></i>
    </a>
  </div>

  <div class="site-nav-right">
    <div class="toggle popup-trigger" aria-label="搜索" role="button">
    </div>
  </div>
</div>



<nav class="site-nav">
  <ul class="main-menu menu"><li class="menu-item menu-item-home"><a href="/" rel="section"><i class="fa fa-home fa-fw"></i>首页</a></li><li class="menu-item menu-item-tags"><a href="/tags/" rel="section"><i class="fa fa-tags fa-fw"></i>标签<span class="badge">32</span></a></li><li class="menu-item menu-item-categories"><a href="/categories/" rel="section"><i class="fa fa-th fa-fw"></i>分类<span class="badge">16</span></a></li><li class="menu-item menu-item-archives"><a href="/archives/" rel="section"><i class="fa fa-archive fa-fw"></i>归档<span class="badge">49</span></a></li>
  </ul>
</nav>




</header>
        
  
  <aside class="sidebar">

    <div class="sidebar-inner sidebar-nav-active sidebar-toc-active">
      <ul class="sidebar-nav">
        <li class="sidebar-nav-toc">
          文章目录
        </li>
        <li class="sidebar-nav-overview">
          站点概览
        </li>
      </ul>

      <div class="sidebar-panel-container">
        <!--noindex-->
        <div class="post-toc-wrap sidebar-panel">
            <div class="post-toc animated"><ol class="nav"><li class="nav-item nav-level-1"><a class="nav-link" href="#urllib"><span class="nav-number">1.</span> <span class="nav-text">urllib</span></a><ol class="nav-child"><li class="nav-item nav-level-2"><a class="nav-link" href="#%E5%8F%91%E9%80%81%E8%AF%B7%E6%B1%82"><span class="nav-number">1.1.</span> <span class="nav-text">发送请求</span></a><ol class="nav-child"><li class="nav-item nav-level-3"><a class="nav-link" href="#urlopen"><span class="nav-number">1.1.1.</span> <span class="nav-text">urlopen</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#data%E5%8F%82%E6%95%B0"><span class="nav-number">1.1.2.</span> <span class="nav-text">data参数</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#timeout%E5%8F%82%E6%95%B0"><span class="nav-number">1.1.3.</span> <span class="nav-text">timeout参数</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#%E5%85%B6%E4%BB%96%E5%8F%82%E6%95%B0"><span class="nav-number">1.1.4.</span> <span class="nav-text">其他参数</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#Request"><span class="nav-number">1.1.5.</span> <span class="nav-text">Request</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#%E9%AB%98%E7%BA%A7%E7%94%A8%E6%B3%95"><span class="nav-number">1.1.6.</span> <span class="nav-text">高级用法</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#%E9%AA%8C%E8%AF%81"><span class="nav-number">1.1.7.</span> <span class="nav-text">验证</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#%E4%BB%A3%E7%90%86"><span class="nav-number">1.1.8.</span> <span class="nav-text">代理</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#Cookie"><span class="nav-number">1.1.9.</span> <span class="nav-text">Cookie</span></a></li></ol></li><li class="nav-item nav-level-2"><a class="nav-link" href="#%E5%A4%84%E7%90%86%E5%BC%82%E5%B8%B8"><span class="nav-number">1.2.</span> <span class="nav-text">处理异常</span></a><ol class="nav-child"><li class="nav-item nav-level-3"><a class="nav-link" href="#URLError"><span class="nav-number">1.2.1.</span> <span class="nav-text">URLError</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#HTTPError"><span class="nav-number">1.2.2.</span> <span class="nav-text">HTTPError</span></a></li></ol></li><li class="nav-item nav-level-2"><a class="nav-link" href="#%E8%A7%A3%E6%9E%90%E9%93%BE%E6%8E%A5"><span class="nav-number">1.3.</span> <span class="nav-text">解析链接</span></a><ol class="nav-child"><li class="nav-item nav-level-3"><a class="nav-link" href="#urlparse"><span class="nav-number">1.3.1.</span> <span class="nav-text">urlparse</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#urlunparse"><span class="nav-number">1.3.2.</span> <span class="nav-text">urlunparse</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#urlsplit"><span class="nav-number">1.3.3.</span> <span class="nav-text">urlsplit</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#urlunsplit"><span class="nav-number">1.3.4.</span> <span class="nav-text">urlunsplit</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#urljoin"><span class="nav-number">1.3.5.</span> <span class="nav-text">urljoin</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#urlencode"><span class="nav-number">1.3.6.</span> <span class="nav-text">urlencode</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#parse-qs"><span class="nav-number">1.3.7.</span> <span class="nav-text">parse_qs</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#parse-qsl"><span class="nav-number">1.3.8.</span> <span class="nav-text">parse_qsl</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#quote"><span class="nav-number">1.3.9.</span> <span class="nav-text">quote</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#unquote"><span class="nav-number">1.3.10.</span> <span class="nav-text">unquote</span></a></li></ol></li><li class="nav-item nav-level-2"><a class="nav-link" href="#%E5%88%86%E6%9E%90Robots%E5%8D%8F%E8%AE%AE"><span class="nav-number">1.4.</span> <span class="nav-text">分析Robots协议</span></a><ol class="nav-child"><li class="nav-item nav-level-3"><a class="nav-link" href="#Robots%E5%8D%8F%E8%AE%AE"><span class="nav-number">1.4.1.</span> <span class="nav-text">Robots协议</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#robotparser"><span class="nav-number">1.4.2.</span> <span class="nav-text">robotparser</span></a></li></ol></li></ol></li><li class="nav-item nav-level-1"><a class="nav-link" href="#requests%E7%9A%84%E4%BD%BF%E7%94%A8"><span class="nav-number">2.</span> <span class="nav-text">requests的使用</span></a></li></ol></div>
        </div>
        <!--/noindex-->

        <div class="site-overview-wrap sidebar-panel">
          <div class="site-author animated" itemprop="author" itemscope itemtype="http://schema.org/Person">
    <img class="site-author-image" itemprop="image" alt="白乐天"
      src="/images/android.svg">
  <p class="site-author-name" itemprop="name">白乐天</p>
  <div class="site-description" itemprop="description">在安卓逆向和爬虫的道路上渐行渐远!</div>
</div>
<div class="site-state-wrap animated">
  <nav class="site-state">
      <div class="site-state-item site-state-posts">
        <a href="/archives/">
          <span class="site-state-item-count">49</span>
          <span class="site-state-item-name">日志</span>
        </a>
      </div>
      <div class="site-state-item site-state-categories">
          <a href="/categories/">
        <span class="site-state-item-count">16</span>
        <span class="site-state-item-name">分类</span></a>
      </div>
      <div class="site-state-item site-state-tags">
          <a href="/tags/">
        <span class="site-state-item-count">32</span>
        <span class="site-state-item-name">标签</span></a>
      </div>
  </nav>
</div>
  <div class="links-of-author animated">
      <span class="links-of-author-item">
        <a href="https://github.com/blttttt" title="GitHub → https:&#x2F;&#x2F;github.com&#x2F;blttttt" rel="noopener me" target="_blank"><i class="fab fa-github fa-fw"></i>GitHub</a>
      </span>
  </div>

        </div>
      </div>
        <div class="back-to-top animated" role="button" aria-label="返回顶部">
          <i class="fa fa-arrow-up"></i>
          <span>0%</span>
        </div>
    </div>

    
  </aside>


    </div>

    <div class="main-inner post posts-expand">


  


<div class="post-block">
  
  

  <article itemscope itemtype="http://schema.org/Article" class="post-content" lang="zh-CN">
    <link itemprop="mainEntityOfPage" href="http://example.com/2024/04/28/%E5%9F%BA%E6%9C%AC%E5%BA%93%E7%9A%84%E4%BD%BF%E7%94%A8/">

    <span hidden itemprop="author" itemscope itemtype="http://schema.org/Person">
      <meta itemprop="image" content="/images/android.svg">
      <meta itemprop="name" content="白乐天">
    </span>

    <span hidden itemprop="publisher" itemscope itemtype="http://schema.org/Organization">
      <meta itemprop="name" content="白乐天">
      <meta itemprop="description" content="在安卓逆向和爬虫的道路上渐行渐远!">
    </span>

    <span hidden itemprop="post" itemscope itemtype="http://schema.org/CreativeWork">
      <meta itemprop="name" content="基本库的使用 | 白乐天">
      <meta itemprop="description" content="">
    </span>
      <header class="post-header">
        <h1 class="post-title" itemprop="name headline">
          基本库的使用
        </h1>

        <div class="post-meta-container">
          <div class="post-meta">
    <span class="post-meta-item">
      <span class="post-meta-item-icon">
        <i class="far fa-calendar"></i>
      </span>
      <span class="post-meta-item-text">发表于</span>

      <time title="创建时间：2024-04-28 20:40:18" itemprop="dateCreated datePublished" datetime="2024-04-28T20:40:18+08:00">2024-04-28</time>
    </span>
    <span class="post-meta-item">
      <span class="post-meta-item-icon">
        <i class="far fa-calendar-check"></i>
      </span>
      <span class="post-meta-item-text">更新于</span>
      <time title="修改时间：2024-05-06 17:02:25" itemprop="dateModified" datetime="2024-05-06T17:02:25+08:00">2024-05-06</time>
    </span>
    <span class="post-meta-item">
      <span class="post-meta-item-icon">
        <i class="far fa-folder"></i>
      </span>
      <span class="post-meta-item-text">分类于</span>
        <span itemprop="about" itemscope itemtype="http://schema.org/Thing">
          <a href="/categories/%E7%88%AC%E8%99%AB/" itemprop="url" rel="index"><span itemprop="name">爬虫</span></a>
        </span>
    </span>

  
</div>

        </div>
      </header>

    
    
    
    <div class="post-body" itemprop="articleBody"><h1 id="urllib"><a href="#urllib" class="headerlink" title="urllib"></a>urllib</h1><p>urllib是一个Python库，利用它可以实现HTTP请求的发送，我们要做的是指定请求的URL、请求头、请求体等信息。此外urllib还可以把服务器返回的对象转化为Python对象，通过该对象可以方便地获取响应的相关信息，如响应状态码、响应头、响应体。<br>urllib库包含如下4个模块。</p>
<ul>
<li>request: 最基本的http请求模块，可以模拟请求的发送。</li>
<li>error: 异常处理模块。</li>
<li>parse: 一个工具模块。</li>
<li>robotparser: 主要用来识别网站的robots.txt文件，然后判断哪些网站可以爬，哪些网站不可以爬。</li>
</ul>
<h2 id="发送请求"><a href="#发送请求" class="headerlink" title="发送请求"></a>发送请求</h2><h3 id="urlopen"><a href="#urlopen" class="headerlink" title="urlopen"></a>urlopen</h3><p>urllib.request模块提供了最基本的构造HTTP请求的方法，利用这个模块可以模拟浏览器的请求发起过程。用百度测试</p>
<figure class="highlight py"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> urllib.request</span><br><span class="line">response = urllib.request.urlopen(<span class="string">&#x27;https://www.baidu.com&#x27;</span>)</span><br><span class="line"><span class="built_in">print</span>(response.read().decode(<span class="string">&#x27;utf-8&#x27;</span>))</span><br></pre></td></tr></table></figure>
<p><img src="/2024/04/28/%E5%9F%BA%E6%9C%AC%E5%BA%93%E7%9A%84%E4%BD%BF%E7%94%A8/1.png"></p>
<blockquote>
<p>response.read()方法<br>属于urllib.response对象，用来从HTTP响应中读取数据。read()方法会读取HTTP响应的主体（body），这通常是你请求的页面的HTML内容或API返回的JSON数据等。read()方法返回的是字节串（bytes），因为HTTP协议传输的是字节数据。由于返回的是字节串，通常需要将其解码为字符串才能阅读。常见的解码方式是使用.decode(‘utf-8’)，这假设响应内容是使用UTF-8编码的。除了read()，urllib.response对象还提供了其他方法来读取数据，如readline()（读取一行数据）和readlines()（读取所有行并返回一个列表）。</p>
</blockquote>
<figure class="highlight py"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> urllib.request</span><br><span class="line">response = urllib.request.urlopen(<span class="string">&#x27;https://www.baidu.com&#x27;</span>)</span><br><span class="line"><span class="built_in">print</span>(<span class="built_in">type</span>(response))</span><br><span class="line">&gt;&gt;&gt;</span><br><span class="line">&lt;<span class="keyword">class</span> <span class="string">&#x27;http.client.HTTPResponse&#x27;</span>&gt;</span><br></pre></td></tr></table></figure>
<p>这里响应的是<code>HTTPResponse</code>类型的对象，包含read、readinto、getheader、getheaders、fileno等方法。得到响应把它赋值给了response，可以通过response对象来调用哪些方法和属性。  </p>
<p>打印响应的属性</p>
<figure class="highlight py"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> urllib.request</span><br><span class="line">response = urllib.request.urlopen(<span class="string">&#x27;https://www.baidu.com&#x27;</span>)</span><br><span class="line"><span class="built_in">print</span>(response.status)  //响应状态码</span><br><span class="line"><span class="built_in">print</span>(response.getheaders())  //响应头的信息</span><br><span class="line"><span class="built_in">print</span>(response.getheader(<span class="string">&quot;Server&quot;</span>))  //提供参数，获取响应头对应参数的信息</span><br><span class="line"><span class="built_in">print</span>(response.read().decode(<span class="string">&#x27;utf-8&#x27;</span>))</span><br></pre></td></tr></table></figure>


<h3 id="data参数"><a href="#data参数" class="headerlink" title="data参数"></a>data参数</h3><p>&emsp;&emsp;data参数可选。在添加该参数的时候，需要使用bytes方法将参数转化为字节流编码格式的内容，即bytes类型。如果传递了这个参数，那么它的请求方式就不再是GET而是POST了。<br>我们请求的站点是<code>https://www.httpbin.org</code> ，它可以提供HTTP请求测试。本次请求的URL是 <code>https://www.httpbin.org/post</code>，这个连接可以测试POST请求，能够输出请求的一些信息，其中包含我们传递的data参数。</p>
<figure class="highlight py"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> urllib.request</span><br><span class="line"><span class="keyword">import</span> urllib.parse</span><br><span class="line">data = <span class="built_in">bytes</span>(urllib.parse.urlencode(&#123;<span class="string">&#x27;name&#x27;</span>:<span class="string">&#x27;germy&#x27;</span>&#125;),encoding=<span class="string">&#x27;utf-8&#x27;</span>)</span><br><span class="line">response = urllib.request.urlopen(<span class="string">&#x27;https://www.httpbin.org/post&#x27;</span>,data)</span><br><span class="line"><span class="built_in">print</span>(response.read().decode(<span class="string">&#x27;utf-8&#x27;</span>))</span><br></pre></td></tr></table></figure>
<p>我们传递的参数出现在了form字段中，表明是模拟表单提交，以POST方式传输数据。<br><img src="/2024/04/28/%E5%9F%BA%E6%9C%AC%E5%BA%93%E7%9A%84%E4%BD%BF%E7%94%A8/2.png"></p>
<h3 id="timeout参数"><a href="#timeout参数" class="headerlink" title="timeout参数"></a>timeout参数</h3><p>&emsp;&emsp;设置超时时间，单位是秒，如果请求超出了设置的这个时间，还没有得到响应，就会抛出异常，如果不指定这个参数，会使用全局默认时间。<br>实例</p>
<figure class="highlight py"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> socket</span><br><span class="line"><span class="keyword">import</span> urllib.request</span><br><span class="line">response = urllib.request.urlopen(<span class="string">&#x27;https://www.httpbin.org/get&#x27;</span>,timeout=<span class="number">0.1</span>)</span><br><span class="line"><span class="built_in">print</span>(response.read().decode(<span class="string">&#x27;utf-8&#x27;</span>))</span><br></pre></td></tr></table></figure>
<p>&emsp;&emsp;可能会出现<code>urllib.error.URLError: &lt;urlopen error timed out&gt;</code>这种报错</p>
<p>实例</p>
<figure class="highlight py"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> socket</span><br><span class="line"><span class="keyword">import</span> urllib.request</span><br><span class="line"><span class="keyword">import</span> urllib.parse</span><br><span class="line"><span class="keyword">import</span> urllib.error</span><br><span class="line"><span class="keyword">try</span> :</span><br><span class="line">    response = urllib.request.urlopen(<span class="string">&#x27;https://www.httpbin.org/get&#x27;</span>,timeout=<span class="number">0.1</span>)</span><br><span class="line"><span class="keyword">except</span> urllib.error.URLError <span class="keyword">as</span> e:</span><br><span class="line">    <span class="keyword">if</span> <span class="built_in">isinstance</span>(e.reason,socket.timeout):</span><br><span class="line">        <span class="built_in">print</span>(<span class="string">&#x27;Time Out&#x27;</span>)</span><br><span class="line">&gt;&gt;&gt;</span><br><span class="line">Time Out</span><br></pre></td></tr></table></figure>
<h3 id="其他参数"><a href="#其他参数" class="headerlink" title="其他参数"></a>其他参数</h3><p>&emsp;&emsp;<strong>context</strong>参数，该参数必须是ssl.SSLContext类型，用来指定SSL的设置。<br>&emsp;&emsp;<strong>cafile</strong>和<strong>capate</strong>这两个参数分别用来指定CA证书和其路径，在请求HTTPS连接时会用。  </p>
<h3 id="Request"><a href="#Request" class="headerlink" title="Request"></a>Request</h3><p>&emsp;&emsp;利用urlopen发起的最基本的请求，那几个参数并不足以构建一个完整的请求。如果想往请求中加入Headers信息，就要利用更强大的Request类来构建请求了。  </p>
<figure class="highlight py"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> urllib.request</span><br><span class="line">request = urllib.request.Request(<span class="string">&#x27;http://www.baidu.com&#x27;</span>)</span><br><span class="line">response = urllib.request.urlopen(request)</span><br><span class="line"><span class="built_in">print</span>(response.read().decode(<span class="string">&#x27;utf-8&#x27;</span>))</span><br></pre></td></tr></table></figure>
<p><img src="/2024/04/28/%E5%9F%BA%E6%9C%AC%E5%BA%93%E7%9A%84%E4%BD%BF%E7%94%A8/3.png"><br>这里我们依然用urlopen发送请求，但是这次该方法的参数不是URL了，而是一个Request类型的对象。<br>构造Request类：</p>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">class urllib.request.Request(url,data=None,headers=&#123;&#125;,origin_req_host,unverifiable=False,method=None)</span><br></pre></td></tr></table></figure>
<p>&emsp;&emsp;第一个参数url用于请求URL，是必传参数，其他都是可选参数。<br>&emsp;&emsp;第二个参数data如果要传数据，必须传bytes类型的。如果数据是字典，可以先用urllib.parse模块里的urlencode方法进行编码。<br>&emsp;&emsp;第三个参数headers是一个字典，就是请求头，在构造请求时，既可以通过headers参数直接构造此项，也可以通过调用请求实例的add_header方法添加。<br>&emsp;&emsp;添加请求头的常见方法是，通过修改User-Agent来伪装浏览器。默认的User-Agent是Python-urllib。<br>&emsp;&emsp;第四个参数origin_req_host指的是请求方的host名称或者IP地址。<br>&emsp;&emsp;第五个参数unverifiable表示请求方是否是无法验证的，默认取值是false，指用户没有足够的权限来接收这个请求的结果。<br>&emsp;&emsp;第六个参数是method是一个字符串，用来指示请求使用的方法。<br>&emsp;&emsp;传入多个参数构建Request类：</p>
<figure class="highlight py"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">from</span> urllib <span class="keyword">import</span> request,parse</span><br><span class="line">url = <span class="string">&#x27;http://www.httpbin.org/post&#x27;</span></span><br><span class="line">headers = &#123;<span class="string">&#x27;User-Agent&#x27;</span>: <span class="string">&#x27;Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/124.0.0.0 Safari/537.36 Edg/124.0.0.0&#x27;</span>,</span><br><span class="line">           <span class="string">&#x27;Host&#x27;</span>:<span class="string">&#x27;www.httpbin.org&#x27;</span>&#125;</span><br><span class="line"><span class="built_in">dict</span> = &#123;<span class="string">&#x27;name&#x27;</span>:<span class="string">&#x27;germy&#x27;</span>&#125;</span><br><span class="line">data = <span class="built_in">bytes</span>(parse.urlencode(<span class="built_in">dict</span>),encoding=<span class="string">&#x27;utf-8&#x27;</span>)</span><br><span class="line">req = request.Request(url,data,headers,method=<span class="string">&#x27;POST&#x27;</span>)</span><br><span class="line">response = request.urlopen(req)</span><br><span class="line"><span class="built_in">print</span>(response.read().decode(<span class="string">&#x27;utf-8&#x27;</span>))</span><br></pre></td></tr></table></figure>
<p>传入4个参数构造了一个Request类，data用urlencode方法和bytes方法把字典数据转换成字节流格式。<br>运行结果  </p>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br></pre></td><td class="code"><pre><span class="line">&#123;</span><br><span class="line">  &quot;args&quot;: &#123;&#125;, </span><br><span class="line">  &quot;data&quot;: &quot;&quot;, </span><br><span class="line">  &quot;files&quot;: &#123;&#125;, </span><br><span class="line">  &quot;form&quot;: &#123;</span><br><span class="line">    &quot;name&quot;: &quot;germy&quot;</span><br><span class="line">  &#125;, </span><br><span class="line">  &quot;headers&quot;: &#123;</span><br><span class="line">    &quot;Accept-Encoding&quot;: &quot;identity&quot;, </span><br><span class="line">    &quot;Content-Length&quot;: &quot;10&quot;, </span><br><span class="line">    &quot;Content-Type&quot;: &quot;application/x-www-form-urlencoded&quot;, </span><br><span class="line">    &quot;Host&quot;: &quot;www.httpbin.org&quot;, </span><br><span class="line">    &quot;User-Agent&quot;: &quot;Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/124.0.0.0 Safari/537.36 Edg/124.0.0.0&quot;, </span><br><span class="line">    &quot;X-Amzn-Trace-Id&quot;: &quot;Root=1-662f5c7d-78dad94c612f72536858fcd6&quot;</span><br><span class="line">  &#125;, </span><br><span class="line">  &quot;json&quot;: null, </span><br><span class="line">  &quot;origin&quot;: &quot;219.156.133.195&quot;, </span><br><span class="line">  &quot;url&quot;: &quot;http://www.httpbin.org/post&quot;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>
<p>成功设置了data,headers,method<br>通过add_header方法添加headers的方式</p>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">req=request.Request(url=url,data=data,method=method)</span><br><span class="line">req.add_header(&#x27;User-Agent&#x27;,&#x27;xxxxx&#x27;)</span><br></pre></td></tr></table></figure>
<h3 id="高级用法"><a href="#高级用法" class="headerlink" title="高级用法"></a>高级用法</h3><p>&emsp;&emsp;学会构建请求之后，还有一些高级操作如Cookie处理、代理设置等，我们需要用Handler。Handler可以理解为各种处理器，有专门处理登录验证的、处理Cookie的、处理代理设置的。利用Handler,几乎可以实现HTTP请求中所有的功能。<br>&emsp;&emsp;urllib.request模块的BaseHandler类是其他所有Handler类的父类。提供了最基本的方法，例如default_open，protocol_request等。<br>有各种Handler子类继承BaseHandler类</p>
<ul>
<li>HTTPDefaultErrorHandler用于处理HTTP响应错误，所有错误都会抛出HTTPError类型的异常。</li>
<li>HTTPRedirectHandler用于处理重定向。</li>
<li>HTTPCookieProcessor用于处理Cookie。</li>
<li>ProxyHandler用于设置代理，代理默认为空。</li>
<li>HTTPPasswordMgr用于管理密码，它维护着用户名密码的对照表。</li>
<li>HTTPBasicAuthHandle用于管理认证，如果一个链接在打开时需要认证，那么可以用这个类来解决认证问题。<br>还有一个重要的类OpenerDirector，我们可以称之为Opener。之前用过的urlopen方法，就是urllib库为我们提供的一个Opener。为了实现更高级的功能，前面使用的Request类和urlopen类相当于类库已经封装好的极其常用的方法，利用这两个类可以完成基本的请求，但现在要实现更高级的功能，就要深入一层进行配置，使用更底层的实例来完成操作，所以要用Opener。<br>Opener类提供open方法，该方法返回的响应类型和urlopen方法如出一辙。使用Handler类来构建Opener类。</li>
</ul>
<h3 id="验证"><a href="#验证" class="headerlink" title="验证"></a>验证</h3><p>&emsp;&emsp;在访问某些网站时，例如 <a target="_blank" rel="noopener" href="https://ssr3.scrape.center/">https://ssr3.scrape.center</a> ,可能会弹出认证窗口<br><img src="/2024/04/28/%E5%9F%BA%E6%9C%AC%E5%BA%93%E7%9A%84%E4%BD%BF%E7%94%A8/4.png"><br>&emsp;&emsp;这种情况是这个网站启用了基本身份认证，英文是HTTP Basic Access Authentication，一种登陆验证方式，允许网页浏览器或其他客户端程序在请求网站时提供用户名和口令形式的身份认证。<br>爬虫可以借助HTTPBasicAuthHandler模块完成。</p>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br></pre></td><td class="code"><pre><span class="line">from urllib.request import HTTPBasicAuthHandler,HTTPPasswordMgrWithDefaultRealm,build_opener</span><br><span class="line">from urllib.error import  URLError</span><br><span class="line"></span><br><span class="line">username = &#x27;admin&#x27;</span><br><span class="line">password = &#x27;admin&#x27;</span><br><span class="line">url=&quot;https://ssr3.scrape.center/&quot;</span><br><span class="line"></span><br><span class="line">p=HTTPPasswordMgrWithDefaultRealm()</span><br><span class="line">p.add_password(None,url,username,password)</span><br><span class="line">auth_handler = HTTPBasicAuthHandler(p)</span><br><span class="line">opener = build_opener(auth_handler)</span><br><span class="line"></span><br><span class="line">try :</span><br><span class="line">    result = opener.open(url)</span><br><span class="line">    html = result.read().decode(&#x27;utf-8&#x27;)</span><br><span class="line">    print(html)</span><br><span class="line">except URLError as e:</span><br><span class="line">    print(e.reason)</span><br></pre></td></tr></table></figure>
<p>&emsp;&emsp;这里首先实例化了一个HTTPBasicAuthHandler对象auth_handler，参数是HTTPPasswordMgrWithDefaultRealm对象，它利用add_passward方法添加用户名和密码，建立了一个用来处理验证的Handler类。<br>&emsp;&emsp;将对象auth_handler作为参数传递给build_opener方法，构建一个Opener，它在发送请求时就相当于验证成功了。<br>&emsp;&emsp;最后利用Opener类中的open方法打开连接，即可完成验证。这里获取的结果就是验证成功后的页面源码内容。  </p>
<h3 id="代理"><a href="#代理" class="headerlink" title="代理"></a>代理</h3><p>添加代理</p>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br></pre></td><td class="code"><pre><span class="line">from urllib.error import URLError</span><br><span class="line">from urllib.request import ProxyHandler , build_opener</span><br><span class="line"></span><br><span class="line">proxy_handler = ProxyHandler(</span><br><span class="line">    &#123;&#x27;http&#x27; : &#x27;http://127.0.0.1:8080&#x27;,</span><br><span class="line">     &#x27;https&#x27; : &#x27;https://127.0.0.1:8080&#x27;&#125;</span><br><span class="line">)</span><br><span class="line"></span><br><span class="line">opener = build_opener(proxy_handler)</span><br><span class="line"></span><br><span class="line">try:</span><br><span class="line">    response = opener.open(&#x27;https://www.baidu.com&#x27;)</span><br><span class="line">    print(response.read().decode(&#x27;utf-8&#x27;))</span><br><span class="line">except URLError as e:</span><br><span class="line">    print(e.reason)</span><br></pre></td></tr></table></figure>
<p>&emsp;&emsp;在本地搭建HTTP代理，让其运行在8080端口上。使用了ProxyHandler，其参数是一个字典，键名是协议类型，键值是代理链接，可以添加多个代理。利用这个Handler和build_opener方法构建了一个Opener，之后发送请求即可。  </p>
<h3 id="Cookie"><a href="#Cookie" class="headerlink" title="Cookie"></a>Cookie</h3><p>处理Cookie需要用到相关的Handler。<br>先用实例看看如何获取Cookie</p>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><span class="line">import http.cookiejar,urllib.request</span><br><span class="line"></span><br><span class="line">cookie = http.cookiejar.CookieJar()</span><br><span class="line">handler = urllib.request.HTTPCookieProcessor(cookie)</span><br><span class="line">opener = urllib.request.build_opener(handler)</span><br><span class="line">response = opener.open(&#x27;https://www.baidu.com&#x27;)</span><br><span class="line">for item in cookie:</span><br><span class="line">    print(item.name+&quot;=&quot;+item.value)</span><br></pre></td></tr></table></figure>
<p>&emsp;&emsp;先声明CookieJar对象。然后利用HTTPCookieProcessor构建一个Handler,再利用build_opener方法构建Opener，执行open函数即可。<br><img src="/2024/04/28/%E5%9F%BA%E6%9C%AC%E5%BA%93%E7%9A%84%E4%BD%BF%E7%94%A8/5.png"><br>&emsp;&emsp;分别输出了每个Cookie条目的名称和值。<br>&emsp;&emsp;输出文件格式的内容，Cookie实际上也是以文本形式保存的。  </p>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><span class="line">import urllib.request,http.cookiejar</span><br><span class="line"></span><br><span class="line">filename = &#x27;cookies.txt&#x27;</span><br><span class="line">cookie = http.cookiejar.MozillaCookieJar(filename)</span><br><span class="line">handler = urllib.request.HTTPCookieProcessor(cookie)</span><br><span class="line">opener = urllib.request.build_opener(handler)</span><br><span class="line">response = opener.open(&#x27;https://www.baidu.com&#x27;)</span><br><span class="line">cookie.save(ignore_discard=True,ignore_expires=True)</span><br></pre></td></tr></table></figure>
<p>&emsp;&emsp;这时将CookieJar换成MozillaCookieJar，它会在生成文件时用到，是CookieJar的子类，用来处理跟Cookie和文件相关的事件，如读取和保存Cookie可以将Cookie保存成Mozilla型浏览器的Cookie格式。<br><img src="/2024/04/28/%E5%9F%BA%E6%9C%AC%E5%BA%93%E7%9A%84%E4%BD%BF%E7%94%A8/6.png"><br>&emsp;&emsp;LWPCookieJar同样可以读取和保存Cookie，只是Cookie文件的保存格式和MozillaCookieJar不一样，它会保存成LWP(libwww-perl)格式。<br>&emsp;&emsp;要保存LWP格式的Cookie文件，只需在声明时修改：<br><code>cookie = http.cookiejar.LWPCookieJar(filename)</code><br><img src="/2024/04/28/%E5%9F%BA%E6%9C%AC%E5%BA%93%E7%9A%84%E4%BD%BF%E7%94%A8/7.png"><br>不同格式的Cookie有一定的差异。<br>生成Cookie之后，对其进行读取和利用<br>以LWPCookieJar格式为例：</p>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><span class="line">import urllib.request,http.cookiejar</span><br><span class="line"></span><br><span class="line">cookie = http.cookiejar.LWPCookieJar()</span><br><span class="line">cookie.load(&#x27;cookies.txt&#x27;,ignore_discard=True,ignore_expires=True)</span><br><span class="line">handler = urllib.request.HTTPCookieProcessor(cookie)</span><br><span class="line">opener = urllib.request.build_opener(handler)</span><br><span class="line">response = opener.open(&#x27;https://www.baidu.com&#x27;)</span><br><span class="line">print(response.read().decode(&#x27;utf-8&#x27;))</span><br></pre></td></tr></table></figure>
<p>这里调用load方法来读取本地的Cookie文件，获取Cookie的内容。这样做的前提是我们已经生成了LWPCookieJar格式的Cookie，并保存成了文件，读取了Cookie之后，用同样的方法构建Handler类和Opener类即可完成操作。  </p>
<h2 id="处理异常"><a href="#处理异常" class="headerlink" title="处理异常"></a>处理异常</h2><h3 id="URLError"><a href="#URLError" class="headerlink" title="URLError"></a>URLError</h3><p>&emsp;&emsp;URLError类来自urllib库的error模块，继承自OSError类，是error异常模块的基类，由request模块产生的异常可以通过捕获这个类来处理。<br>它的一个属性reaseon，返回错误的原因。<br>用一个实例来看看</p>
<figure class="highlight py"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">from</span> urllib <span class="keyword">import</span> request,error</span><br><span class="line"><span class="keyword">try</span>:</span><br><span class="line">    response = request.urlopen(<span class="string">&#x27;https://blttttt.com/404&#x27;</span>)</span><br><span class="line"><span class="keyword">except</span> error.URLError <span class="keyword">as</span> e:</span><br><span class="line">    <span class="built_in">print</span>(e.reason)</span><br><span class="line">&gt;&gt;&gt;</span><br><span class="line">[Errno <span class="number">11001</span>] getaddrinfo failed</span><br></pre></td></tr></table></figure>
<p>我们访问了一个不存在的页面，是会报错的，但是通过捕获URLError这个异常，没有直接报错，而是输出了报错的原因，可以避免程序异常终止。  </p>
<h3 id="HTTPError"><a href="#HTTPError" class="headerlink" title="HTTPError"></a>HTTPError</h3><p>HTTPError是URLError的子类，专门用来处理HTTP请求的错误，例如认证请求失败。<br>有三个属性：</p>
<ul>
<li>code<br>返回HTTP状态码  </li>
<li>reason<br>同父类，返回错误原因</li>
<li>headers<br>返回请求头</li>
</ul>
<p>实例</p>
<figure class="highlight py"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">from</span> urllib <span class="keyword">import</span> request,error</span><br><span class="line"><span class="keyword">try</span>:</span><br><span class="line">    response = request.urlopen(<span class="string">&#x27;https://cuiqingcai.com/404&#x27;</span>)</span><br><span class="line"><span class="keyword">except</span> error.HTTPError <span class="keyword">as</span> e:</span><br><span class="line">    <span class="built_in">print</span>(e.reason,e.code,e.headers,sep=<span class="string">&#x27;\n&#x27;</span>)</span><br></pre></td></tr></table></figure>
<p>运行结果<br><img src="/2024/04/28/%E5%9F%BA%E6%9C%AC%E5%BA%93%E7%9A%84%E4%BD%BF%E7%94%A8/8.png"><br>URLError是HTTPError的父类，所以可以先选择捕获子类的错误，再捕获父类的错误，代码写法可以改为：</p>
<figure class="highlight py"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">from</span> urllib <span class="keyword">import</span> request,error</span><br><span class="line"><span class="keyword">try</span> :</span><br><span class="line">    response = request.urlopen(<span class="string">&#x27;https://cuiqingcai.com/404&#x27;</span>)</span><br><span class="line"><span class="keyword">except</span> error.HTTPError <span class="keyword">as</span> e:</span><br><span class="line">    <span class="built_in">print</span>(e.reason,e.code,e.headers,sep=<span class="string">&#x27;\n&#x27;</span>)</span><br><span class="line"><span class="keyword">except</span> error.URLError <span class="keyword">as</span> e:</span><br><span class="line">    <span class="built_in">print</span>(e.reason)</span><br><span class="line"><span class="keyword">else</span>:</span><br><span class="line">    <span class="built_in">print</span>(<span class="string">&quot;Request succeeded&quot;</span>)</span><br></pre></td></tr></table></figure>
<p>这样可以先捕获HTTPError，捕获它的错误原因、状态码、请求头信息。如果不是HTTPError异常，就会捕获URLError异常，输出错误原因。最后用else语句来处理正常的逻辑。<br>有时reason属性返回的不一定是字符串，也可能是一个对象。</p>
<figure class="highlight py"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> socket</span><br><span class="line"><span class="keyword">import</span> urllib.request</span><br><span class="line"><span class="keyword">import</span> urllib.error</span><br><span class="line"><span class="keyword">try</span> :</span><br><span class="line">    response = urllib.request.urlopen(<span class="string">&#x27;https://cuiqingcai.com/404&#x27;</span>,timeout=<span class="number">0.01</span>)</span><br><span class="line"><span class="keyword">except</span> urllib.error.URLError <span class="keyword">as</span> e:</span><br><span class="line">    <span class="built_in">print</span>(<span class="built_in">type</span>(e.reason))</span><br><span class="line">    <span class="keyword">if</span> <span class="built_in">isinstance</span>(e.reason,socket.timeout):</span><br><span class="line">        <span class="built_in">print</span>(<span class="string">&#x27;TIME OUT&#x27;</span>)</span><br><span class="line">&gt;&gt;&gt;</span><br><span class="line">&lt;<span class="keyword">class</span> <span class="string">&#x27;socket.timeout&#x27;</span>&gt;</span><br><span class="line">TIME OUT</span><br></pre></td></tr></table></figure>
<p>reason的类型是socket.timeout类，这里使用了isinstance方法来判断它的类型。  </p>
<h2 id="解析链接"><a href="#解析链接" class="headerlink" title="解析链接"></a>解析链接</h2><p>urllib库里还提供了parse模块，这个模块定义了处理URL的标准接口，如实现URL各部分的抽取、合并以及转换。它支持如下协议处理：file、ftp、gopher、hdl、http、https、imap、mailto、mms、news、nntp、prospero、rsync、rtsp、rtspu、sftp、sip、sips、snews、svn、svn+ssh、telnet和wais。  </p>
<h3 id="urlparse"><a href="#urlparse" class="headerlink" title="urlparse"></a>urlparse</h3><p>这个方法可以实现url的识别和分段</p>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><span class="line">from urllib.parse import urlparse</span><br><span class="line"></span><br><span class="line">result = urlparse(&#x27;https://www.baidu.com/index.html;user?id=5#comment&#x27;)</span><br><span class="line">print(type(result))</span><br><span class="line">print(result)</span><br><span class="line">&gt;&gt;&gt;</span><br><span class="line">&lt;class &#x27;urllib.parse.ParseResult&#x27;&gt;</span><br><span class="line">ParseResult(scheme=&#x27;https&#x27;, netloc=&#x27;www.baidu.com&#x27;, path=&#x27;/index.html&#x27;, params=&#x27;user&#x27;, query=&#x27;id=5&#x27;, fragment=&#x27;comment&#x27;)</span><br></pre></td></tr></table></figure>
<p>解析的结果是ParseResult类型的对象，包含6部分，分别是scheme、netloc、path、params、query、fragment。<br>观察URL：<code>https://www.baidu.com/index.html;user?id=5#comment</code><br>urlparse方法在解析URL时有特定的分隔符。如<code>://</code>前面的内容就是scheme，代表协议。第一个<code>/</code>符号前面便是netloc，即域名；后面是path，访问路径。分号<code>;</code>后面是params，参数。问号<code>?</code>后面是查询条件query，一般用作GET类型的URL。井号<code>#</code>后面是锚点fragment，用于直接定位页面内部的下拉位置。<br>标准链接格式：<code>scheme://netloc/path;params?quert#fragment</code>，一个标准的URL都会符合这个规则，利用urlparse方法就可以将它拆分开来。<br>urlparse的API用法：<br><code>urllib.parse.urlparse(urlstring,scheme=&#39;&#39;,allow_fragment=True)</code></p>
<ul>
<li>urlstring:<br>这是必填项，即待解析的URL。</li>
<li>scheme:<br>这是默认的协议。如果待解析的URL没有带协议信息，就会将这个作为默认协议。</li>
<li>allow_fragment:<br>是否忽略fragment。如果此项被设置为False，那么fragment部分就会被忽略，它会被解析为path、params或者query的一部分，而fragment部分为空。</li>
</ul>
<h3 id="urlunparse"><a href="#urlunparse" class="headerlink" title="urlunparse"></a>urlunparse</h3><p>此方法用于构造URL。接收的参数是一个可迭代对象，其长度必须是6，否则会抛出参数数量不足或者过多的问题。<br>实例</p>
<figure class="highlight py"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">from</span> urllib.parse <span class="keyword">import</span> urlunparse</span><br><span class="line">data = [<span class="string">&#x27;https&#x27;</span>,<span class="string">&#x27;www.baidu.com&#x27;</span>,<span class="string">&#x27;index.html&#x27;</span>,<span class="string">&#x27;uesr&#x27;</span>,<span class="string">&#x27;a=6&#x27;</span>,<span class="string">&#x27;comment&#x27;</span>]</span><br><span class="line"><span class="built_in">print</span>(urlunparse((data)))</span><br><span class="line">&gt;&gt;&gt;</span><br><span class="line">https://www.baidu.com/index.html;uesr?a=<span class="number">6</span><span class="comment">#comment</span></span><br></pre></td></tr></table></figure>
<p>这里的参数使用的data列表类型。也可以使用其他类型，如元组或特定的数据结构。  </p>
<h3 id="urlsplit"><a href="#urlsplit" class="headerlink" title="urlsplit"></a>urlsplit</h3><p>此方法和urlparse类似，不过它不再单独解析params这部分（params会合并到path中），只返回5个结果。<br>实例</p>
<figure class="highlight py"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">from</span> urllib.parse <span class="keyword">import</span> urlsplit</span><br><span class="line">result = urlsplit(<span class="string">&#x27;https://www.baidu.com/index.html;user?id=5#comment&#x27;</span>)</span><br><span class="line"><span class="built_in">print</span>(result)</span><br><span class="line"><span class="built_in">print</span>(<span class="built_in">type</span>(result))</span><br><span class="line">&gt;&gt;&gt;</span><br><span class="line">SplitResult(scheme=<span class="string">&#x27;https&#x27;</span>, netloc=<span class="string">&#x27;www.baidu.com&#x27;</span>, path=<span class="string">&#x27;/index.html;user&#x27;</span>, query=<span class="string">&#x27;id=5&#x27;</span>, fragment=<span class="string">&#x27;comment&#x27;</span>)</span><br><span class="line">&lt;<span class="keyword">class</span> <span class="string">&#x27;urllib.parse.SplitResult&#x27;</span>&gt;</span><br></pre></td></tr></table></figure>
<p>返回的结果是SplitResult，其实也是元组，可以通过属性取其值，也可以通过索引取值。</p>
<figure class="highlight py"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">from</span> urllib.parse <span class="keyword">import</span> urlsplit</span><br><span class="line">result = urlsplit(<span class="string">&#x27;https://www.baidu.com/index.html;user?id=5#comment&#x27;</span>)</span><br><span class="line"><span class="built_in">print</span>(result.scheme,result[<span class="number">0</span>])</span><br><span class="line">&gt;&gt;&gt;</span><br><span class="line">https https</span><br></pre></td></tr></table></figure>
<h3 id="urlunsplit"><a href="#urlunsplit" class="headerlink" title="urlunsplit"></a>urlunsplit</h3><p>与urlunparse类似，也是将链接各部分组合成完整链接的方法，传入的参数是可迭代对象，如列表，元组，参数长度必须是5。<br>实例</p>
<figure class="highlight py"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">from</span> urllib.parse <span class="keyword">import</span> urlunsplit</span><br><span class="line">data=[<span class="string">&#x27;https&#x27;</span>,<span class="string">&#x27;www.baidu.com&#x27;</span>,<span class="string">&#x27;index.html&#x27;</span>,<span class="string">&#x27;a=6&#x27;</span>,<span class="string">&#x27;comment&#x27;</span>]</span><br><span class="line"><span class="built_in">print</span>(urlunsplit(data))</span><br><span class="line">&gt;&gt;&gt;</span><br><span class="line">https://www.baidu.com/index.html?a=<span class="number">6</span><span class="comment">#comment</span></span><br></pre></td></tr></table></figure>
<h3 id="urljoin"><a href="#urljoin" class="headerlink" title="urljoin"></a>urljoin</h3><p>&emsp;&emsp;urlunparse和urlunsplit方法都可以完成链接的合并，但前提是有特定的对象，链接的每一部分要清晰分开。<br>&emsp;&emsp;urljoin也可以生成链接。要提供base_url(基础链接)作为该方法的第一个参数，将新链接作为第二个参数。urljoin方法会分析base_url的scheme、netloc和path三个内容，并对新链接缺失的部分进行补充，最后返回结果。<br>实例</p>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br></pre></td><td class="code"><pre><span class="line">from urllib.parse import urljoin</span><br><span class="line"></span><br><span class="line">print(urljoin(&#x27;https://www.baidu.com&#x27;, &#x27;FAQ.html&#x27;))</span><br><span class="line">print(urljoin(&#x27;https://www.baidu.com&#x27;, &#x27;https://cuiqingcai.com/FAQ.html&#x27;))</span><br><span class="line">print(urljoin(&#x27;https://www.baidu.com/about.html&#x27;, &#x27;https://cuiqingcai.com/FAQ.html&#x27;))</span><br><span class="line">print(urljoin(&#x27;https://www.baidu.com/about.html&#x27;, &#x27;https://cuiqingcai.com/FAQ.html?question=2&#x27;))</span><br><span class="line">print(urljoin(&#x27;https://www.baidu.com?wd=abc&#x27;, &#x27;https://cuiqingcai.com/index,php&#x27;))</span><br><span class="line">print(urljoin(&#x27;https://www.baidu.com&#x27;, &#x27;?category=2#comment&#x27;))</span><br><span class="line">print(urljoin(&#x27;www.baidu.com&#x27;, &#x27;?category=2#comment&#x27;))</span><br><span class="line">print(urljoin(&#x27;www.baidu.com#comment&#x27;, &#x27;?caetgory=2&#x27;))</span><br><span class="line">&gt;&gt;&gt;</span><br><span class="line">https://www.baidu.com/FAQ.html</span><br><span class="line">https://cuiqingcai.com/FAQ.html</span><br><span class="line">https://cuiqingcai.com/FAQ.html</span><br><span class="line">https://cuiqingcai.com/FAQ.html?question=2</span><br><span class="line">https://cuiqingcai.com/index,php</span><br><span class="line">https://www.baidu.com?category=2#comment</span><br><span class="line">www.baidu.com?category=2#comment</span><br><span class="line">www.baidu.com?caetgory=2</span><br></pre></td></tr></table></figure>
<p>&emsp;&emsp;可以发现，base_url提供了scheme、netloc和path。如果新链接不存在这三项，就进行补充，如果存在，base_url是不起作用的。<br>&emsp;&emsp;通过urljoin方法，实现链接的解析、拼合与生成。  </p>
<h3 id="urlencode"><a href="#urlencode" class="headerlink" title="urlencode"></a>urlencode</h3><p>&emsp;&emsp;它在构造GET请求参数的时候非常有用。<br>实例：</p>
<figure class="highlight py"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">from</span> urllib.parse <span class="keyword">import</span> urlencode</span><br><span class="line">params=&#123;</span><br><span class="line">    <span class="string">&#x27;name&#x27;</span>:<span class="string">&#x27;germy&#x27;</span>,</span><br><span class="line">    <span class="string">&#x27;age&#x27;</span>:<span class="number">25</span></span><br><span class="line">&#125;</span><br><span class="line">base_url = <span class="string">&#x27;http://www.baidu.com?&#x27;</span></span><br><span class="line">url=base_url+urlencode(params)</span><br><span class="line"><span class="built_in">print</span>(url)</span><br><span class="line">&gt;&gt;&gt;</span><br><span class="line">http://www.baidu.com?name=germy&amp;age=<span class="number">25</span></span><br></pre></td></tr></table></figure>
<p>&emsp;&emsp;先声明了一个字典params，用于将参数显示出来，然后调用urlencode方法将params序列化为GET请求的参数。<br>&emsp;&emsp;urlencode方法很常用，有时为了更方便地构造参数。我们会先用字典将参数表示出来，然后将字典转化为URL的参数时，调用该方法即可。  </p>
<h3 id="parse-qs"><a href="#parse-qs" class="headerlink" title="parse_qs"></a>parse_qs</h3><p>&emsp;&emsp;有了序列化就会有反序列化。利用parse_qs方法，可以将一串GET请求参数转回字典。<br>实例：</p>
<figure class="highlight py"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">from</span> urllib.parse <span class="keyword">import</span> parse_qs</span><br><span class="line">query = <span class="string">&#x27;name=germy&amp;age=25&#x27;</span></span><br><span class="line"><span class="built_in">print</span>(parse_qs(query))</span><br><span class="line">&gt;&gt;&gt;</span><br><span class="line">&#123;<span class="string">&#x27;name&#x27;</span>: [<span class="string">&#x27;germy&#x27;</span>], <span class="string">&#x27;age&#x27;</span>: [<span class="string">&#x27;25&#x27;</span>]&#125;</span><br></pre></td></tr></table></figure>
<p>可以看到URL的参数车公共转换为了字典类型。</p>
<blockquote>
<p>查询字符串是 URL 中 ? 符号后的部分，通常用于在 HTTP 请求中传递参数。<br>输出结果会是一个字典，其中每个键对应一个参数名，每个值是一个列表，包含该参数名对应的所有值。  </p>
</blockquote>
<h3 id="parse-qsl"><a href="#parse-qsl" class="headerlink" title="parse_qsl"></a>parse_qsl</h3><p>&emsp;&emsp;此方法用于将参数转化为由元组组成的列表：</p>
<figure class="highlight py"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">from</span> urllib.parse <span class="keyword">import</span> parse_qsl</span><br><span class="line">query = <span class="string">&#x27;name=germy&amp;age=25&#x27;</span></span><br><span class="line"><span class="built_in">print</span>(parse_qsl(query))</span><br><span class="line">&gt;&gt;&gt;</span><br><span class="line">[(<span class="string">&#x27;name&#x27;</span>, <span class="string">&#x27;germy&#x27;</span>), (<span class="string">&#x27;age&#x27;</span>, <span class="string">&#x27;25&#x27;</span>)]</span><br></pre></td></tr></table></figure>
<p>&emsp;&emsp;运行结果是一个列表，该列表中的每一个元素是一个元组，元组第一个内容是参数名，第二个内容是参数值。  </p>
<h3 id="quote"><a href="#quote" class="headerlink" title="quote"></a>quote</h3><p>&emsp;&emsp;此方法可以将内容转化为URL编码的格式。当URL中带有中文参数的时候，有可能导致乱码，使用quote可以将中文字符转化为URL编码。  </p>
<figure class="highlight py"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">from</span> urllib.parse <span class="keyword">import</span> quote</span><br><span class="line">keyword = <span class="string">&#x27;壁纸&#x27;</span></span><br><span class="line">url=<span class="string">&#x27;http://www.baidu.com/s?wd=&#x27;</span>+quote(keyword)</span><br><span class="line"><span class="built_in">print</span>(url)</span><br><span class="line">&gt;&gt;&gt;</span><br><span class="line">http://www.baidu.com/s?wd=%E5%A3%<span class="number">81</span>%E7%BA%B8</span><br></pre></td></tr></table></figure>
<h3 id="unquote"><a href="#unquote" class="headerlink" title="unquote"></a>unquote</h3><p>它可以进行URL解码。<br>实例：</p>
<figure class="highlight py"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">from</span> urllib.parse <span class="keyword">import</span> unquote</span><br><span class="line">url = <span class="string">&#x27;http://www.baidu.com/s?wd=%E5%A3%81%E7%BA%B8&#x27;</span></span><br><span class="line"><span class="built_in">print</span>(unquote(url))</span><br><span class="line">&gt;&gt;&gt;</span><br><span class="line">http://www.baidu.com/s?wd=壁纸</span><br></pre></td></tr></table></figure>
<h2 id="分析Robots协议"><a href="#分析Robots协议" class="headerlink" title="分析Robots协议"></a>分析Robots协议</h2><p>利用urllib库的robotparser模块，可以分析网站的Robots协议。</p>
<h3 id="Robots协议"><a href="#Robots协议" class="headerlink" title="Robots协议"></a>Robots协议</h3><p>&emsp;&emsp;该协议也称爬虫协议或机器人协议，全名是网络爬虫排除标准。来告诉爬虫和搜索引擎哪些页面可以抓取，哪些不可以。通常有一个叫robots.txt的文本文件，一般在网站的根目录下。<br>&emsp;&emsp;爬虫在访问一个站点时，首先检查这个站点的根目录下是否存在robots.txt文件，如果存在，会根据其中定义的爬取范围来爬取。若没有这个文件，搜索爬虫会访问所有可直接访问的页面。  </p>
<h3 id="robotparser"><a href="#robotparser" class="headerlink" title="robotparser"></a>robotparser</h3><p>&emsp;&emsp;使用robotparser模块来解析robots.txt文件。该模块提供了一个RobotFileParser，它可以根据某网站的robots.txt文件判断一个爬取爬虫是否有权限爬取这个网页。<br>&emsp;&emsp;这个类的使用只需在构造方法里传入robots.txt文件的链接即可。<br>&emsp;&emsp;看它的声明<code>urllib.robotparser.RobotFileParser(url=&#39;&#39;)</code><br>&emsp;&emsp;也可以不在声明时传入robots.txt文件的链接，让其默认为空，最后使用set_url()方法去设置也可以。</p>
<ul>
<li>set_url:<br>用来设置robots.txt文件的链接。如果在创建RobotFileParser对象时传入了链接，就不需要使用这个方法设置了。</li>
<li>read:<br>读取robots.txt文件并进行分析。这个方法执行读取和分析操作，如果不调用这个方法，接下来接下来的判断都会为False，所以一定要调用这个方法。这个方法虽然不返回内容，但是执行了读取操作。</li>
<li>parse:<br>用来解析robots.txt文件，传入其中的参数是robots.txt文件中某些行的内容，会按照robots.txt的语法规则来分析这些内容。  </li>
<li>can_fetch:<br>该方法有两个参数，第一个是User-Agent，第二个是要抓取的URL。返回结果是True或False，表示搜索引擎是否可以抓取这个URL。</li>
<li>mtime:<br>返回上次抓取和分析robots.txt文件的时间，这对于长时间分析和抓取robots.txt文件的搜索爬虫很有必要，可能需要定期检查以抓取最新的robots.txt文件。</li>
<li>modified:<br>同样对长时间分析和抓取的搜索爬虫很有帮助，可以将时间设置为上次抓取和分析robots.txt文件的时间。<br>实例  <blockquote>
<p>首先创建一个RobotFileParser对象rp，然后通过set_url方法设置robots.txt文件的链接。利用can_fetch方法判断网页是否可以被抓取。  </p>
</blockquote>
<figure class="highlight py"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">from</span> urllib.robotparser <span class="keyword">import</span> RobotFileParser</span><br><span class="line">rp = RobotFileParser()</span><br><span class="line">rp.set_url(<span class="string">&quot;https://www.baidu.com/robots.txt&quot;</span>)</span><br><span class="line">rp.read()</span><br><span class="line"><span class="built_in">print</span>(rp.can_fetch(<span class="string">&#x27;Baiduspider&#x27;</span>,<span class="string">&#x27;https://www.baidu.com&#x27;</span>))</span><br><span class="line"><span class="built_in">print</span>(rp.can_fetch(<span class="string">&#x27;Baiduspider&#x27;</span>,<span class="string">&#x27;https://www.baidu.com/homepage/&#x27;</span>))</span><br><span class="line"><span class="built_in">print</span>(rp.can_fetch(<span class="string">&#x27;Googlebot&#x27;</span>,<span class="string">&#x27;https://www.baidu.com/homepage/&#x27;</span>))</span><br><span class="line">&gt;&gt;&gt;</span><br><span class="line"><span class="literal">True</span></span><br><span class="line"><span class="literal">True</span></span><br><span class="line"><span class="literal">False</span></span><br></pre></td></tr></table></figure>
&emsp;&emsp;利用Baiduspider就可以抓取百度的首页以及homepage页面，但是Googlebot就不能抓取homepage页面。<br>&emsp;&emsp;还可以用parse方法执行对robots.txt文件的读取和分析。<br>实例<figure class="highlight py"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">from</span> urllib.request <span class="keyword">import</span> urlopen</span><br><span class="line"><span class="keyword">from</span> urllib.robotparser <span class="keyword">import</span> RobotFileParser</span><br><span class="line"></span><br><span class="line">rp = RobotFileParser()</span><br><span class="line">rp.parse(urlopen(<span class="string">&#x27;https://www.baidu.com/robots.txt&#x27;</span>).read().decode(<span class="string">&#x27;utf-8&#x27;</span>).split(<span class="string">&#x27;\n&#x27;</span>))</span><br><span class="line"><span class="built_in">print</span>(rp.can_fetch(<span class="string">&#x27;Baiduspider&#x27;</span>,<span class="string">&#x27;https://www.baidu.com&#x27;</span>))</span><br><span class="line"><span class="built_in">print</span>(rp.can_fetch(<span class="string">&#x27;Baiduspider&#x27;</span>,<span class="string">&#x27;https://www.baidu.com/homepage/&#x27;</span>))</span><br><span class="line"><span class="built_in">print</span>(rp.can_fetch(<span class="string">&#x27;Googlebot&#x27;</span>,<span class="string">&#x27;https://www.baidu.com/homepage/&#x27;</span>))</span><br><span class="line">&gt;&gt;&gt;</span><br><span class="line"><span class="literal">True</span></span><br><span class="line"><span class="literal">True</span></span><br><span class="line"><span class="literal">False</span></span><br></pre></td></tr></table></figure>
运行结果一样</li>
</ul>
<h1 id="requests的使用"><a href="#requests的使用" class="headerlink" title="requests的使用"></a>requests的使用</h1>
    </div>

    
    
    

    <footer class="post-footer">
          <div class="post-tags">
              <a href="/tags/%E5%9F%BA%E6%9C%AC%E5%BA%93%E7%9A%84%E4%BD%BF%E7%94%A8/" rel="tag">基本库的使用</a>
          </div>

        

          <div class="post-nav">
            <div class="post-nav-item">
                <a href="/2024/04/25/%E7%88%AC%E8%99%AB%E5%9F%BA%E7%A1%80/" rel="prev" title="爬虫基础">
                  <i class="fa fa-angle-left"></i> 爬虫基础
                </a>
            </div>
            <div class="post-nav-item">
            </div>
          </div>
    </footer>
  </article>
</div>






</div>
  </main>

  <footer class="footer">
    <div class="footer-inner">

  <div class="copyright">
    &copy; 
    <span itemprop="copyrightYear">2024</span>
    <span class="with-love">
      <i class="fa fa-heart"></i>
    </span>
    <span class="author" itemprop="copyrightHolder">白乐天</span>
  </div>

<span id="timeDate">载入天数...</span>
<span id="times">载入时分秒...</span>
<script>
    var now = new Date();
    function createtime() {
        var grt= new Date("01/01/2024 00:00:00"); //修改为你的网站开始运行的时间
        now.setTime(now.getTime()+250);
        days = (now - grt ) / 1000 / 60 / 60 / 24; dnum = Math.floor(days);
        hours = (now - grt ) / 1000 / 60 / 60 - (24 * dnum); hnum = Math.floor(hours);
        if(String(hnum).length ==1 ){hnum = "0" + hnum;} minutes = (now - grt ) / 1000 /60 - (24 * 60 * dnum) - (60 * hnum);
        mnum = Math.floor(minutes); if(String(mnum).length ==1 ){mnum = "0" + mnum;}
        seconds = (now - grt ) / 1000 - (24 * 60 * 60 * dnum) - (60 * 60 * hnum) - (60 * mnum);
        snum = Math.round(seconds); if(String(snum).length ==1 ){snum = "0" + snum;}
        document.getElementById("timeDate").innerHTML = "本站已安全运行 "+dnum+" 天 ";
        document.getElementById("times").innerHTML = hnum + " 小时 " + mnum + " 分 " + snum + " 秒.";
    }
setInterval("createtime()",250);
</script>
    </div>
  </footer>

  

<noscript>
  <div class="noscript-warning">Theme NexT works best with JavaScript enabled</div>
</noscript>


  
  <script src="https://cdnjs.cloudflare.com/ajax/libs/animejs/3.2.1/anime.min.js" integrity="sha256-XL2inqUJaslATFnHdJOi9GfQ60on8Wx1C2H8DYiN1xY=" crossorigin="anonymous"></script>
<script src="/js/comments.js"></script><script src="/js/utils.js"></script><script src="/js/motion.js"></script><script src="/js/next-boot.js"></script>

  






  





</body>
</html>
